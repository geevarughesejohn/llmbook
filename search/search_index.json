{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.1%20learning%20objective.html","title":"Chapter 1: The Language Modeling Problem","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.1%20learning%20objective.html#1-learning-objectives","title":"1. Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ul> <li>Understand what a language model is and what it means to \"model language\" using a computer.</li> <li>Differentiate between rule-based, statistical, and neural approaches to natural language processing (NLP).</li> <li>Appreciate the complexity of language and why building machines that understand or generate it is a challenging yet vital goal in artificial intelligence.</li> </ul>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.2%20Introduction.html","title":"1.2 Introduction","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.2%20Introduction.html#2-introduction","title":"2. Introduction","text":"<p>Imagine you're having a conversation with someone who always knows what to say next\u2014whether it's finishing your sentence, answering a question, or crafting a story. That\u2019s what a language model tries to do.</p> <p>A language model is a computer program trained to understand and generate human language. At its core, it's answering one big question:</p> <p>What is the most likely next word, sentence, or idea, given what has already been said?</p> <p>You interact with language models every day\u2014when your phone suggests words while you type, when a chatbot answers your query, or when translation apps convert text between languages. These systems work thanks to powerful models trained on massive amounts of text.</p> <p>But building machines that understand language is not as simple as feeding them grammar rules or dictionaries. Language is full of ambiguity, context, sarcasm, multiple meanings, and ever-changing patterns. That\u2019s what makes the language modeling problem so fascinating\u2014and so challenging.</p> <p>In this chapter, we\u2019ll trace the journey from early rule-based methods to modern neural networks that power tools like ChatGPT. You\u2019ll see how language modeling evolved, why it matters, and how it works at a fundamental level.</p> <p>Let\u2019s begin by exploring what it means to model language in the first place.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html","title":"1.3 What Does It Mean to \u201cModel\u201d Language?","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html#3-what-does-it-mean-to-model-language","title":"3. What Does It Mean to \u201cModel\u201d Language?","text":"<p>At a basic level, to \"model\" something means to create a simplified version of it that helps us understand or predict how it works.</p> <p>So when we model language, we\u2019re trying to build a system that can understand and generate human language in a way that\u2019s useful.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html#a-simple-analogy-the-language-oracle","title":"\ud83e\udde0 A Simple Analogy: The Language Oracle","text":"<p>Imagine you have a magical oracle that completes your sentences. You say:</p> <p>\"The cat sat on the...\"</p> <p>And the oracle instantly replies:</p> <p>\"mat.\"</p> <p>How did it know? Because it has learned\u2014by reading a huge number of sentences\u2014that \u201ccat sat on the mat\u201d is a common and logical phrase. That\u2019s essentially what a language model does: it predicts the next word (or words) based on what came before.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html#from-words-to-probabilities","title":"\ud83d\udcca From Words to Probabilities","text":"<p>Let\u2019s make it a bit more formal (but still beginner-friendly).</p> <p>Suppose we give a model the phrase:</p> <p><code>\"The sun is shining in the\"</code></p> <p>We want it to guess the next word. It might come up with:</p> <ul> <li><code>\"sky\"</code> with a probability of 0.7</li> <li><code>\"morning\"</code> with a probability of 0.2</li> <li><code>\"kitchen\"</code> with a probability of 0.05</li> <li>and so on...</li> </ul> <p>The model assigns a probability to each possible word based on how likely it is to appear next. The word with the highest probability is often chosen as the output.</p> <p>So, language modeling is essentially about predicting the probability of word sequences.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html#what-makes-this-hard","title":"\ud83e\udd16 What Makes This Hard?","text":"<p>Natural language is messy:</p> <ul> <li>Words have multiple meanings (\"bank\" could be a riverbank or a financial institution).</li> <li>Sentences depend on context (what was said earlier matters).</li> <li>People use slang, jokes, and non-standard grammar all the time.</li> </ul> <p>This makes building accurate models difficult, especially if we rely on simple rules.</p> <p>But before we dive into neural networks, let\u2019s take a quick tour of how people tried to model language before AI got smart.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html","title":"1.4 A Brief History of Language Modeling: Rules \u2192 Statistics \u2192 Neural Nets","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#4-a-brief-history-of-language-modeling-rules-statistics-neural-nets","title":"4. A Brief History of Language Modeling: Rules \u2192 Statistics \u2192 Neural Nets","text":"<p>Before today's intelligent chatbots and code-completing AIs, language modeling took a long, winding road\u2014from rigid rules to clever statistics to powerful neural networks. Each step brought us closer to machines that can truly work with language rather than just follow instructions.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#rule-based-systems-the-age-of-handcrafted-logic","title":"\ud83e\uddfe Rule-Based Systems: The Age of Handcrafted Logic","text":"<p>In the earliest days of NLP, people believed we could teach computers to understand language by writing down all the grammar rules ourselves.</p> <p>These systems looked something like this:</p> <ul> <li>If you see \u201cis\u201d, expect a noun or adjective after it.</li> <li>If you see \u201cthe\u201d, expect a noun next.</li> </ul> <p>This approach is called rule-based NLP.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#pros","title":"\u2705 Pros:","text":"<ul> <li>Clear and interpretable.</li> <li>Useful for specific tasks (e.g., grammar checking).</li> </ul>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#cons","title":"\u274c Cons:","text":"<ul> <li>Doesn\u2019t scale well.</li> <li>Can\u2019t handle ambiguity, slang, or creative language.</li> <li>Requires human experts to write and update the rules.</li> </ul> <p>It's like trying to describe the entire ocean by writing down a few puddle-sized rules. Eventually, we needed something more flexible.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#statistical-language-models-learning-from-data","title":"\ud83d\udcc8 Statistical Language Models: Learning from Data","text":"<p>As more digital text became available, researchers had a better idea:</p> <p>\u201cWhy not let the computer learn patterns from real language, instead of writing rules by hand?\u201d</p> <p>Enter statistical language models.</p> <p>These models look at large text datasets and calculate how often words and phrases appear together.</p> <p>For example, they might learn:</p> <ul> <li>\u201cNew York\u201d occurs more often than \u201cNew banana\u201d.</li> <li>\u201cThe cat sat\u201d is more common than \u201cCat the sat\u201d.</li> </ul> <p>One simple example is the n-gram model, where the model predicts the next word based on the last n words. A bigram looks at 1 previous word; a trigram looks at 2, and so on.</p> <p>Example: In the sentence <code>\"I love deep\"</code>, a trigram model might guess <code>\"learning\"</code> is the next word because it often follows that sequence.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#pros_1","title":"\u2705 Pros:","text":"<ul> <li>Learns from real data.</li> <li>Doesn\u2019t need hand-coded rules.</li> </ul>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#cons_1","title":"\u274c Cons:","text":"<ul> <li>Can't remember long context (e.g., what happened 10 words ago).</li> <li>Struggles with rare or new phrases.</li> <li>Requires large datasets to be useful.</li> </ul> <p>Statistical models were smarter than rules, but still had limits. They treated language more like math than meaning.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html#neural-networks-learning-meaning-not-just-patterns","title":"\ud83e\udde0 Neural Networks: Learning Meaning, Not Just Patterns","text":"<p>The breakthrough came when computers started using neural networks\u2014algorithms inspired by the human brain\u2014to model language.</p> <p>Instead of just counting words, neural models learned relationships and meanings by training on massive datasets.</p> <ul> <li>They could represent words as vectors (<code>word embeddings</code>) capturing meaning.</li> <li>They could look at longer contexts, not just the last 2 or 3 words.</li> <li>They could even generate new sentences that sounded natural.</li> </ul> <p>Early models like RNNs (Recurrent Neural Networks) and LSTMs started this trend. Then came transformers, which revolutionized everything.</p> <p>We'll go deeper into these models in later chapters, but for now, understand this shift:</p> <p>Rule-based \u2192 count words. Statistical \u2192 count patterns. Neural \u2192 learn meaning and context.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html","title":"1.5 Why Is Modeling Language So Difficult?","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#5-why-is-modeling-language-so-difficult","title":"5. Why Is Modeling Language So Difficult?","text":"<p>At first glance, language may seem easy to us humans. We pick it up as children, use it without much thought, and often don\u2019t realize how complex it really is.</p> <p>But when you try to teach a computer to understand or generate language, all that hidden complexity shows up. Let\u2019s look at why modeling language is such a tough problem, even for modern AI.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#1-ambiguity-is-everywhere","title":"\ud83c\udf00 1. Ambiguity Is Everywhere","text":"<p>Human language is full of ambiguity\u2014words and sentences can have multiple meanings depending on context.</p> <p>\"He saw the man with the telescope.\"</p> <p>Who has the telescope? The man? Or he?</p> <p>Even a simple sentence like that can confuse a model unless it has enough context.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#2-context-can-span-sentences-or-pages","title":"\ud83d\udcda 2. Context Can Span Sentences or Pages","text":"<p>Understanding language requires remembering what was said earlier\u2014sometimes much earlier.</p> <p>\u201cAlice picked up the book. She opened it and began to read.\u201d</p> <p>A human knows that \u201cit\u201d refers to \u201cthe book.\u201d A model must be able to track entities across time and words. This long-range dependency is a hard problem for many traditional models.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#3-tone-sarcasm-and-emotion","title":"\ud83d\udcac 3. Tone, Sarcasm, and Emotion","text":"<p>Consider this sentence:</p> <p>\"Yeah, right. That\u2019s just what I needed today.\"</p> <p>Depending on tone or context, this could be sincere or sarcastic. Humans use intonation, facial expressions, and social cues to interpret meaning. A computer only sees text\u2014it must learn to infer tone from patterns in data.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#4-cultural-and-world-knowledge","title":"\ud83c\udf0d 4. Cultural and World Knowledge","text":"<p>To understand or generate meaningful responses, a model often needs background knowledge.</p> <p>For example:</p> <p>\"The Eiffel Tower is in Paris.\" \"He took a selfie in front of it.\"</p> <p>Understanding that \u201cit\u201d refers to the Eiffel Tower requires common sense and world knowledge\u2014things we take for granted but that a model has to learn from data.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#5-language-is-always-changing","title":"\ud83d\udd23 5. Language Is Always Changing","text":"<p>New words, slang, and ways of speaking pop up constantly:</p> <ul> <li>\"I ghosted him.\"</li> <li>\"This meme is fire.\"</li> <li>\"Can you ChatGPT this paragraph?\"</li> </ul> <p>Language evolves, and so must our models.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html#summary-the-hidden-complexity","title":"\ud83d\udea7 Summary: The Hidden Complexity","text":"<p>Language isn\u2019t just about grammar or spelling. It\u2019s a rich, flexible, messy system that reflects human thought, emotion, and culture. This makes it one of the most complex things for a machine to model.</p> <p>That\u2019s why language modeling is not just a technical task\u2014it\u2019s a human challenge.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html","title":"1.6 Code Example: A Tiny Predictive Language Model (with N Grams)","text":"<p>Awesome! Here\u2019s a simple, beginner-friendly code example to help demystify the basics of language modeling using a small n-gram model in Python.</p> <p>We\u2019ll build a tiny model that predicts the next word based on the previous word\u2014called a bigram model.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html#6-code-example-a-tiny-predictive-language-model-with-n-grams","title":"6. Code Example: A Tiny Predictive Language Model (with N-Grams)","text":"<p>Before deep learning came into the picture, language models were often built using n-grams\u2014sequences of n words. For example:</p> <ul> <li>A unigram model predicts words based on how often they occur.</li> <li>A bigram model predicts the next word based on the previous one.</li> <li>A trigram model uses the previous two words, and so on.</li> </ul> <p>Let\u2019s build a bigram model from scratch to see how this works.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html#python-code-simple-bigram-model","title":"\ud83d\udc0d Python Code: Simple Bigram Model","text":"<pre><code>import random\nfrom collections import defaultdict\n\n# Our tiny corpus (training data)\ncorpus = [\n    \"I love deep learning\",\n    \"I love machine learning\",\n    \"deep learning is fun\",\n    \"machine learning is powerful\"\n]\n\n# Step 1: Tokenize sentences into word pairs (bigrams)\nbigram_model = defaultdict(list)\n\nfor sentence in corpus:\n    words = sentence.split()\n    for i in range(len(words) - 1):\n        prefix = words[i]\n        next_word = words[i + 1]\n        bigram_model[prefix].append(next_word)\n\n# Step 2: Predict the next word given a previous word\ndef predict_next_word(previous_word):\n    possible_words = bigram_model.get(previous_word, [])\n    if not possible_words:\n        return \"&lt;no prediction&gt;\"\n    return random.choice(possible_words)\n\n# Step 3: Try the model\ntest_word = \"learning\"\npredicted = predict_next_word(test_word)\nprint(f\"Given '{test_word}', the model predicts: '{predicted}'\")\n</code></pre>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html#whats-happening-here","title":"\ud83e\udde0 What\u2019s Happening Here?","text":"<ol> <li> <p>Tokenization: We split each sentence into words and collect bigram pairs like (\"I\", \"love\") or (\"machine\", \"learning\").</p> </li> <li> <p>Model Creation: For each word, we store a list of words that follow it in the training data.</p> </li> <li> <p>Prediction: Given a word, we randomly pick one of its likely next words based on what it saw during training.</p> </li> </ol> <p>\ud83d\udca1 Note: This is a toy model. Real language models learn probabilities and generalize better\u2014but this example shows the core idea behind sequence prediction.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html#sample-output","title":"\ud83d\udce5 Sample Output","text":"<pre><code>Given 'learning', the model predicts: 'is'\n</code></pre> <p>Try changing <code>test_word = \"I\"</code> or <code>\"deep\"</code> to see how it reacts.</p> <p>In the next section, we\u2019ll summarize what we\u2019ve learned and connect it to the bigger picture of modern LLMs.</p> <p>Shall we move on to 7. Summary?</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.7%20Summary.html","title":"1.7 Summary","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.7%20Summary.html#7-summary","title":"7. Summary","text":"<p>In this chapter, we took our first step into the world of language models by understanding what it means to \u201cmodel language.\u201d</p> <p>We began with the basic idea: a language model tries to predict the next word or phrase based on what has already been written or said. This seemingly simple task hides a lot of complexity due to the ambiguous, contextual, and evolving nature of human language.</p> <p>We then walked through how the field of language modeling has evolved over time:</p> <ul> <li>From rule-based systems that relied on hand-crafted grammar,</li> <li>To statistical models like n-grams that learned from frequency patterns,</li> <li>And finally, to neural network models that learn deep meaning and structure from massive datasets.</li> </ul> <p>We also saw why language is hard to model: ambiguity, long-range context, sarcasm, world knowledge, and constant change make it a truly challenging AI problem.</p> <p>Finally, we built a tiny predictive language model using bigrams, showing how early models were constructed and how prediction from context works\u2014even at the simplest level.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.8%20Key%20Takeaways.html","title":"1.8 Key Takeaways","text":"<p>Perfect! Here's the Key Takeaways section for Chapter 1, written to reinforce the main concepts clearly and memorably.</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.8%20Key%20Takeaways.html#8-key-takeaways","title":"8. Key Takeaways","text":"<ul> <li> <p>\u2705 A language model predicts the next word or sequence of words based on previous context. It\u2019s the foundation of modern NLP systems like ChatGPT, Google Translate, and more.</p> </li> <li> <p>\ud83e\uddf1 Language modeling has evolved from:</p> </li> <li> <p>Rule-based systems (hard-coded grammar),</p> </li> <li>to statistical models (like n-grams),</li> <li> <p>to neural networks (which learn meaning and context).</p> </li> <li> <p>\ud83d\udd0d Modeling language is hard because of:</p> </li> <li> <p>Ambiguity (multiple meanings),</p> </li> <li>Long context (understanding across sentences),</li> <li>Emotion &amp; tone (sarcasm, humor),</li> <li>World knowledge (things not explicitly said),</li> <li> <p>Constant change (new words, slang, trends).</p> </li> <li> <p>\ud83e\uddea Even simple models like bigrams can teach the basic principles of prediction in language. But modern LLMs use deep neural architectures to handle these challenges at scale.</p> </li> <li> <p>\ud83d\udca1 The ability to model language is not just a technical feat\u2014it\u2019s a window into replicating human understanding and communication using machines.</p> </li> </ul> <p>Would you like to add a Quiz / Exercises section to wrap up the chapter? (It\u2019s optional per your structure, but can be a great retention tool!)</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises.html","title":"1.9 Quiz Exercises","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises.html#9-quiz-exercises","title":"9. Quiz / Exercises","text":""},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises.html#concept-check-multiple-choice","title":"\ud83e\udde0 Concept Check (Multiple Choice)","text":"<p>1. What does a language model do? A. Translates languages B. Predicts the next word(s) in a sentence C. Checks grammar D. Extracts keywords from text \ud83d\udc49 Answer: B</p> <p>2. Which of the following is a limitation of rule-based language models? A. They can\u2019t run on computers B. They require too much training data C. They can\u2019t handle ambiguity or new patterns D. They use too much GPU \ud83d\udc49 Answer: C</p> <p>3. In a bigram model, the next word is predicted based on: A. The previous two words B. The last letter of the previous word C. The most recent single word D. Random guessing \ud83d\udc49 Answer: C</p> <p>4. Which of these is not a reason language is hard to model? A. Words can have multiple meanings B. Language doesn\u2019t follow any rules at all C. Context can span across long texts D. Cultural knowledge is often required \ud83d\udc49 Answer: B</p>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises.html#coding-exercise","title":"\ud83e\uddea Coding Exercise","text":"<p>Build a trigram model!</p> <p>Modify the bigram code from earlier to create a trigram model that uses the previous two words to predict the next word.</p> <p>Hint: You\u2019ll need to store keys as word pairs, like <code>(\"I\", \"love\")</code>, and look up the third word from there.</p> <pre><code># Your starter code\ncorpus = [\n    \"I love deep learning\",\n    \"I love machine learning\",\n    \"deep learning is fun\",\n    \"machine learning is powerful\"\n]\n\n# Your task:\n# 1. Build trigram_model where key = (word1, word2), value = list of next words\n# 2. Predict the next word given a word pair like (\"I\", \"love\")\n</code></pre>"},{"location":"Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/ngrms.html","title":"Ngrms","text":"In\u00a0[21]: Copied! <pre>from collections import defaultdict, Counter\nfrom typing import List, Tuple\n</pre> from collections import defaultdict, Counter from typing import List, Tuple In\u00a0[22]: Copied! <pre># Sample corpus (can be extended)\ncorpus = [\n    \"I love deep learning\",\n    \"I love machine learning\",\n    \"deep learning is fun\",\n    \"machine learning is powerful\"\n]\n</pre> # Sample corpus (can be extended) corpus = [     \"I love deep learning\",     \"I love machine learning\",     \"deep learning is fun\",     \"machine learning is powerful\" ] In\u00a0[23]: Copied! <pre># Tokenize sentences into words\ndef tokenize(corpus: List[str]) -&gt; List[List[str]]:\n    return [sentence.lower().split() for sentence in corpus]\n</pre> # Tokenize sentences into words def tokenize(corpus: List[str]) -&gt; List[List[str]]:     return [sentence.lower().split() for sentence in corpus]  In\u00a0[24]: Copied! <pre># Generate n-grams\ndef generate_ngrams(tokens: List[List[str]], n: int) -&gt; List[Tuple[str, ...]]:\n    ngrams = []\n    for sentence in tokens:\n        if len(sentence) &gt;= n:\n            for i in range(len(sentence) - n + 1):\n                ngrams.append(tuple(sentence[i:i+n]))\n    return ngrams\n</pre> # Generate n-grams def generate_ngrams(tokens: List[List[str]], n: int) -&gt; List[Tuple[str, ...]]:     ngrams = []     for sentence in tokens:         if len(sentence) &gt;= n:             for i in range(len(sentence) - n + 1):                 ngrams.append(tuple(sentence[i:i+n]))     return ngrams In\u00a0[25]: Copied! <pre># Build n-gram model (frequency based)\ndef build_ngram_model(ngrams: List[Tuple[str, ...]]) -&gt; Counter:\n    return Counter(ngrams)\n</pre> # Build n-gram model (frequency based) def build_ngram_model(ngrams: List[Tuple[str, ...]]) -&gt; Counter:     return Counter(ngrams) In\u00a0[26]: Copied! <pre># Tokenize corpus\ntokenized = tokenize(corpus)\n</pre> # Tokenize corpus tokenized = tokenize(corpus) In\u00a0[27]: Copied! <pre># Generate unigram, bigram, trigram models\nunigrams = generate_ngrams(tokenized, 1)\nbigrams = generate_ngrams(tokenized, 2)\ntrigrams = generate_ngrams(tokenized, 3)\n\nprint(\"Unigrams:\", unigrams)\nprint(\"Unigrams:\", bigrams)\nprint(\"Trigrams:\", trigrams)\n</pre> # Generate unigram, bigram, trigram models unigrams = generate_ngrams(tokenized, 1) bigrams = generate_ngrams(tokenized, 2) trigrams = generate_ngrams(tokenized, 3)  print(\"Unigrams:\", unigrams) print(\"Unigrams:\", bigrams) print(\"Trigrams:\", trigrams) <pre>Unigrams: [('i',), ('love',), ('deep',), ('learning',), ('i',), ('love',), ('machine',), ('learning',), ('deep',), ('learning',), ('is',), ('fun',), ('machine',), ('learning',), ('is',), ('powerful',)]\nUnigrams: [('i', 'love'), ('love', 'deep'), ('deep', 'learning'), ('i', 'love'), ('love', 'machine'), ('machine', 'learning'), ('deep', 'learning'), ('learning', 'is'), ('is', 'fun'), ('machine', 'learning'), ('learning', 'is'), ('is', 'powerful')]\nTrigrams: [('i', 'love', 'deep'), ('love', 'deep', 'learning'), ('i', 'love', 'machine'), ('love', 'machine', 'learning'), ('deep', 'learning', 'is'), ('learning', 'is', 'fun'), ('machine', 'learning', 'is'), ('learning', 'is', 'powerful')]\n</pre> In\u00a0[28]: Copied! <pre># Build models\nunigram_model = build_ngram_model(unigrams)\nbigram_model = build_ngram_model(bigrams)\ntrigram_model = build_ngram_model(trigrams)\nprint(\"Unigram Model:\", unigram_model)\nprint(\"Bigram Model:\", bigram_model)\nprint(\"Trigram Model:\", trigram_model)\n</pre> # Build models unigram_model = build_ngram_model(unigrams) bigram_model = build_ngram_model(bigrams) trigram_model = build_ngram_model(trigrams) print(\"Unigram Model:\", unigram_model) print(\"Bigram Model:\", bigram_model) print(\"Trigram Model:\", trigram_model)   <pre>Unigram Model: Counter({('learning',): 4, ('i',): 2, ('love',): 2, ('deep',): 2, ('machine',): 2, ('is',): 2, ('fun',): 1, ('powerful',): 1})\nBigram Model: Counter({('i', 'love'): 2, ('deep', 'learning'): 2, ('machine', 'learning'): 2, ('learning', 'is'): 2, ('love', 'deep'): 1, ('love', 'machine'): 1, ('is', 'fun'): 1, ('is', 'powerful'): 1})\nTrigram Model: Counter({('i', 'love', 'deep'): 1, ('love', 'deep', 'learning'): 1, ('i', 'love', 'machine'): 1, ('love', 'machine', 'learning'): 1, ('deep', 'learning', 'is'): 1, ('learning', 'is', 'fun'): 1, ('machine', 'learning', 'is'): 1, ('learning', 'is', 'powerful'): 1})\n</pre> In\u00a0[29]: Copied! <pre># Show top 5 most common in each model\ntop_unigrams = unigram_model.most_common(5)\ntop_bigrams = bigram_model.most_common(5)\ntop_trigrams = trigram_model.most_common(5)\ntop_unigrams, top_bigrams, top_trigrams\n</pre> # Show top 5 most common in each model top_unigrams = unigram_model.most_common(5) top_bigrams = bigram_model.most_common(5) top_trigrams = trigram_model.most_common(5) top_unigrams, top_bigrams, top_trigrams Out[29]: <pre>([(('learning',), 4),\n  (('i',), 2),\n  (('love',), 2),\n  (('deep',), 2),\n  (('machine',), 2)],\n [(('i', 'love'), 2),\n  (('deep', 'learning'), 2),\n  (('machine', 'learning'), 2),\n  (('learning', 'is'), 2),\n  (('love', 'deep'), 1)],\n [(('i', 'love', 'deep'), 1),\n  (('love', 'deep', 'learning'), 1),\n  (('i', 'love', 'machine'), 1),\n  (('love', 'machine', 'learning'), 1),\n  (('deep', 'learning', 'is'), 1)])</pre> In\u00a0[37]: Copied! <pre># Function to predict next word based on n-gram model\ndef predict_next_word(ngram_model: Counter, context: Tuple[str, ...], n: int) -&gt; List[Tuple[str, int]]:\n    predictions = []\n    for ngram, count in ngram_model.items():\n        if ngram[:-1] == context:\n            predictions.append((ngram[-1], count))\n    return sorted(predictions, key=lambda x: x[1], reverse=True)\n</pre> # Function to predict next word based on n-gram model def predict_next_word(ngram_model: Counter, context: Tuple[str, ...], n: int) -&gt; List[Tuple[str, int]]:     predictions = []     for ngram, count in ngram_model.items():         if ngram[:-1] == context:             predictions.append((ngram[-1], count))     return sorted(predictions, key=lambda x: x[1], reverse=True)  In\u00a0[41]: Copied! <pre># Example usage of prediction\ncontext_bigram = (\"deep\", )\npredictions = predict_next_word(bigram_model, context_bigram, 2)\nprint(\"Predictions for context\", context_bigram, \":\", predictions)\n</pre> # Example usage of prediction context_bigram = (\"deep\", ) predictions = predict_next_word(bigram_model, context_bigram, 2) print(\"Predictions for context\", context_bigram, \":\", predictions)  <pre>Predictions for context ('deep',) : [('learning', 2)]\n</pre> In\u00a0[42]: Copied! <pre># Example usage of prediction for trigram\ncontext_trigram = (\"machine\", \"learning\", )\npredictions_trigram = predict_next_word(trigram_model, context_trigram, 3)\nprint(\"Predictions for context\", context_trigram, \":\", predictions_trigram)\n</pre> # Example usage of prediction for trigram context_trigram = (\"machine\", \"learning\", ) predictions_trigram = predict_next_word(trigram_model, context_trigram, 3) print(\"Predictions for context\", context_trigram, \":\", predictions_trigram)  <pre>Predictions for context ('machine', 'learning') : [('is', 1)]\n</pre> In\u00a0[44]: Copied! <pre># Example usage of prediction for unigram\ncontext_unigram = (\"i\",)\npredictions_unigram = predict_next_word(bigram_model, context_unigram, 1)\nprint(\"Predictions for context\", context_unigram, \":\", predictions_unigram)\n</pre> # Example usage of prediction for unigram context_unigram = (\"i\",) predictions_unigram = predict_next_word(bigram_model, context_unigram, 1) print(\"Predictions for context\", context_unigram, \":\", predictions_unigram)  <pre>Predictions for context ('i',) : [('love', 2)]\n</pre> In\u00a0[34]: Copied! <pre># Example usage of prediction for bigram\ncontext_bigram = (\"i\", \"love\")\npredictions_bigram = predict_next_word(bigram_model, context_bigram, 2)\n</pre> # Example usage of prediction for bigram context_bigram = (\"i\", \"love\") predictions_bigram = predict_next_word(bigram_model, context_bigram, 2)"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html","title":"1 Introduction: What is Classical NLP?","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html#21-introduction-what-is-classical-nlp","title":"2.1 Introduction: What is Classical NLP?","text":"<p>Before we dive into the world of deep learning and giant language models like GPT, it\u2019s worth taking a step back and asking: How did we get here? How did computers handle language before neural networks became popular?</p> <p>The answer lies in a field known as classical Natural Language Processing (NLP)\u2014a collection of techniques developed over decades, which gave computers the first taste of understanding human language.</p> <p>These methods didn\u2019t rely on huge models or vast amounts of data. Instead, they used clever rules, basic statistics, and a fair amount of linguistic knowledge to process and analyze text. In fact, many tasks that we now perform with deep learning were once handled quite well using these older approaches. Detecting spam emails, tagging parts of speech in a sentence, or even doing simple sentiment analysis\u2014all of this was possible without transformers or GPUs.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html#why-should-you-learn-classical-nlp","title":"Why Should You Learn Classical NLP?","text":"<p>You might be wondering: If modern language models can do all of this and more, why should we bother with the old ways?</p> <p>That\u2019s a great question\u2014and one that many newcomers to the field ask. The truth is, classical NLP is still incredibly valuable for a number of reasons:</p> <ul> <li> <p>It teaches the basics. Before we can appreciate the sophistication of large models, we need to understand the simple tools they build upon\u2014things like breaking a sentence into words (tokenization), reducing words to their root forms (stemming), or representing text with numbers (TF-IDF).</p> </li> <li> <p>It\u2019s efficient and interpretable. Classical techniques often work faster, use less memory, and produce results that are easier to understand. If you're working on a small project or a problem where interpretability is important, these methods might be the perfect fit.</p> </li> <li> <p>It\u2019s still used today. Many real-world systems, especially those running in resource-constrained environments, continue to rely on classical NLP techniques. Even modern models often include preprocessing steps from the classical toolkit.</p> </li> </ul> <p>In short, classical NLP is like learning the fundamentals of music before composing symphonies with an orchestra of AI instruments.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html#what-will-you-learn-in-this-chapter","title":"What Will You Learn in This Chapter?","text":"<p>In this chapter, we\u2019ll build your classical NLP foundation step by step. We\u2019ll begin with text preprocessing, where you'll learn how to clean and prepare raw text. You\u2019ll meet concepts like tokenization, stemming, lemmatization, and part-of-speech tagging\u2014the essential building blocks of language analysis.</p> <p>Next, we\u2019ll explore how to represent text as numbers using approaches like the Bag-of-Words model and TF-IDF. These models might seem simple, but they paved the way for everything that came later.</p> <p>Then, we\u2019ll see how these elements come together in rule-based and statistical NLP pipelines\u2014the traditional ways of solving tasks like text classification.</p> <p>Finally, we\u2019ll reflect on the limitations of these classical methods, and understand why the world moved toward machine learning and, eventually, deep learning.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html#a-quick-analogy-the-toolbox-vs-the-robot-brain","title":"A Quick Analogy: The Toolbox vs. the Robot Brain","text":"<p>Imagine classical NLP as a well-stocked toolbox. You\u2019ve got scissors to cut out words, a measuring tape to count their frequency, and glue to piece ideas together. You, the human, must decide what tools to use and when.</p> <p>Now contrast that with modern LLMs like GPT, which act like a robot with a brain. It has learned to use the tools, anticipate your needs, and build language understanding on its own.</p> <p>But here\u2019s the catch\u2014if you don\u2019t know what\u2019s in the toolbox, you won\u2019t know how the robot works, how to fix it, or when a simpler tool would do the job better.</p> <p>This chapter teaches you that toolbox.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html","title":"2 Text Preprocessing: Cleaning Up Language","text":"<p>Great! Let\u2019s dive into the heart of classical NLP by beginning with one of its most important and practical components:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#22-text-preprocessing-cleaning-up-language","title":"2.2 Text Preprocessing: Cleaning Up Language","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#why-do-we-need-to-preprocess-text","title":"\ud83e\uddf9 Why Do We Need to Preprocess Text?","text":"<p>Human language is messy.</p> <p>We speak in slang, write with typos, and use punctuation, emojis, or abbreviations in all kinds of unpredictable ways. Computers, unfortunately, aren\u2019t great at handling this mess\u2014at least not without help.</p> <p>Before any analysis or modeling can be done, we need to clean and prepare the text. This is what text preprocessing is all about: transforming raw, unstructured text into a cleaner, more structured format that a machine can understand.</p> <p>Think of it like preparing vegetables before cooking. You need to wash, peel, and chop them into usable pieces before they go into the pan. Preprocessing text is no different.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#221-basic-text-cleaning-steps","title":"\ud83d\udd27 2.2.1 Basic Text Cleaning Steps","text":"<p>Here are some common operations in the text preprocessing toolbox:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#1-lowercasing","title":"1. Lowercasing","text":"<p>Most NLP systems treat <code>Python</code>, <code>PYTHON</code>, and <code>python</code> as different words. But in most cases, we want to treat them the same.</p> <pre><code>text = \"Python is Great!\"\ntext = text.lower()  # Output: \"python is great!\"\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#2-removing-punctuation","title":"2. Removing Punctuation","text":"<p>Punctuation marks don\u2019t always carry meaning in basic NLP tasks. For simplicity, we often strip them away.</p> <pre><code>import string\n\ntext = \"Hello, world! How's everything?\"\ntext = text.translate(str.maketrans('', '', string.punctuation))\n# Output: \"Hello world Hows everything\"\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#3-removing-stopwords","title":"3. Removing Stopwords","text":"<p>Stopwords are common words like \u201cthe\u201d, \u201cis\u201d, and \u201cin\u201d that occur frequently but carry little meaning in isolation. Removing them can reduce noise in the data.</p> <pre><code>from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntext = \"This is a simple sentence.\"\ntokens = word_tokenize(text.lower())\nfiltered = [word for word in tokens if word not in stopwords.words('english')]\n# Output: ['simple', 'sentence']\n</code></pre> <p>\ud83d\udccc Note: In more advanced models, stopwords are often kept, but for classical NLP, removing them is common.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#222-tokenization-breaking-text-into-pieces","title":"\u2702\ufe0f 2.2.2 Tokenization: Breaking Text into Pieces","text":"<p>Imagine you're a chef chopping a vegetable into pieces\u2014that\u2019s what tokenization does to text.</p> <p>It breaks a paragraph or sentence into smaller parts called tokens\u2014usually words, sometimes sentences or even characters.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#word-tokenization","title":"Word Tokenization","text":"<p>Splitting a sentence into individual words.</p> <pre><code>from nltk.tokenize import word_tokenize\n\ntext = \"Natural Language Processing is fun!\"\ntokens = word_tokenize(text)\n# Output: ['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#sentence-tokenization","title":"Sentence Tokenization","text":"<p>Splitting a paragraph into sentences.</p> <pre><code>from nltk.tokenize import sent_tokenize\n\nparagraph = \"Hello world. NLP is interesting. Let's learn more!\"\nsentences = sent_tokenize(paragraph)\n# Output: ['Hello world.', 'NLP is interesting.', \"Let's learn more!\"]\n</code></pre> <p>\ud83e\udde0 Tokenization is the first step in almost every NLP task. Without it, a computer wouldn\u2019t know what pieces of text to work with.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#223-stemming-and-lemmatization","title":"\ud83c\udf31 2.2.3 Stemming and Lemmatization","text":"<p>After tokenization, we often want to reduce words to their base form, so that \u201crunning\u201d, \u201cruns\u201d, and \u201cran\u201d all point to the same root idea.</p> <p>There are two ways to do this:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#stemming-a-crude-cutter","title":"Stemming \u2013 A Crude Cutter","text":"<p>Stemming is like chopping off parts of words using rules, without caring much for grammar. It may not produce real words.</p> <pre><code>from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nwords = [\"running\", \"runner\", \"ran\"]\nstems = [stemmer.stem(word) for word in words]\n# Output: ['run', 'runner', 'ran']\n</code></pre> <p>\u26a0\ufe0f Notice that \u201crunner\u201d stays the same and \u201cran\u201d isn\u2019t stemmed to \u201crun\u201d.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#lemmatization-a-smarter-approach","title":"Lemmatization \u2013 A Smarter Approach","text":"<p>Lemmatization uses a dictionary and part-of-speech tags to return the correct base word, called the lemma.</p> <pre><code>from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize(\"running\", pos=\"v\")  # Output: 'run'\nlemmatizer.lemmatize(\"better\", pos=\"a\")   # Output: 'good'\n</code></pre> <p>\u2705 Lemmatization is generally better for real applications, but it\u2019s slower and requires more information.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#224-part-of-speech-pos-tagging","title":"\ud83d\udd20 2.2.4 Part-of-Speech (POS) Tagging","text":"<p>Part-of-speech tagging assigns each word a role in the sentence: is it a noun, verb, adjective, or something else?</p> <p>Why is this useful?</p> <ul> <li>It helps lemmatizers decide the correct root form.</li> <li>It enables rule-based text analysis.</li> <li>It helps in parsing and understanding sentence structure.</li> </ul> <pre><code>import nltk\nfrom nltk import pos_tag, word_tokenize\n\nsentence = \"The quick brown fox jumps over the lazy dog\"\ntokens = word_tokenize(sentence)\npos_tags = pos_tag(tokens)\n# Output: [('The', 'DT'), ('quick', 'JJ'), ..., ('dog', 'NN')]\n</code></pre> <p>\ud83e\uddfe POS tags like <code>NN</code> (noun), <code>VB</code> (verb), <code>JJ</code> (adjective) are based on the Penn Treebank tagging scheme.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#figure-basic-nlp-preprocessing-pipeline","title":"\ud83d\udcca Figure: Basic NLP Preprocessing Pipeline","text":"<p>Let\u2019s visualize what we\u2019ve just learned with a simple diagram:</p> <pre><code>Raw Text\n   \u2193\nLowercase \u2192 Remove Punctuation \u2192 Tokenize\n   \u2193                   \u2193\nStopword Removal   POS Tagging\n   \u2193                   \u2193\nStemming / Lemmatization\n   \u2193\nCleaned Tokens\n</code></pre> <p>This pipeline is the foundation of many NLP tasks\u2014from search engines to chatbots. Master it, and you\u2019re well on your way.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#detailed-look-stemming-lemmatization-pos-tagging","title":"\ud83d\udd0d Detailed Look: Stemming, Lemmatization &amp; POS Tagging","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#stemming-cutting-without-thinking-too-much","title":"\ud83c\udf3f Stemming \u2013 Cutting Without Thinking Too Much","text":"<p>Stemming is like using a machete to chop off the ends of words. It works by applying predefined rules that remove common suffixes (like <code>-ing</code>, <code>-ed</code>, <code>-ly</code>) to reduce a word to its stem\u2014not necessarily a real word, just a root form.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#how-it-works-internally","title":"\u2699\ufe0f How it works internally:","text":"<p>Take the Porter Stemmer, one of the most popular stemmers.</p> <p>It applies a sequence of steps like:</p> <ol> <li> <p>Step 1: Remove plurals and past participles:</p> </li> <li> <p><code>caresses \u2192 caress</code>, <code>ponies \u2192 poni</code>, <code>ties \u2192 ti</code></p> </li> <li> <p>Step 2: Remove common suffixes:</p> </li> <li> <p><code>running \u2192 run</code>, <code>hopping \u2192 hop</code></p> </li> <li> <p>Step 3: Apply transformation rules:</p> </li> <li> <p><code>national \u2192 nation</code>, <code>relational \u2192 relation</code></p> </li> <li> <p>Step 4+: Continue trimming based on patterns until a minimal root is found.</p> </li> </ol> <p>Each rule is hand-crafted using pattern matching (like regular expressions) and applied in a fixed order.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#example-breakdown","title":"\ud83e\udde0 Example Breakdown:","text":"<pre><code>from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nprint(stemmer.stem(\"studies\"))  # Output: \"studi\"\nprint(stemmer.stem(\"studying\")) # Output: \"studi\"\nprint(stemmer.stem(\"study\"))    # Output: \"studi\"\n</code></pre> <p>Even though all three words mean similar things, \u201cstudy\u201d is turned into \u201cstudi\u201d, which is not a valid word. That\u2019s okay for a stemmer\u2014it\u2019s not aiming for perfect grammar, just consistency in representation.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#lemmatization-cutting-with-grammar-and-a-dictionary","title":"\ud83d\udcda Lemmatization \u2013 Cutting with Grammar and a Dictionary","text":"<p>Lemmatization is more intelligent than stemming. It tries to reduce a word to its lemma\u2014the actual dictionary form of the word.</p> <p>It doesn\u2019t just strip suffixes blindly; instead, it looks up the word in a lexicon and considers its part of speech (POS).</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#how-it-works-internally_1","title":"\u2699\ufe0f How it works internally:","text":"<ol> <li>Token Identification: Take the word and its POS (noun, verb, etc.).</li> <li>Morphological Analysis: Use rules to figure out if the word can be inflected or transformed.</li> <li>Dictionary Lookup: Check a large database (like WordNet) for the base form.</li> <li>Return Lemma: If a match is found, return the base form. Otherwise, return the original word.</li> </ol>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#example-breakdown_1","title":"\ud83e\udde0 Example Breakdown:","text":"<pre><code>from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: \"good\"\nprint(lemmatizer.lemmatize(\"running\", pos=\"v\")) # Output: \"run\"\n</code></pre> <p>Without the POS:</p> <pre><code>print(lemmatizer.lemmatize(\"running\"))  # Output: \"running\"\n</code></pre> <p>Without knowing whether \"running\" is a verb or a noun, the lemmatizer defaults to treating it as a noun\u2014which doesn't change.</p> <p>That\u2019s why lemmatization often needs POS tagging first. Which brings us to\u2026</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#part-of-speech-pos-tagging-labeling-words-with-their-roles","title":"\ud83c\udff7\ufe0f Part-of-Speech (POS) Tagging \u2013 Labeling Words with Their Roles","text":"<p>POS tagging is the task of assigning a part of speech to each word in a sentence: is it a noun, verb, adjective, etc.?</p> <p>This matters because the same word can play different roles:</p> <ul> <li>\u201cHe can book a room.\u201d \u2192 Verb</li> <li>\u201cRead a good book.\u201d \u2192 Noun</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#how-pos-taggers-work-internally","title":"\u2699\ufe0f How POS Taggers Work Internally","text":"<p>There are three main approaches:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#1-rule-based-tagging","title":"1. Rule-Based Tagging","text":"<ul> <li>Uses hand-written grammar rules.</li> <li>For example: If a word ends in -ly, it\u2019s probably an adverb.</li> <li>Example Rule:   <code>\"if word ends in 'ing' and follows a verb \u2192 likely a gerund\"</code></li> </ul> <p>\u2705 Simple, interpretable \u274c Fragile, limited coverage</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#2-statistical-tagging","title":"2. Statistical Tagging","text":"<ul> <li>Uses probabilistic models trained on labeled data (like the Penn Treebank).</li> <li> <p>Example: Hidden Markov Models (HMMs)</p> </li> <li> <p>Model how likely each word is to follow another.</p> </li> <li>Use transition probabilities between tags and emission probabilities of words given a tag.</li> </ul> <pre><code>Previous tag: DT (determiner)\nWord: 'book'\n\u2192 Probabilities:\n   NN (noun) = 0.75\n   VB (verb) = 0.25\n</code></pre> <p>\u2705 Adaptable to many domains \u274c Requires training data</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#3-neural-tagging-modern","title":"3. Neural Tagging (Modern)","text":"<ul> <li>Uses deep learning (e.g., BiLSTM, Transformers).</li> <li>Captures both context and word semantics.</li> <li>Often uses pretrained word embeddings or contextual embeddings.</li> </ul> <p>\u2705 Highly accurate \u274c Overkill for classical NLP</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#example-with-nltk","title":"\ud83e\udde0 Example with NLTK:","text":"<pre><code>import nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nfrom nltk import pos_tag, word_tokenize\n\nsentence = \"The old man the boats.\"\ntokens = word_tokenize(sentence)\ntags = pos_tag(tokens)\n# Output: [('The', 'DT'), ('old', 'JJ'), ('man', 'VB'), ('the', 'DT'), ('boats', 'NNS')]\n</code></pre> <p>In this grammatically tricky sentence, \u201cman\u201d is correctly tagged as a verb, not a noun\u2014showing that context matters!</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html#summary-of-differences","title":"Summary of Differences","text":"Feature Stemming Lemmatization POS Tagging Goal Crude root extraction Meaningful base word Identify word type (noun, etc.) Method Rule-based suffix stripping Dictionary + grammar rules Rule-based, statistical, or neural Accuracy Low to medium High (if POS given) Varies by method Output Example \"studying\" \u2192 \"studi\" \"studying\" \u2192 \"study\" \"run\" \u2192 VB (verb), NN (noun)"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html","title":"3 Why Machines Need Numbers for clarity?","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#23-representing-text-with-numbers","title":"2.3 Representing Text with Numbers","text":"<p>Before we explore the models like Bag-of-Words or TF-IDF, let\u2019s take a step back and ask a simple but important question:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#230-why-machines-need-numbers","title":"2.3.0 Why Machines Need Numbers","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#computers-dont-understand-wordsthey-understand-numbers","title":"\ud83e\udde0 Computers Don\u2019t Understand Words\u2014They Understand Numbers","text":"<p>Humans are great with words. We understand meaning, tone, sarcasm, and context. But machines? They don\u2019t see words at all.</p> <p>To a computer, the sentence:</p> <p>\u201cNatural language is amazing.\u201d</p> <p>...is just a string of characters\u2014meaningless symbols like <code>\"N\"</code>, <code>\"a\"</code>, <code>\"t\"</code>, <code>\"u\"</code>.</p> <p>So before a machine can analyze, compare, or make predictions on text, we must translate that text into a form it can understand: numbers.</p> <p>This process is called vectorization\u2014turning text into numerical vectors.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#why-not-just-use-a-dictionary","title":"\ud83d\udce6 Why Not Just Use a Dictionary?","text":"<p>You might think:</p> <p>Can\u2019t we just assign each word an ID?</p> <p>For example:</p> <ul> <li><code>\"natural\"</code> \u2192 1</li> <li><code>\"language\"</code> \u2192 2</li> <li><code>\"is\"</code> \u2192 3</li> <li><code>\"amazing\"</code> \u2192 4</li> </ul> <p>That\u2019s called integer encoding, and it's sometimes useful.</p> <p>But this alone doesn\u2019t help much.</p> <p>\ud83e\udd14 The problem? It treats all words as equally unrelated. There\u2019s no way for a model to know that <code>\"amazing\"</code> and <code>\"great\"</code> are more similar than <code>\"amazing\"</code> and <code>\"banana\"</code>.</p> <p>Also, order doesn\u2019t matter and meaning is ignored.</p> <p>So instead of just assigning IDs, we need representations that:</p> <ul> <li>Capture frequency (how often a word appears)</li> <li>Capture importance (how useful a word is in a document)</li> <li>Eventually, capture meaning and context (as we\u2019ll see in later chapters)</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#from-words-to-vectors","title":"\ud83d\udd01 From Words to Vectors","text":"<p>What we really want is a way to represent text as vectors: Ordered lists of numbers that can be compared, clustered, or classified.</p> <p>These vectors are:</p> <ul> <li>Input to machine learning models</li> <li>Mathematically analyzable</li> <li>The bridge between language and logic</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html#where-were-headed","title":"\ud83e\udded Where We\u2019re Headed","text":"<p>In the rest of this section, you\u2019ll explore two powerful and foundational methods for turning words into numbers:</p> <ul> <li>Bag-of-Words \u2013 which counts word occurrences</li> <li>TF-IDF \u2013 which scores words based on how unique and important they are</li> </ul> <p>These techniques don\u2019t \u201cunderstand\u201d language\u2014but they\u2019re good enough to power spam filters, search engines, and many real-world applications.</p> <p>In the next chapter, we\u2019ll move from counting words to understanding meaning using word embeddings\u2014but for now, let\u2019s master these classical representations first.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html","title":"3.1 The Bag of Words (BoW) Model","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#231-the-bag-of-words-bow-model","title":"2.3.1 The Bag-of-Words (BoW) Model","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#what-is-the-bag-of-words-model","title":"\ud83d\udce6 What is the Bag-of-Words Model?","text":"<p>Imagine you\u2019re asked to describe a sentence, but instead of giving its meaning or structure, you just count how many times each word appears.</p> <p>That\u2019s exactly what the Bag-of-Words (BoW) model does. It treats a piece of text like a bag of individual words\u2014ignoring grammar and word order\u2014and just records word frequency.</p> <p>Why? Because in many NLP tasks (like spam detection, topic classification, or document similarity), the presence or absence of certain words can be more important than how the sentence is written.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#intuition","title":"\ud83c\udfa8 Intuition","text":"<p>Let\u2019s take three simple sentences (documents):</p> <pre><code>Doc 1: \"I love NLP\"\nDoc 2: \"I love machine learning\"\nDoc 3: \"NLP is fun\"\n</code></pre> <p>Our first job is to build a vocabulary\u2014a list of all unique words found in all documents.</p> <pre><code>Vocabulary:\n[\"I\", \"love\", \"NLP\", \"machine\", \"learning\", \"is\", \"fun\"]\n</code></pre> <p>Next, we turn each document into a vector (a list of numbers), where each number represents how often a word from the vocabulary appears in that document.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#example-bow-matrix","title":"\ud83e\uddee Example: BoW Matrix","text":"<p>Here\u2019s how the model represents our three documents numerically:</p> Word Doc 1 Doc 2 Doc 3 I 1 1 0 love 1 1 0 NLP 1 0 1 machine 0 1 0 learning 0 1 0 is 0 0 1 fun 0 0 1 <p>Each document becomes a vector, where each dimension corresponds to a word in the vocabulary.</p> <p>\ud83d\udd0d This is a simple way to \u201cnumerically describe\u201d a piece of text.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#code-example-bow-with-scikit-learn","title":"\ud83d\udcbb Code Example: BoW with Scikit-learn","text":"<p>Let\u2019s implement this in Python using <code>CountVectorizer</code> from <code>scikit-learn</code>.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\n# Our tiny corpus\ncorpus = [\n    \"I love NLP\",\n    \"I love machine learning\",\n    \"NLP is fun\"\n]\n\n# Create the vectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the corpus\nbow_matrix = vectorizer.fit_transform(corpus)\n\n# View the vocabulary\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\n\n# Convert the BoW matrix to array format\nimport pandas as pd\ndf = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#output","title":"\ud83d\udce4 Output:","text":"<pre><code>Vocabulary: ['fun' 'is' 'learning' 'love' 'machine' 'nlp']\n</code></pre> fun is learning love machine nlp Doc 1 0 0 0 1 0 1 Doc 2 0 0 1 1 1 0 Doc 3 1 1 0 0 0 1"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#limitations-of-bag-of-words","title":"\ud83d\udea7 Limitations of Bag-of-Words","text":"<p>The BoW model is easy and effective, but it comes with some major drawbacks:</p> <ul> <li> <p>Ignores word order:   \u201cI love NLP\u201d vs. \u201cNLP loves I\u201d \u2192 same vector.</p> </li> <li> <p>No understanding of meaning:   \u201cgreat\u201d, \u201cexcellent\u201d, and \u201cgood\u201d are treated as completely different.</p> </li> <li> <p>Sparse vectors:   In real datasets with thousands of unique words, most documents will contain only a small fraction\u2014leading to lots of zeros.</p> </li> <li> <p>Loses context:   It doesn\u2019t know if words are used positively, negatively, or sarcastically.</p> </li> </ul> <p>Still, BoW remains a valuable baseline\u2014simple, fast, and surprisingly useful in many NLP tasks.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html#when-to-use-bow","title":"\ud83e\udde0 When to Use BoW","text":"<ul> <li>For quick text classification with small datasets</li> <li>When interpretability matters</li> <li>As a baseline to compare against more advanced methods</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html","title":"3.2 Term Frequency\u2013Inverse Document Frequency (TF IDF)","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#232-term-frequencyinverse-document-frequency-tf-idf","title":"2.3.2 Term Frequency\u2013Inverse Document Frequency (TF-IDF)","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#why-not-just-count-words","title":"\ud83e\udde0 Why Not Just Count Words?","text":"<p>The Bag-of-Words model is a great starting point. It\u2019s simple, fast, and often surprisingly effective. But as you saw earlier, it treats all words as equally important\u2014and that\u2019s a problem.</p> <p>Imagine you\u2019re building a document classifier, and every document starts with the phrase:</p> <p>\u201cThis document is about\u2026\u201d</p> <p>The words \u201cthis\u201d, \u201cis\u201d, and \u201cabout\u201d will show up in almost every document. As a result, they\u2019ll have high counts\u2014but they\u2019re not really helpful in telling documents apart.</p> <p>What we want is a way to give more importance to informative words and less importance to common, boring ones.</p> <p>That\u2019s exactly what TF-IDF does.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#what-is-tf-idf","title":"\ud83d\udcd0 What is TF-IDF?","text":"<p>TF-IDF stands for:</p> <ul> <li>Term Frequency (TF) \u2013 How often a word appears in a document</li> <li>Inverse Document Frequency (IDF) \u2013 How rare the word is across all documents</li> </ul> <p>By combining these two ideas, TF-IDF gives higher scores to important words (frequent in one document, rare in others), and lower scores to common words (frequent everywhere).</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#lets-break-it-down","title":"\ud83d\udd22 Let\u2019s Break It Down","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#term-frequency-tf","title":"\u2705 Term Frequency (TF)","text":"<p>This measures how often a word shows up in a document. The more frequent, the more important\u2014within that document.</p> <p>$$ \\text{TF}(w, d) = \\frac{\\text{Number of times } w \\text{ appears in } d}{\\text{Total number of words in } d} $$</p> <p>So if \u201cmachine\u201d appears 3 times in a 100-word document, its TF is <code>3 / 100 = 0.03</code>.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#inverse-document-frequency-idf","title":"\u2705 Inverse Document Frequency (IDF)","text":"<p>This measures how rare a word is across all documents in the dataset. The fewer documents it appears in, the higher its score.</p> <p>$$ \\text{IDF}(w) = \\log\\left(\\frac{N}{1 + n_w}\\right) $$</p> <p>Where:</p> <ul> <li>$N$ is the total number of documents</li> <li>$n_w$ is the number of documents containing the word $w$</li> <li>Adding 1 prevents division by zero</li> </ul> <p>So if \u201clearning\u201d appears in 2 out of 10 documents, its IDF is:</p> <p>$$ \\log\\left(\\frac{10}{1 + 2}\\right) = \\log\\left(\\frac{10}{3}\\right) \\approx 0.52 $$</p> <p>\ud83d\udd0d Common words like \u201cis\u201d or \u201cthe\u201d will have very low IDF scores. That\u2019s the magic of TF-IDF\u2014it automatically downweights them.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#tf-idf-tf-idf","title":"\u2705 TF-IDF = TF \u00d7 IDF","text":"<p>To get the final score for a word in a document, multiply its TF and IDF.</p> <p>$$ \\text{TF-IDF}(w, d) = \\text{TF}(w, d) \\times \\text{IDF}(w) $$</p> <p>This score is high when:</p> <ul> <li>The word is frequent in one document, and</li> <li>Rare in the overall dataset</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#mini-example","title":"\ud83d\udd0d Mini Example","text":"<p>Let\u2019s use the same three documents again:</p> <pre><code>Doc 1: \"I love NLP\"\nDoc 2: \"I love machine learning\"\nDoc 3: \"NLP is fun\"\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#step-1-count-term-frequencies","title":"Step 1: Count Term Frequencies","text":"<ul> <li>\u201clove\u201d appears in Doc 1 and Doc 2: TF = 1 / 3 = 0.33 (for each)</li> <li>\u201cmachine\u201d appears only in Doc 2: TF = 1 / 4 = 0.25</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#step-2-compute-idf","title":"Step 2: Compute IDF","text":"<ul> <li>Total documents = 3</li> <li>\u201clove\u201d appears in 2 docs \u2192 IDF = log(3 / (1 + 2)) = log(1) = 0.0</li> <li>\u201cmachine\u201d appears in 1 doc \u2192 IDF = log(3 / (1 + 1)) = log(1.5) \u2248 0.405</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#step-3-final-tf-idf-scores","title":"Step 3: Final TF-IDF Scores","text":"<ul> <li>\u201clove\u201d \u2192 0.33 \u00d7 0.0 = 0.0 \u2705 common word, downweighted</li> <li>\u201cmachine\u201d \u2192 0.25 \u00d7 0.405 \u2248 0.101 \u2705 rare, boosted</li> </ul> <p>\ud83d\udca1 So TF-IDF gives high scores to unique, meaningful words\u2014and low scores to generic, frequent words.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#code-example-tf-idf-in-python","title":"\ud83e\uddd1\u200d\ud83d\udcbb Code Example: TF-IDF in Python","text":"<p>Let\u2019s implement TF-IDF using <code>TfidfVectorizer</code> from Scikit-learn:</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocs = [\n    \"I love NLP\",\n    \"I love machine learning\",\n    \"NLP is fun\"\n]\n\n# Create and fit the TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(docs)\n\n# Convert to readable format\nimport pandas as pd\ndf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#output","title":"\ud83d\udce4 Output","text":"fun is learning love machine nlp Doc 1 0 0 0 0.707 0 0.707 Doc 2 0 0 0.577 0.577 0.577 0 Doc 3 0.577 0.577 0 0 0 0.577 <p>Notice:</p> <ul> <li>Words like \u201clove\u201d appear in multiple documents \u2192 lower scores</li> <li>Words like \u201cfun\u201d, \u201cmachine\u201d, and \u201clearning\u201d are more specific \u2192 higher scores</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#comparison-with-bag-of-words","title":"\u2696\ufe0f Comparison with Bag-of-Words","text":"Feature Bag-of-Words TF-IDF Uses raw word counts \u2705 Yes \u274c No Weights rare terms \u274c No \u2705 Yes Penalizes common terms \u274c No \u2705 Yes Captures meaning \u274c No \u274c Still limited Output vectors Sparse, high-dim Sparse, weighted, high-dim"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html#when-to-use-tf-idf","title":"\ud83c\udfaf When to Use TF-IDF","text":"<p>Use TF-IDF when:</p> <ul> <li>You want to highlight unique or rare words in your text</li> <li>You\u2019re doing tasks like document classification, search engines, or information retrieval</li> <li>You want to improve performance over basic BoW without jumping into deep learning</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html","title":"3.3 TF IDF: Sparse Vectors and High Dimensionality","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#233-tf-idf-sparse-vectors-and-high-dimensionality","title":"2.3.3 TF-IDF: Sparse Vectors and High Dimensionality","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#what-happens-when-vocabulary-grows","title":"\ud83e\uddee What Happens When Vocabulary Grows?","text":"<p>As you saw in the last section, TF-IDF turns a document into a vector\u2014a list of numbers, each representing a word in the vocabulary.</p> <p>That works fine for small examples.</p> <p>But what happens when you apply it to real-world datasets, like a collection of news articles or customer reviews?</p> <p>You quickly end up with:</p> <ul> <li>Thousands or even hundreds of thousands of unique words</li> <li>Very long vectors (e.g., length 50,000)</li> <li>Most values being zero</li> </ul> <p>This is where we run into two challenges:</p> <ul> <li>Sparsity</li> <li>High dimensionality</li> </ul> <p>Let\u2019s break each down.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#1-sparse-vectors-lots-of-zeros","title":"\ud83e\uddca 1. Sparse Vectors: Lots of Zeros","text":"<p>Most documents use only a tiny fraction of the total vocabulary.</p> <p>So when you convert them into vectors, most positions will be 0, indicating that the word doesn't appear in the document.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#example","title":"Example:","text":"<p>Imagine a vocabulary of 10,000 words. A short review like:</p> <p>\u201cExcellent service and friendly staff.\u201d</p> <p>might only use 5\u201310 of those words. So its TF-IDF vector will look like this:</p> <pre><code>[0, 0, 0.72, 0, 0, 0, 0.33, 0, 0, ..., 0]\n</code></pre> <p>Out of 10,000 numbers, maybe only 10 are non-zero. That\u2019s 99.9% empty.</p> <p>\ud83d\udd0d This is called a sparse vector\u2014mostly zeros with a few meaningful values.</p> <p>While this is normal in NLP, it brings computational challenges.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#2-high-dimensionality-the-curse-of-too-many-features","title":"\ud83e\uddf1 2. High Dimensionality: The Curse of Too Many Features","text":"<p>Each word in the vocabulary becomes a dimension in the feature space. With a large vocabulary, your vectors can be:</p> <ul> <li>10,000 dimensions (small dataset)</li> <li>100,000+ dimensions (large corpus)</li> </ul> <p>This is called high dimensionality, and it leads to several problems:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#key-issues","title":"\u26a0\ufe0f Key Issues:","text":"<ul> <li>Increased memory usage: Each document\u2019s vector consumes more RAM.</li> <li>Slower computation: Algorithms like clustering or classification become slower.</li> <li>Overfitting: More dimensions = more risk of the model memorizing noise.</li> <li>Harder to visualize: We can\u2019t \u201csee\u201d or interpret vectors in high-dimensional space.</li> </ul> <p>\ud83e\udde0 This is often called the curse of dimensionality in machine learning.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#what-can-we-do-about-it","title":"\ud83d\udd27 What Can We Do About It?","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#1-limit-the-vocabulary-size","title":"\u2705 1. Limit the Vocabulary Size","text":"<p>Use only the top N most frequent words (e.g., 2,000 or 10,000). This reduces vector size and focuses on useful words.</p> <pre><code>TfidfVectorizer(max_features=5000)\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#2-remove-rare-words","title":"\u2705 2. Remove Rare Words","text":"<p>Words that appear in just 1\u20132 documents don\u2019t help much. You can set a minimum document frequency (<code>min_df</code>) to filter them out.</p> <pre><code>TfidfVectorizer(min_df=5)\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#3-use-dimensionality-reduction","title":"\u2705 3. Use Dimensionality Reduction","text":"<p>Techniques like PCA (Principal Component Analysis) or Truncated SVD (Latent Semantic Analysis) can reduce TF-IDF vectors to fewer dimensions while preserving important patterns.</p> <pre><code>from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components=100)\nreduced = svd.fit_transform(tfidf_matrix)\n</code></pre> <p>This is like compressing your data while keeping its meaning\u2014similar to reducing a high-res image without losing what\u2019s important.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#sparse-bad-always","title":"\ud83d\udcc9 Sparse \u2260 Bad (Always)","text":"<p>Even though sparse vectors are inefficient in some ways, they aren\u2019t always bad.</p> <p>In fact, many algorithms (like Naive Bayes or Logistic Regression) are optimized to work well with sparse data.</p> <p>The key is to understand the trade-offs and choose preprocessing methods that match your problem and data size.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html#summary","title":"\ud83d\udca1 Summary","text":"<ul> <li>TF-IDF vectors are high-dimensional and sparse because each document is represented using a huge vocabulary.</li> <li>Sparsity means most values are 0, leading to memory and speed issues.</li> <li>Dimensionality means each document has thousands of features, making models prone to overfitting or inefficiency.</li> <li> <p>To manage this, we can:</p> </li> <li> <p>Limit the vocabulary</p> </li> <li>Remove rare words</li> <li>Apply dimensionality reduction techniques</li> </ul> <p>These strategies help you build faster, more efficient, and more generalizable NLP models.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html","title":"4 Classical NLP Pipelines","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#24-classical-nlp-pipelines","title":"2.4 Classical NLP Pipelines","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#what-is-an-nlp-pipeline","title":"\ud83d\udee0\ufe0f What Is an NLP Pipeline?","text":"<p>A Natural Language Processing (NLP) pipeline is a step-by-step process that takes raw text and turns it into something a machine learning model can understand and use.</p> <p>Think of it like a factory assembly line:</p> <ul> <li>Text comes in as raw material</li> <li>Each step processes or enriches it</li> <li>The final product is a clean, structured representation of the original text</li> </ul> <p>These pipelines form the backbone of many real-world NLP tasks like spam detection, sentiment analysis, chatbots, and more.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#typical-stages-in-a-classical-nlp-pipeline","title":"\ud83d\udd01 Typical Stages in a Classical NLP Pipeline","text":"<p>Here\u2019s what a basic classical pipeline might look like:</p> <ol> <li>Text Cleaning &amp; Normalization</li> <li>Tokenization</li> <li>Stemming or Lemmatization</li> <li>Stopword Removal</li> <li>Part-of-Speech Tagging (optional)</li> <li>Vectorization (BoW or TF-IDF)</li> <li>Machine Learning Model</li> </ol> <p>Let\u2019s walk through each step with intuitive explanations and examples.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#1-text-cleaning-normalization","title":"\u2702\ufe0f 1. Text Cleaning &amp; Normalization","text":"<p>Before you analyze anything, it helps to clean your data.</p> <p>Typical cleaning includes:</p> <ul> <li>Lowercasing all text   <code>\"Hello\" \u2192 \"hello\"</code></li> <li>Removing punctuation   <code>\"You're great!\" \u2192 \"youre great\"</code></li> <li>Removing numbers or symbols (optional)</li> </ul> <p>This step ensures consistency across the dataset.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#2-tokenization","title":"\u2728 2. Tokenization","text":"<p>Splitting text into individual words (or tokens).</p> <pre><code>from nltk.tokenize import word_tokenize\nword_tokenize(\"I love NLP!\") \n# Output: ['I', 'love', 'NLP', '!']\n</code></pre> <p>These tokens become the units we work with in all future steps.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#3-stemming-or-lemmatization","title":"\ud83c\udf31 3. Stemming or Lemmatization","text":"<p>Reducing words to a root form:</p> <ul> <li><code>running \u2192 run</code></li> <li><code>better \u2192 good</code></li> </ul> <p>You usually choose one, not both.</p> <ul> <li>Use stemming for speed and simplicity</li> <li>Use lemmatization for better accuracy</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#4-stopword-removal","title":"\ud83d\udeab 4. Stopword Removal","text":"<p>Some words are too common to be useful, like:</p> <ul> <li>\u201cthe\u201d, \u201cis\u201d, \u201cand\u201d, \u201cof\u201d, \u201ca\u201d</li> </ul> <p>Removing them reduces noise and dimensionality.</p> <pre><code>from nltk.corpus import stopwords\nstopwords.words('english')  # List of common stopwords\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#5-part-of-speech-pos-tagging-optional","title":"\ud83c\udff7\ufe0f 5. Part-of-Speech (POS) Tagging (Optional)","text":"<p>Adds grammatical meaning to words\u2014especially useful if you plan to use lemmatization or extract only certain kinds of words (e.g., nouns, verbs).</p> <pre><code>from nltk import pos_tag\npos_tag(['NLP', 'is', 'awesome'])\n# Output: [('NLP', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ')]\n</code></pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#6-vectorization-bow-or-tf-idf","title":"\ud83d\udd22 6. Vectorization (BoW or TF-IDF)","text":"<p>Convert tokens into numerical vectors using methods like:</p> <ul> <li>Bag-of-Words: raw word counts</li> <li>TF-IDF: weighted importance of words</li> </ul> <p>These vectors become the features for machine learning models.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#7-train-a-machine-learning-model","title":"\ud83d\udcca 7. Train a Machine Learning Model","text":"<p>Now you can feed the vectors into algorithms like:</p> <ul> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>Support Vector Machines</li> </ul> <p>Your model learns patterns in word usage to make predictions, like:</p> <ul> <li>Is this email spam?</li> <li>Is this review positive or negative?</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#mini-example-a-classical-pipeline-in-code","title":"\ud83e\uddea Mini Example: A Classical Pipeline in Code","text":"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Step 1: Sample data\ntexts = [\n    \"I love this movie\",\n    \"This film was awful\",\n    \"Amazing story and great characters\",\n    \"Terrible plot and boring scenes\"\n]\nlabels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n\n# Step 2: Vectorize with TF-IDF\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(texts)\n\n# Step 3: Train a model\nmodel = MultinomialNB()\nmodel.fit(X, labels)\n\n# Step 4: Predict\ntest = vectorizer.transform([\"great film\"])\nprint(model.predict(test))  # Output: [1]\n</code></pre> <p>\ud83e\udde0 This is a basic sentiment classifier built entirely from a classical pipeline\u2014no deep learning required!</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#tools-for-pipelines","title":"\ud83e\uddf0 Tools for Pipelines","text":"<p>If you're working with lots of data or want to automate these steps, libraries like spaCy and NLTK offer prebuilt pipelines.</p> <p>You can also build custom pipelines using <code>scikit-learn</code>'s <code>Pipeline</code> class:</p> <pre><code>from sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('clf', MultinomialNB())\n])\n</code></pre> <p>This makes the pipeline modular and reproducible.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html#summary","title":"\ud83c\udfc1 Summary","text":"<p>A classical NLP pipeline:</p> <ul> <li>Starts with raw text</li> <li>Processes and transforms it</li> <li>Produces a vector-based representation</li> <li>Trains a traditional machine learning model</li> </ul> <p>These pipelines were the workhorses of NLP for years\u2014and still remain valuable today in many production systems.</p> <p>In the next section, we\u2019ll reflect on the strengths and limitations of these classical methods, especially compared to modern deep learning approaches.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html","title":"5 Limitations of Rule Based and Statistical Methods","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#25-limitations-of-rule-based-and-statistical-methods","title":"2.5 Limitations of Rule-Based and Statistical Methods","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#why-move-beyond-classical-nlp","title":"\ud83e\udde0 Why Move Beyond Classical NLP?","text":"<p>Classical NLP pipelines\u2014like the ones we\u2019ve explored\u2014have powered real applications for decades. From search engines to spam filters to sentiment analysis, these methods get the job done.</p> <p>So why did the field shift toward more advanced techniques like deep learning and transformers?</p> <p>Because as useful as rule-based and statistical approaches are, they come with important limitations\u2014especially when dealing with the richness, ambiguity, and complexity of human language.</p> <p>Let\u2019s look at the main challenges.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#1-rigid-rules-dont-scale","title":"\ud83e\uddf1 1. Rigid Rules Don't Scale","text":"<p>Rule-based systems rely on manually crafted patterns\u2014like regular expressions or grammar rules.</p> <p>That\u2019s fine for small, controlled tasks. But:</p> <ul> <li>Human language is messy, irregular, and constantly evolving</li> <li>It's nearly impossible to write rules for every possible phrase or exception</li> <li>Rules break when you change language, domain, or writing style</li> </ul> <p>\u270f\ufe0f For example: A chatbot using if\u2013else rules might handle 10 questions well. But add just 100 more and the logic becomes unmanageable.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#2-no-deep-understanding","title":"\u274c 2. No Deep Understanding","text":"<p>Classical statistical methods (like BoW and TF-IDF):</p> <ul> <li>Don\u2019t know what words mean</li> <li>Can\u2019t understand context</li> <li>Treat words like independent tokens</li> </ul> <p>That means they can\u2019t tell:</p> <ul> <li>\u201cI love this movie\u201d from \u201cI don\u2019t love this movie\u201d</li> <li>\u201cHe banked the money\u201d from \u201cHe sat on the river bank\u201d</li> </ul> <p>These methods miss the nuance and polysemy (words with multiple meanings) that are common in natural language.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#3-sparse-high-dimensional-representations","title":"\ud83e\uddca 3. Sparse, High-Dimensional Representations","text":"<p>As you learned earlier:</p> <ul> <li>BoW and TF-IDF produce large vectors</li> <li>Most values are zeros (sparse)</li> <li>Adding more data = more dimensions = more complexity</li> </ul> <p>This causes:</p> <ul> <li>High memory use</li> <li>Slow training and inference</li> <li>Risk of overfitting</li> </ul> <p>Modern methods solve this using dense embeddings (you\u2019ll learn about these in Chapter 3).</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#4-language-and-domain-sensitivity","title":"\ud83c\udf0d 4. Language and Domain Sensitivity","text":"<p>A model trained on:</p> <ul> <li>News articles won\u2019t perform well on social media posts</li> <li>English rules won\u2019t work on Chinese or Arabic</li> <li>Product reviews might confuse medical terms</li> </ul> <p>Rule-based and statistical methods are brittle\u2014they don\u2019t generalize well across languages, topics, or domains.</p> <p>\ud83d\udd01 Every new use case often requires rebuilding the pipeline from scratch.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#5-limited-performance-in-complex-tasks","title":"\ud83d\udcc9 5. Limited Performance in Complex Tasks","text":"<p>Classical NLP works well for:</p> <ul> <li>Spam detection</li> <li>Keyword extraction</li> <li>Simple classification</li> </ul> <p>But struggles with:</p> <ul> <li>Text summarization</li> <li>Question answering</li> <li>Language generation</li> <li>Conversational AI</li> </ul> <p>Why? Because those tasks require understanding, reasoning, and context, which classical pipelines lack.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#why-machine-learningand-later-deep-learningtook-over","title":"\ud83e\udd16 Why Machine Learning\u2014and Later Deep Learning\u2014Took Over","text":"<p>The limitations of classical NLP motivated a shift toward data-driven models that learn language patterns directly from large corpora.</p> <p>First came:</p> <ul> <li>Word embeddings (Word2Vec, GloVe)</li> <li>Neural networks (RNNs, CNNs)</li> </ul> <p>Then evolved into:</p> <ul> <li>Transformers</li> <li>Large Language Models (LLMs) like GPT, BERT, and beyond</li> </ul> <p>These models don\u2019t just match patterns\u2014they learn meanings, model context, and even generate human-like text.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html#summary","title":"\ud83c\udfc1 Summary","text":"<p>While classical NLP methods are still useful, they fall short in several key areas:</p> Limitation Description No understanding of context Words are treated in isolation Can\u2019t handle complex language Sarcasm, ambiguity, irony are missed Brittle and rule-dependent Need manual updates for every new use case Sparse and inefficient vectors BoW/TF-IDF waste memory and processing time Poor generalization Struggles to adapt to new domains or languages <p>These drawbacks laid the foundation for modern NLP with neural networks, which we\u2019ll begin exploring in the next chapter.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/E%3AA%20Classical%20Pipeline.html","title":"E:A Classical Pipeline","text":"In\u00a0[11]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB  In\u00a0[12]: Copied! <pre># Step 1: Sample data\ntexts = [\n    \"I love this movie\",\n    \"This film was awful\",\n    \"Amazing story and great characters\",\n    \"Terrible plot and boring scenes\"\n]\nlabels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n</pre> # Step 1: Sample data texts = [     \"I love this movie\",     \"This film was awful\",     \"Amazing story and great characters\",     \"Terrible plot and boring scenes\" ] labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative In\u00a0[13]: Copied! <pre># Step 2: Vectorize with TF-IDF\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(texts)\n</pre> # Step 2: Vectorize with TF-IDF vectorizer = TfidfVectorizer(stop_words='english') X = vectorizer.fit_transform(texts) In\u00a0[14]: Copied! <pre># Step 3: Train Naive Bayes classifier\nmodel = MultinomialNB()\nmodel.fit(X, labels)\n</pre> # Step 3: Train Naive Bayes classifier model = MultinomialNB() model.fit(X, labels)  Out[14]: <pre>MultinomialNB()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNB<pre>MultinomialNB()</pre> In\u00a0[15]: Copied! <pre># Step 4: Predict\ntest = vectorizer.transform([\"Amazing movie with great acting\"])\nprint(model.predict(test))  # Output: [1]\n</pre> # Step 4: Predict test = vectorizer.transform([\"Amazing movie with great acting\"]) print(model.predict(test))  # Output: [1] <pre>[1]\n</pre> In\u00a0[16]: Copied! <pre># Step 4: Predict on new data\nnew_texts = [\n    \"I really enjoyed this film\",\n    \"It was a waste of time\"\n]\nX_new = vectorizer.transform(new_texts)\npredictions = model.predict(X_new)\n# Step 5: Output predictions\nfor text, prediction in zip(new_texts, predictions):\n    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n    print(f\"Text: '{text}' | Sentiment: {sentiment}\")\n</pre> # Step 4: Predict on new data new_texts = [     \"I really enjoyed this film\",     \"It was a waste of time\" ] X_new = vectorizer.transform(new_texts) predictions = model.predict(X_new) # Step 5: Output predictions for text, prediction in zip(new_texts, predictions):     sentiment = \"Positive\" if prediction == 1 else \"Negative\"     print(f\"Text: '{text}' | Sentiment: {sentiment}\") <pre>Text: 'I really enjoyed this film' | Sentiment: Negative\nText: 'It was a waste of time' | Sentiment: Negative\n</pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/E%3ABoW%20with%20Scikit-learn.html","title":"E:BoW with Scikit learn","text":"In\u00a0[1]: Copied! <pre>from sklearn.feature_extraction.text import CountVectorizer\n\n# Our tiny corpus\ncorpus = [\n    \"I love NLP\",\n    \"I love machine learning\",\n    \"NLP is fun\"\n]\n\n# Create the vectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the corpus\nbow_matrix = vectorizer.fit_transform(corpus)\n\n# View the vocabulary\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\n\n# Convert the BoW matrix to array format\nimport pandas as pd\ndf = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n</pre> from sklearn.feature_extraction.text import CountVectorizer  # Our tiny corpus corpus = [     \"I love NLP\",     \"I love machine learning\",     \"NLP is fun\" ]  # Create the vectorizer vectorizer = CountVectorizer()  # Fit and transform the corpus bow_matrix = vectorizer.fit_transform(corpus)  # View the vocabulary print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Convert the BoW matrix to array format import pandas as pd df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out()) print(df) <pre>Vocabulary: ['fun' 'is' 'learning' 'love' 'machine' 'nlp']\n   fun  is  learning  love  machine  nlp\n0    0   0         0     1        0    1\n1    0   0         1     1        1    0\n2    1   1         0     0        0    1\n</pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/E%3ATF-IDF.html","title":"E:TF IDF","text":"In\u00a0[1]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocs = [\n    \"I love NLP\",\n    \"I love machine learning\",\n    \"NLP is fun\"\n]\n\n# Create and fit the TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(docs)\n\n# Convert to readable format\nimport pandas as pd\ndf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  docs = [     \"I love NLP\",     \"I love machine learning\",     \"NLP is fun\" ]  # Create and fit the TF-IDF vectorizer vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(docs)  # Convert to readable format import pandas as pd df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out()) print(df) <pre>        fun        is  learning      love   machine       nlp\n0  0.000000  0.000000  0.000000  0.707107  0.000000  0.707107\n1  0.000000  0.000000  0.622766  0.473630  0.622766  0.000000\n2  0.622766  0.622766  0.000000  0.000000  0.000000  0.473630\n</pre>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html","title":"Chapter","text":"<p>Great! Let's begin by outlining Chapter 2: Classical NLP Techniques in a clear, friendly, and structured way, following your book\u2019s format and style guide.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#chapter-2-classical-nlp-techniques","title":"\ud83d\udcd8 Chapter 2: Classical NLP Techniques","text":""},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#1-learning-objectives","title":"1. Learning Objectives","text":"<p>By the end of this chapter, you will:</p> <ul> <li>Understand the foundations of how machines process and analyze language.</li> <li>Learn about tokenization, stemming, lemmatization, and part-of-speech (POS) tagging.</li> <li>Explore Bag-of-Words (BoW) and TF-IDF, and how they help in text representation.</li> <li>Understand how rule-based and statistical NLP pipelines work.</li> <li>Recognize the limitations of classical NLP approaches and why more advanced models became necessary.</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#2-chapter-outline","title":"2. Chapter Outline","text":"<p>Here\u2019s a structured breakdown of the sections and subsections for this chapter:</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#21-introduction-what-is-classical-nlp","title":"2.1 Introduction: What is Classical NLP?","text":"<ul> <li>What came before large language models</li> <li>Why classical NLP is still relevant</li> <li>Overview of this chapter</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#22-text-preprocessing-cleaning-up-language","title":"2.2 Text Preprocessing: Cleaning Up Language","text":"<ul> <li>2.2.1 What is Text Preprocessing?</li> <li>2.2.2 Lowercasing, Removing Punctuation &amp; Stopwords</li> <li> <p>2.2.3 Tokenization</p> </li> <li> <p>Word tokenization</p> </li> <li>Sentence tokenization</li> <li> <p>2.2.4 Stemming vs Lemmatization</p> </li> <li> <p>Rule-based stemming (Porter Stemmer)</p> </li> <li>Dictionary-based lemmatization</li> <li> <p>2.2.5 Part-of-Speech (POS) Tagging</p> </li> <li> <p>What is POS?</p> </li> <li>Why it matters</li> </ul> <p>\ud83d\udd0d Visual: Flowchart of basic NLP preprocessing pipeline</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#23-representing-text-with-numbers","title":"2.3 Representing Text with Numbers","text":"<ul> <li>2.3.1 Why Machines Need Numbers</li> <li> <p>2.3.2 Bag-of-Words (BoW)</p> </li> <li> <p>Vocabulary building</p> </li> <li>Word count vectors</li> <li> <p>2.3.3 Term Frequency-Inverse Document Frequency (TF-IDF)</p> </li> <li> <p>Intuition behind TF and IDF</p> </li> <li>Calculating TF-IDF</li> <li>2.3.4 Vector Sparsity and Dimensionality</li> </ul> <p>\ud83d\udd22 Table: Example BoW and TF-IDF matrix for 3 simple documents</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#24-building-classical-nlp-pipelines","title":"2.4 Building Classical NLP Pipelines","text":"<ul> <li> <p>2.4.1 Rule-Based Approaches</p> </li> <li> <p>Regex, pattern-matching</p> </li> <li>Heuristics and grammar rules</li> <li> <p>2.4.2 Statistical NLP</p> </li> <li> <p>N-gram models</p> </li> <li>Basic text classification using BoW or TF-IDF</li> <li>2.4.3 Libraries and Tools (NLTK, spaCy, scikit-learn)</li> </ul> <p>\ud83d\udd0d Visual: Diagram comparing rule-based and statistical NLP pipelines</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#25-code-example-spam-detection-with-tf-idf","title":"2.5 Code Example: Spam Detection with TF-IDF","text":"<ul> <li>Load a dataset (e.g., SMS spam)</li> <li>Preprocess the text</li> <li>Convert to TF-IDF</li> <li>Train a simple classifier (e.g., Logistic Regression)</li> <li>Evaluate accuracy</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#26-summary","title":"2.6 Summary","text":"<p>A recap of all techniques introduced, highlighting how they fit together in traditional NLP workflows.</p>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#27-key-takeaways","title":"2.7 Key Takeaways","text":"<ul> <li>Bullet list of the most important insights from the chapter</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#28-quiz-exercises-optional","title":"2.8 Quiz / Exercises (Optional)","text":"<ul> <li>Fill-in-the-blanks and short coding tasks</li> <li>Think: \"Tokenization is the process of ____\"</li> <li>Code: \"Write a function to remove stopwords from a list of words.\"</li> </ul>"},{"location":"Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html#ready-to-proceed","title":"\u2705 Ready to Proceed?","text":"<p>Would you like to begin with Section 2.1: Introduction: What is Classical NLP?, or do you want to adjust or expand the outline first?</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html","title":"Code Example: Using Pretrained Word Embeddings","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#code-example-using-pretrained-word-embeddings","title":"\ud83d\udd0d Code Example: Using Pretrained Word Embeddings","text":"<p>Now that you understand how different word embedding models work under the hood, let\u2019s explore how to use pretrained embeddings in practice.</p> <p>We\u2019ll cover:</p> <ol> <li>Loading pretrained embeddings using <code>gensim</code></li> <li>Looking up vectors for specific words</li> <li>Measuring word similarity</li> <li>Visualizing embeddings with <code>t-SNE</code></li> <li>(Optional) A tiny sentiment classifier using averaged word vectors</li> </ol> <p>Let\u2019s dive in.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#setup","title":"\ud83e\uddf0 Setup","text":"<p>Install the required packages (if you haven\u2019t already):</p> <pre><code>pip install gensim nltk matplotlib scikit-learn\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#1-load-a-pretrained-model","title":"\ud83d\udce5 1. Load a Pretrained Model","text":"<p><code>gensim</code> provides easy access to many pretrained embeddings. Let\u2019s start with GloVe.</p> <pre><code>import gensim.downloader as api\n\n# Load 100-dimensional GloVe vectors trained on Wikipedia and Gigaword\nmodel = api.load(\"glove-wiki-gigaword-100\")\n</code></pre> <p>You can also load:</p> <ul> <li><code>\"word2vec-google-news-300\"</code> \u2013 pretrained Word2Vec model</li> <li><code>\"fasttext-wiki-news-subwords-300\"</code> \u2013 FastText with subword info</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#2-get-the-vector-for-a-word","title":"\ud83d\udd24 2. Get the Vector for a Word","text":"<pre><code># Get vector for the word 'apple'\nvector = model['apple']\nprint(vector.shape)  # Should be (100,)\n</code></pre> <p>This gives you a dense 100-dimensional vector representing \u201capple.\u201d</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#3-measure-similarity-between-words","title":"\ud83e\udd1d 3. Measure Similarity Between Words","text":"<p>You can find out which words are most similar:</p> <pre><code># Top 5 words similar to 'king'\nprint(model.most_similar('king', topn=5))\n</code></pre> <p>And try analogies:</p> <pre><code># king - man + woman \u2248 queen\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'])\nprint(result[0])  # Should be close to 'queen'\n</code></pre> <p>You can also directly compute cosine similarity:</p> <pre><code>similarity = model.similarity('dog', 'cat')\nprint(f\"Similarity between dog and cat: {similarity:.2f}\")\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#4-visualize-word-embeddings-with-t-sne","title":"\ud83c\udfa8 4. Visualize Word Embeddings with t-SNE","text":"<p>Let\u2019s plot a small set of word vectors in 2D using t-SNE from <code>sklearn</code>.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Pick some words to visualize\nwords = ['king', 'queen', 'man', 'woman', 'dog', 'cat', 'apple', 'banana']\nword_vectors = [model[word] for word in words]\n\n# Reduce dimensions to 2D\ntsne = TSNE(n_components=2, random_state=0, perplexity=5)\nX_2d = tsne.fit_transform(word_vectors)\n\n# Plot\nplt.figure(figsize=(8, 6))\nfor i, word in enumerate(words):\n    plt.scatter(X_2d[i, 0], X_2d[i, 1])\n    plt.annotate(word, (X_2d[i, 0]+0.5, X_2d[i, 1]+0.5))\nplt.title(\"2D Visualization of Word Embeddings\")\nplt.show()\n</code></pre> <p>This produces a nice plot showing semantic clustering\u2014e.g., \u201cman\u201d and \u201cwoman\u201d near each other, \u201cdog\u201d and \u201ccat\u201d close together.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#optional-tiny-sentiment-classifier-with-averaged-embeddings","title":"\ud83e\uddea (Optional) Tiny Sentiment Classifier with Averaged Embeddings","text":"<p>Let\u2019s build a simple sentiment classifier using averaged word vectors for each sentence.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\n# Simple dataset\ntexts = [\n    \"I love this movie\", \"This is terrible\", \"What a great experience\", \"I hate it\",\n    \"Absolutely fantastic!\", \"Worst film ever\", \"Not bad at all\", \"Awful, just awful\"\n]\nlabels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n\n# Convert each sentence to an average of its word vectors\ndef sentence_to_vec(sentence):\n    tokens = word_tokenize(sentence.lower())\n    vectors = [model[word] for word in tokens if word in model]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n\nX = np.array([sentence_to_vec(text) for text in texts])\ny = np.array(labels)\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train classifier\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n</code></pre> <p>\u27a1\ufe0f You should get a reasonably good accuracy\u2014even with this tiny dataset\u2014just using pretrained vectors + logistic regression.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html#summary","title":"\u2705 Summary","text":"<p>In this code section, you learned how to:</p> <ul> <li>Load pretrained word embeddings (GloVe, Word2Vec, FastText)</li> <li>Explore similarity and analogy tasks</li> <li>Visualize word meanings with t-SNE</li> <li>Use embeddings for a basic NLP classification task</li> </ul> <p>These tools are a powerful way to bring pretrained semantic knowledge into your own NLP workflows\u2014even before jumping into deep neural networks.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html","title":"Chapter 3: Word Embeddings and Dense Representations","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this chapter, the reader will be able to:</p> <ul> <li>Understand what word embeddings are and why they matter.</li> <li>See how embeddings solve the limitations of sparse representations like TF-IDF.</li> <li>Explore how models like Word2Vec, GloVe, and FastText learn embeddings.</li> <li>Visualize word relationships in vector space.</li> <li>Use pre-trained embeddings in code and analyze their structure.</li> <li>Recognize the limitations of static embeddings, and understand why contextual representations (like BERT or GPT) are needed.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#chapter-sections","title":"\ud83d\udcda Chapter Sections","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#31-introduction-from-counting-to-understanding","title":"3.1 Introduction: From Counting to Understanding","text":"<ul> <li>Reflect on the limitations of Bag-of-Words and TF-IDF: sparsity, lack of meaning.</li> <li>Introduce the need for dense, semantic representations.</li> <li>Build the intuitive idea that words appearing in similar contexts tend to have similar meanings.</li> <li>Set the stage for the chapter.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#32-what-are-word-embeddings","title":"3.2 What Are Word Embeddings?","text":"<ul> <li>Introduce word embeddings formally and intuitively.</li> <li>Explain that a word is represented as a dense vector of real numbers (e.g., 100\u2013300 dimensions).</li> <li>Discuss properties like semantic similarity, vector closeness, and continuous representation.</li> <li>Use simple analogies (e.g., \u201cwords on a map\u201d) to make it visual and relatable.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#33-how-embeddings-capture-meaning","title":"3.3 How Embeddings Capture Meaning","text":"<ul> <li>Explain how embeddings are learned from large corpora through co-occurrence and context.</li> <li>Discuss the distributional hypothesis (\u201cYou shall know a word by the company it keeps\u201d).</li> <li> <p>Show how embedding spaces reflect relationships:</p> </li> <li> <p>Clustering: animals, countries, emotions</p> </li> <li>Analogies: <code>king \u2013 man + woman \u2248 queen</code></li> <li>Use visualizations (t-SNE or PCA) and diagrams to show word groupings.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#34-word2vec-learning-embeddings-from-context","title":"3.4 Word2Vec: Learning Embeddings from Context","text":"<ul> <li>Introduce Word2Vec, its origin (Google, 2013), and significance.</li> <li> <p>Explain the two training strategies:</p> </li> <li> <p>CBOW: predict a word from its context</p> </li> <li>Skip-gram: predict the context from a word</li> <li>Dive into the architecture and training logic (without heavy math).</li> <li>Explain the role of context window, negative sampling, and why Word2Vec is unsupervised.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#35-glove-global-vectors-from-co-occurrence","title":"3.5 GloVe: Global Vectors from Co-occurrence","text":"<ul> <li>Explain the motivation behind GloVe (Stanford, 2014).</li> <li> <p>Compare to Word2Vec:</p> </li> <li> <p>Word2Vec: predictive, local</p> </li> <li>GloVe: count-based, global</li> <li>Describe how GloVe uses a co-occurrence matrix and learns embeddings by factorizing it.</li> <li>Show how it balances frequency with meaning.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#36-fasttext-going-beyond-words","title":"3.6 FastText: Going Beyond Words","text":"<ul> <li>Present the key innovation: subword embeddings using character n-grams.</li> <li> <p>Explain how this helps:</p> </li> <li> <p>Represent rare or misspelled words</p> </li> <li>Capture morphological structure (e.g., \u201crun\u201d, \u201crunning\u201d, \u201crunner\u201d)</li> <li>Discuss FastText\u2019s use in multilingual and noisy datasets.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#37-working-with-pretrained-embeddings-code-case-study","title":"3.7 Working with Pretrained Embeddings (Code + Case Study)","text":"<ul> <li>Introduce real-world usage of embeddings.</li> <li> <p>Use <code>gensim</code> or HuggingFace to:</p> </li> <li> <p>Load pre-trained Word2Vec, GloVe, or FastText models</p> </li> <li>Retrieve vectors</li> <li>Compute similarity</li> <li>Visualize clusters with PCA or t-SNE</li> <li>Build a simple classifier (e.g., sentiment analysis) using averaged word vectors.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#38-limitations-of-word-embeddings","title":"3.8 Limitations of Word Embeddings","text":"<ul> <li> <p>Discuss problems that embeddings don\u2019t solve:</p> </li> <li> <p>Context insensitivity: same vector for \u201cbank\u201d in different meanings</p> </li> <li>Static nature: embeddings don\u2019t change based on sentence</li> <li>Out-of-vocabulary (OOV): unknown words = no vector</li> <li>Use real examples to show failure cases.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#39-from-static-to-contextual-the-next-step","title":"3.9 From Static to Contextual: The Next Step","text":"<ul> <li>Set up the transition to contextual embeddings (Chapter 4).</li> <li> <p>Briefly introduce the idea behind ELMo, BERT, and GPT:</p> </li> <li> <p>Vectors now depend on sentence context</p> </li> <li>Meaning is dynamic, not fixed</li> <li>End with a teaser: \u201cHow does a model know what 'Java' means in different sentences? That\u2019s the magic of contextual embeddings.\u201d</li> </ul> <p>Would you like me to now begin writing Section 3.1 in the same clear, descriptive, and structured teaching style as 2.3.1 / 2.3.2?</p> <p>Great \u2014 let\u2019s now write Section 3.1: Introduction: From Counting to Understanding in the same descriptive, warm, beginner-friendly style as Sections 2.3.1 / 2.3.2 from Chapter 2.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#31-introduction-from-counting-to-understanding_1","title":"3.1 Introduction: From Counting to Understanding","text":"<p>In the early stages of working with text data, we focused on turning words into numbers\u2014a necessary first step in making language accessible to machines. Classical techniques like Bag-of-Words and TF-IDF helped us do that. They allowed us to convert entire sentences or documents into vectors based on how often certain words appear.</p> <p>At first, this seems like progress. Machines can now process text numerically, and that\u2019s useful for tasks like spam detection, topic classification, or keyword matching.</p> <p>But let\u2019s pause for a moment and ask: Do these models really understand language?</p> <p>The answer, unfortunately, is no.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#the-limits-of-counting","title":"\ud83e\uddf1 The Limits of Counting","text":"<p>Imagine you have two short sentences:</p> <ul> <li>\u201cI love sunny beaches.\u201d</li> <li>\u201cWarm coastlines make me happy.\u201d</li> </ul> <p>If you feed these into a Bag-of-Words model, it will treat each sentence as a collection of individual words. Since \u201cbeaches\u201d and \u201ccoastlines\u201d are different tokens, the model sees no similarity between them\u2014even though you and I know they mean nearly the same thing.</p> <p>TF-IDF adds a layer of intelligence by adjusting scores based on how common or rare a word is across many documents. But even with TF-IDF, the model doesn\u2019t know that \u201chappy\u201d and \u201clove\u201d are both positive emotions, or that \u201csunny\u201d and \u201cwarm\u201d might be describing similar settings.</p> <p>In short, these methods count, but they don\u2019t understand.</p> <p>And there are deeper problems too.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#the-problem-of-sparse-vectors","title":"\ud83e\uddca The Problem of Sparse Vectors","text":"<p>Classical vector models create huge, high-dimensional vectors\u2014often tens or hundreds of thousands of dimensions long\u2014where each dimension corresponds to a word in the vocabulary.</p> <p>Most of the time, these vectors are sparse, meaning they\u2019re mostly zeros. If a sentence only uses a handful of words from a massive vocabulary, the vector representation will be nearly empty.</p> <p>Sparse vectors:</p> <ul> <li>Waste memory and computing power</li> <li>Make it hard for machine learning models to generalize</li> <li>Carry no sense of meaning or structure</li> </ul> <p>It\u2019s like trying to understand someone\u2019s personality by looking at which words they use, without considering how or why they use them.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#words-without-context","title":"\ud83c\udfad Words Without Context","text":"<p>Another big issue is polysemy\u2014when a single word has multiple meanings.</p> <p>Consider the word \u201cbat\u201d in these two sentences:</p> <ul> <li>\u201cHe hit the ball with a bat.\u201d</li> <li>\u201cThe bat flew out of the cave.\u201d</li> </ul> <p>These are clearly very different uses of the word. But in both TF-IDF and Bag-of-Words, \u201cbat\u201d is just one word\u2014one vector. There\u2019s no way for the model to distinguish between a wooden stick and a flying mammal.</p> <p>That\u2019s a serious limitation. It means the model has no way of knowing what a word means in a given sentence.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html#the-need-for-better-representations","title":"\ud83c\udf31 The Need for Better Representations","text":"<p>By now, it should be clear: Classical methods give us numbers, but not meaning.</p> <p>We want something better\u2014something that can tell us:</p> <ul> <li>That \u201ccat\u201d and \u201cdog\u201d are more similar than \u201ccat\u201d and \u201ccarrot\u201d</li> <li>That \u201cking\u201d and \u201cqueen\u201d have a relationship that\u2019s similar to \u201cman\u201d and \u201cwoman\u201d</li> <li>That \u201cbank\u201d means something different in \u201criver bank\u201d versus \u201csavings bank\u201d</li> </ul> <p>To do this, we need a new kind of representation\u2014one that:</p> <ul> <li>Is dense (uses compact vectors with real information)</li> <li>Is learned from actual usage of words in real text</li> <li>Captures the semantic relationships between words</li> <li>Reflects how words are used in context</li> </ul> <p>This is where word embeddings come in. They offer a way to represent words as vectors in a multi-dimensional space, where the distance and direction between vectors actually mean something.</p> <p>Words that occur in similar contexts will have similar vectors. Words that are used in different ways will be positioned far apart. And, as we\u2019ll soon see, even abstract relationships\u2014like gender, verb tense, or geographical association\u2014can emerge from these vector spaces.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html","title":"3.2.1 What Is a Word Embedding?","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#32-what-are-word-embeddings-subsections-overview","title":"\u2705 3.2 What Are Word Embeddings? \u2014 Subsections Overview","text":"<p>Goal: To clearly explain what word embeddings are, why they matter, and how they work at a conceptual and structural level.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#321-what-is-a-word-embedding-conceptual-introduction","title":"3.2.1 What Is a Word Embedding? (Conceptual Introduction)","text":"<p>A beginner-friendly explanation of what embeddings are, using analogies, examples, and contrast with sparse vectors.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#322-sparse-vs-dense-representations","title":"3.2.2 Sparse vs. Dense Representations","text":"<p>Why sparse vectors (like BoW, TF-IDF) are problematic, and how dense vectors solve this. Visualize the difference.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#323-how-embeddings-represent-similarity","title":"3.2.3 How Embeddings Represent Similarity","text":"<p>How closeness in vector space reflects semantic similarity. Introduce cosine similarity in a conceptual way.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#324-the-shape-of-meaning-embedding-space","title":"3.2.4 The Shape of Meaning: Embedding Space","text":"<p>Discuss how embeddings capture clusters, analogies (e.g., king - man + woman \u2248 queen), and concepts as directions.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#325-a-glimpse-at-the-math-light-touch","title":"3.2.5 A Glimpse at the Math (Light Touch)","text":"<p>Very light mathematical intuition about how embeddings are vectors with real-valued dimensions. No heavy formalism.</p> <p>Now, let\u2019s write the first subsection in full detail.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#321-what-is-a-word-embedding","title":"3.2.1 What Is a Word Embedding?","text":"<p>When we speak or write, words flow naturally. We rarely stop to think about what a word \u201cis\u201d\u2014we just know what it means based on experience, context, and usage. But when a machine encounters language, it doesn\u2019t have that shared human background.</p> <p>So, how can a machine understand a word like \u201cfriendship\u201d, \u201cdemocracy\u201d, or \u201cocean\u201d?</p> <p>That\u2019s where word embeddings come in.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#the-big-idea","title":"\ud83e\udde0 The Big Idea","text":"<p>A word embedding is a way of representing a word as a vector of numbers\u2014specifically, a dense list of real values\u2014so that it can be understood, compared, and manipulated by a machine learning model.</p> <p>Unlike the huge, mostly-empty vectors used in classical approaches (like Bag-of-Words), embeddings are compact and meaningful. Each number in the vector captures something about how the word is used, what it tends to appear next to, or how it relates to other words.</p> <p>Think of a word embedding as a location on a map, where:</p> <ul> <li>Words with similar meanings are placed close together</li> <li>Words used in different contexts are far apart</li> <li>Relationships like gender, tense, or category appear as directions or distances</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#an-analogy-words-as-coordinates","title":"\ud83d\uddfa\ufe0f An Analogy: Words as Coordinates","text":"<p>Imagine you have a globe\u2014not of countries, but of words.</p> <p>On this globe:</p> <ul> <li>\u201cking\u201d and \u201cqueen\u201d are near each other</li> <li>\u201cdog\u201d and \u201cpuppy\u201d share a region</li> <li>\u201chappiness\u201d, \u201cjoy\u201d, and \u201cdelight\u201d all live in the same neighborhood</li> </ul> <p>Each word\u2019s position on this globe is its embedding\u2014a unique set of coordinates in multi-dimensional space.</p> <p>So instead of saying:</p> <p>\"These words are similar because they appear next to each other a lot,\"</p> <p>...we can say:</p> <p>\"These words are similar because their vectors point in the same direction.\"</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#what-does-an-embedding-look-like","title":"\ud83d\udd22 What Does an Embedding Look Like?","text":"<p>Let\u2019s look at an actual example. Here\u2019s what the embedding for the word <code>\"apple\"</code> might look like (with just 5 out of 100+ values shown):</p> <pre><code>[0.13, -0.27, 0.44, 0.89, -0.35, ...]\n</code></pre> <p>Every word gets its own vector like this\u2014learned from real text data. The model doesn\u2019t assign these numbers randomly; it learns them by observing how words behave in natural language.</p> <p>For example:</p> <ul> <li>If \u201capple\u201d often appears in similar contexts as \u201cbanana\u201d or \u201cfruit,\u201d their vectors will end up nearby.</li> <li>If \u201capple\u201d sometimes appears in tech contexts (as in \u201cApple Inc.\u201d), it will share some space with words like \u201ciPhone\u201d or \u201cMac.\u201d</li> </ul> <p>This is part of what makes embeddings powerful: they are trained by usage, not by definitions.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html#why-is-this-useful","title":"\ud83d\udce6 Why Is This Useful?","text":"<p>Word embeddings make it possible for machines to do all sorts of intelligent things with language:</p> <ul> <li>Compare words based on meaning, not just spelling</li> <li>Find synonyms automatically</li> <li>Understand analogies (e.g., Paris is to France as Berlin is to Germany)</li> <li>Group related words together in clusters</li> <li>Serve as input for powerful models like neural networks and transformers</li> </ul> <p>In short, embeddings let us go beyond counting words, and start learning from them.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html","title":"3.2.2 Sparse vs. Dense Representations","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html#322-sparse-vs-dense-representations","title":"3.2.2 Sparse vs. Dense Representations","text":"<p>To understand why word embeddings were such a major leap forward in natural language processing, we first need to understand the problem they solved.</p> <p>Before embeddings came along, most NLP systems used sparse representations. This means that every word or sentence was represented as a very long vector\u2014often tens of thousands of dimensions\u2014where most of the values were simply zero.</p> <p>Let\u2019s look at this in a little more detail.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html#sparse-representations-the-bag-of-words-reminder","title":"\ud83d\udce6 Sparse Representations: The Bag-of-Words Reminder","text":"<p>In the Bag-of-Words approach, we create a vocabulary\u2014a giant list of every word that appears in our dataset. If the vocabulary has 10,000 words, then every sentence becomes a vector of length 10,000.</p> <p>Each position in that vector corresponds to a specific word:</p> <ul> <li>If the word appears in the sentence, we place a <code>1</code> (or some frequency count)</li> <li>If it doesn't, we place a <code>0</code></li> </ul> <p>So the sentence:</p> <p>\"I love natural language processing\"</p> <p>might turn into a vector like:</p> <pre><code>[0, 0, 1, 0, 0, 1, 0, 0, ..., 1, 0, 0]\n</code></pre> <p>Only a few numbers are non-zero. The rest\u2014often over 99%\u2014are zeros.</p> <p>These vectors are:</p> <ul> <li>High-dimensional (because vocabularies are huge)</li> <li>Sparse (because most words don\u2019t appear in a given sentence)</li> <li>Unstructured (the values don\u2019t reflect relationships between words)</li> </ul> <p>There\u2019s no way to tell that \u201clove\u201d is similar to \u201clike,\u201d or that \u201clanguage\u201d and \u201ccommunication\u201d often go together. In fact, the words are treated as if they have nothing in common, just because they occupy different positions in the vector.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html#dense-representations-to-the-rescue","title":"\ud83d\udca1 Dense Representations to the Rescue","text":"<p>A dense vector, on the other hand, is much more compact. Instead of tens of thousands of dimensions, we use something like 100 or 300. And unlike sparse vectors, every dimension contains useful information\u2014not just a 0 or 1.</p> <p>Let\u2019s look at an example.</p> <p>Suppose the word \u201capple\u201d is represented like this:</p> <pre><code>[0.21, -0.17, 0.09, 0.62, -0.44, ..., 0.13]\n</code></pre> <p>This is a dense embedding vector\u2014a small list of real numbers that has been learned from data. There are no wasted dimensions. Every number helps define what \u201capple\u201d means, based on its usage in the real world.</p> <p>The same goes for other words:</p> <pre><code>\u201cbanana\u201d \u2192 [0.20, -0.15, 0.11, 0.59, -0.40, ..., 0.15]\n\u201ccomputer\u201d \u2192 [-0.45, 0.03, 0.87, -0.01, 0.74, ..., -0.09]\n</code></pre> <p>Here\u2019s what\u2019s exciting: Words with similar meanings (like \u201capple\u201d and \u201cbanana\u201d) end up with vectors that are close to each other. Words that are unrelated (like \u201cbanana\u201d and \u201ccomputer\u201d) end up far apart.</p> <p>This allows us to measure similarity, cluster words, find analogies, and feed these representations into neural networks.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html#why-dense-beats-sparse","title":"\ud83d\udd0d Why Dense Beats Sparse","text":"<p>Let\u2019s summarize the differences:</p> Feature Sparse Vectors (e.g., BoW, TF-IDF) Dense Vectors (Embeddings) Dimensionality Very high (10,000+) Low (50\u2013300) Values Mostly 0s All meaningful real numbers Similar words \u2192 similar? \u274c No \u2705 Yes Learns from usage/context \u274c No \u2705 Yes Captures meaning \u274c No \u2705 Yes <p>Sparse vectors are simple but shallow. Dense vectors are powerful and compact\u2014they actually learn meaning from the way words are used.</p> <p>That\u2019s why embeddings have become the foundation of all modern NLP techniques.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations.html#a-quick-visualization","title":"\ud83d\udccc A Quick Visualization","text":"<p>Imagine plotting sparse vectors in a giant space. Words like \u201ccat\u201d and \u201cdog\u201d are nowhere near each other\u2014even though they should be.</p> <p>Now imagine plotting dense embeddings. \u201cCat\u201d and \u201cdog\u201d cluster together. So do \u201csad\u201d and \u201cunhappy,\u201d \u201ccity\u201d and \u201cvillage,\u201d \u201cking\u201d and \u201cqueen.\u201d Dense spaces map meaning into geometry.</p> <p>We\u2019ll explore this more in the next section, where we look at how similarity is measured between word vectors.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html","title":"3.2.3 How Embeddings Represent Similarity","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#323-how-embeddings-represent-similarity","title":"3.2.3 How Embeddings Represent Similarity","text":"<p>One of the most powerful and exciting features of word embeddings is that they make it possible to measure how similar two words are\u2014not by spelling or frequency, but by meaning.</p> <p>This is something that classical NLP methods couldn\u2019t do. In Bag-of-Words or TF-IDF, the words <code>\"happy\"</code> and <code>\"joyful\"</code> are just two separate entries in a list. There\u2019s no mathematical reason to treat them as similar, even if they often appear in the same kinds of texts.</p> <p>But with word embeddings, we get something different. Words are not just symbols anymore\u2014they\u2019re points in space, and that changes everything.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#similarity-as-distance-in-space","title":"\ud83e\udded Similarity as Distance in Space","text":"<p>Let\u2019s say each word is represented by a vector: a list of numbers like this:</p> <pre><code>\"happy\"   \u2192 [0.62, -0.14, 0.55, ..., 0.09]  \n\"joyful\"  \u2192 [0.60, -0.18, 0.58, ..., 0.12]\n\"sad\"     \u2192 [-0.45,  0.27, -0.32, ..., 0.78]\n</code></pre> <p>Now, we can use simple geometry to ask:</p> <p>How close are these points to each other?</p> <p>Words that are similar in meaning will have vectors that point in the same direction, or are located near one another in the vector space.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#cosine-similarity-a-common-metric","title":"\ud83d\udcd0 Cosine Similarity: A Common Metric","text":"<p>To measure how \u201cclose\u201d two vectors are, we often use a technique called cosine similarity. Don\u2019t worry\u2014the name sounds more complicated than it really is.</p> <p>Cosine similarity looks at the angle between two vectors. If they point in the same direction, the angle is small, and the cosine value is close to <code>1</code>\u2014meaning the words are similar. If they point in opposite directions, the cosine is close to <code>-1</code>\u2014meaning they\u2019re dissimilar.</p> <p>You don\u2019t need to calculate this by hand. But conceptually, here\u2019s how it works:</p> <ul> <li><code>\"happy\"</code> and <code>\"joyful\"</code> \u2192 cosine similarity \u2248 <code>0.95</code> \u2192 very similar</li> <li><code>\"happy\"</code> and <code>\"sad\"</code> \u2192 cosine similarity \u2248 <code>-0.2</code> \u2192 very different</li> <li><code>\"happy\"</code> and <code>\"dog\"</code> \u2192 cosine similarity \u2248 <code>0.1</code> \u2192 mostly unrelated</li> </ul> <p>This is what makes embeddings semantically meaningful: the numbers are not arbitrary. They reflect the actual behavior of words in language.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#understanding-through-context","title":"\ud83e\udde0 Understanding Through Context","text":"<p>Why do similar words end up with similar vectors?</p> <p>Because embeddings are learned by observing the contexts in which words appear. If two words appear in similar contexts, they\u2019re likely to mean similar things.</p> <p>For example:</p> <ul> <li> <p><code>\"happy\"</code> might appear in sentences like:</p> </li> <li> <p>\u201cShe felt happy after the exam.\u201d</p> </li> <li>\u201cIt was a happy day for everyone.\u201d</li> <li> <p><code>\"joyful\"</code> might appear in:</p> </li> <li> <p>\u201cHe looked joyful at the celebration.\u201d</p> </li> <li>\u201cThe joyful news spread quickly.\u201d</li> </ul> <p>The words around them\u2014like \u201cfelt,\u201d \u201ccelebration,\u201d \u201cnews,\u201d \u201cday\u201d\u2014are often shared or similar. This shared context pulls their vectors closer together during training.</p> <p>This idea is based on something called the distributional hypothesis:</p> <p>\u201cYou shall know a word by the company it keeps.\u201d \u2014 J.R. Firth (1957)</p> <p>In short: meaning emerges from usage.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#words-in-clusters","title":"\ud83d\udccd Words in Clusters","text":"<p>If you could see embeddings in 2D or 3D space (which we\u2019ll do later using tools like t-SNE or PCA), you\u2019d notice something amazing:</p> <ul> <li> <p>Words form clusters based on themes:   \u201ccat\u201d, \u201cdog\u201d, \u201chamster\u201d \u2192 pets   \u201cParis\u201d, \u201cBerlin\u201d, \u201cRome\u201d \u2192 European cities   \u201crun\u201d, \u201cwalk\u201d, \u201cjog\u201d \u2192 physical actions</p> </li> <li> <p>Words also form semantic gradients\u2014smooth transitions across related meanings.</p> </li> </ul> <p>This is a huge leap from earlier methods. We\u2019re not just seeing which words appear together; we\u2019re seeing how their meanings relate to one another in space.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity.html#why-it-matters","title":"\u2728 Why It Matters","text":"<p>Once we can measure similarity between words, we can:</p> <ul> <li>Find synonyms automatically</li> <li>Group documents by meaning</li> <li>Improve search results (e.g., searching for \u201cdoctor\u201d might also match \u201cphysician\u201d)</li> <li>Power neural networks for tasks like sentiment analysis, question answering, and machine translation</li> </ul> <p>This is the beating heart of modern NLP: instead of treating words as symbols, we treat them as concepts with geometry.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html","title":"3.2.4 The Shape of Meaning: Embedding Space","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html#324-the-shape-of-meaning-embedding-space","title":"3.2.4 The Shape of Meaning: Embedding Space","text":"<p>By now, we\u2019ve seen that word embeddings represent words as dense vectors\u2014compact lists of numbers that capture meaning based on usage. We\u2019ve also seen that similar words have similar vectors, and we can measure that similarity using cosine distance.</p> <p>But there\u2019s something even more fascinating about word embeddings: They don\u2019t just show how similar words are\u2014they show how words relate to one another.</p> <p>In fact, when visualized, the embedding space begins to look less like a spreadsheet of numbers and more like a semantic landscape, where directions and distances encode real linguistic patterns.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html#words-arent-just-closetheyre-structured","title":"\ud83e\udded Words Aren\u2019t Just Close\u2014They\u2019re Structured","text":"<p>Let\u2019s imagine a small region of embedding space that contains the following words:</p> <ul> <li><code>\"king\"</code></li> <li><code>\"queen\"</code></li> <li><code>\"man\"</code></li> <li><code>\"woman\"</code></li> </ul> <p>When these words are embedded well, something remarkable happens: You can actually do arithmetic with their vectors\u2014and it works.</p> <pre><code>vector(\"king\") - vector(\"man\") + vector(\"woman\") \u2248 vector(\"queen\")\n</code></pre> <p>This famous example from Word2Vec isn\u2019t just a party trick. It shows that embeddings capture relationships as directions in space.</p> <p>Let\u2019s unpack this.</p> <ul> <li>The difference between <code>\"king\"</code> and <code>\"man\"</code> is roughly: royalty minus male.</li> <li>Adding <code>\"woman\"</code> brings female into the equation.</li> <li>The resulting vector points almost exactly to <code>\"queen\"</code>.</li> </ul> <p>This isn\u2019t magic. It\u2019s a result of how the model learns to organize meaning through geometry.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html#vector-arithmetic-and-analogies","title":"\ud83e\uddf1 Vector Arithmetic and Analogies","text":"<p>This ability to represent analogies in vector space is one of the most mind-blowing features of word embeddings.</p> <p>Here are more examples you\u2019d often find:</p> <pre><code>\"Paris\" - \"France\" + \"Germany\" \u2248 \"Berlin\"\n\n\"walking\" - \"walk\" + \"swim\" \u2248 \"swimming\"\n\n\"biggest\" - \"big\" + \"fast\" \u2248 \"fastest\"\n</code></pre> <p>These patterns show that:</p> <ul> <li>Countries and capitals form parallel lines.</li> <li>Verb tenses create directional patterns.</li> <li>Adjective to superlative transformations follow consistent vectors.</li> </ul> <p>This tells us that embedding space isn\u2019t random\u2014it\u2019s full of linguistic structure.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html#visualizing-the-space","title":"\ud83d\udcca Visualizing the Space","text":"<p>Embedding vectors often live in hundreds of dimensions, which are hard to picture. But we can use tools like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to project them into 2D or 3D.</p> <p>When we do that, we often see:</p> <ul> <li> <p>Clusters of related words:   <code>\"dog\"</code>, <code>\"cat\"</code>, <code>\"rabbit\"</code> \u2192 pets   <code>\"red\"</code>, <code>\"blue\"</code>, <code>\"green\"</code> \u2192 colors   <code>\"run\"</code>, <code>\"walk\"</code>, <code>\"swim\"</code> \u2192 actions</p> </li> <li> <p>Semantic directions:   Moving from <code>\"France\"</code> to <code>\"Paris\"</code> looks the same as moving from <code>\"Germany\"</code> to <code>\"Berlin\"</code>.</p> </li> </ul> <p>These aren\u2019t manually programmed relationships. They\u2019re discovered by the model from language usage alone.</p> <p>This gives embeddings the power to:</p> <ul> <li>Group and organize words by theme</li> <li>Support analogical reasoning</li> <li>Transfer knowledge across domains</li> </ul> <p>It\u2019s like building a map of language, where distance means similarity, and direction means transformation.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html#why-this-matters","title":"\ud83e\udde0 Why This Matters","text":"<p>Understanding the structure of embedding space is more than a curiosity. It\u2019s what enables:</p> <ul> <li>Semantic search (\u201cFind me things like this\u201d)</li> <li>Text generation (predict the next meaningful word)</li> <li>Recommendation systems (suggest similar products, documents, or terms)</li> <li>Bias detection and debiasing (because social biases can show up as measurable patterns in vector space)</li> </ul> <p>Word embeddings gave machines a way to reason geometrically about meaning\u2014a superpower that classical NLP methods never had.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html","title":"3.2.5 A Glimpse at the Math","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#325-a-glimpse-at-the-math","title":"3.2.5 A Glimpse at the Math","text":"<p>By now, you\u2019ve seen that word embeddings are dense vectors that capture meaning, similarity, and even relationships like analogies. But how exactly are these embeddings formed? What do those numbers actually represent?</p> <p>In this section, we\u2019ll take a gentle look at the math behind word embeddings. Don\u2019t worry\u2014we\u2019ll keep it light and intuitive. You don\u2019t need to be a mathematician to understand the big ideas.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#what-is-a-word-vector-mathematically","title":"\ud83d\udd22 What Is a Word Vector, Mathematically?","text":"<p>A word embedding is just a vector: a list of real numbers.</p> <pre><code>\"apple\" \u2192 [0.25, -0.17, 0.08, ..., 0.41]  # Maybe 100 or 300 numbers long\n</code></pre> <p>Each number in this list is a dimension\u2014think of them as \u201cfeatures\u201d or \u201ccoordinates\u201d in a high-dimensional space. A 100-dimensional embedding means every word is located somewhere in a 100D semantic space.</p> <p>So mathematically, we can say:</p> <p>A word embedding is a point in \u211d\u207f (n-dimensional real space).</p> <p>What\u2019s important is that the numbers are not randomly chosen. They\u2019re learned from massive text corpora, based on how often and in what contexts a word appears.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#how-are-these-vectors-learned","title":"\ud83e\udde0 How Are These Vectors Learned?","text":"<p>Word embeddings are typically learned using unsupervised learning. That means we don\u2019t tell the model what a word means\u2014we just let it figure things out by looking at real-world text.</p> <p>Here\u2019s the idea:</p> <p>Words that appear in similar contexts should have similar vectors.</p> <p>This is the distributional hypothesis in action.</p> <p>To learn the vectors, algorithms like Word2Vec or GloVe define a simple prediction task:</p> <ul> <li> <p>In Word2Vec, the model is trained to:</p> </li> <li> <p>Predict a word from its surrounding words (CBOW)</p> </li> <li>Or predict surrounding words from the current word (Skip-gram)</li> </ul> <p>The model tries to adjust the word vectors so that predictions become more accurate. In doing so, the vectors gradually take on meaningful shapes.</p> <p>It works a bit like this:</p> <ol> <li>Each word is first assigned a random vector.</li> <li>The model reads many word sequences from real text.</li> <li>For each training step, it updates the vectors to better predict neighboring words.</li> <li>Over time, similar words end up with similar vectors.</li> </ol> <p>No human labels. Just statistics. Just context.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#what-about-similarity","title":"\ud83e\uddee What About Similarity?","text":"<p>Once we have these vectors, we can compare any two words using cosine similarity.</p> <p>Mathematically, the cosine similarity between two vectors A and B is:</p> <p>$$ \\text{cosine_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||} $$</p> <p>This means:</p> <ul> <li>Take the dot product of the two vectors.</li> <li>Divide by the product of their magnitudes (lengths).</li> <li>The result is between <code>-1</code> (opposite) and <code>1</code> (identical direction).</li> </ul> <p>This tells us how aligned the two word vectors are in space. If they point in the same direction, they\u2019re semantically similar.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#a-matrix-view-optional-insight","title":"\ud83e\udde9 A Matrix View (Optional Insight)","text":"<p>If we zoom out from individual vectors, we can think of all embeddings as forming a matrix:</p> <pre><code>Word     |  dim_1 | dim_2 | dim_3 | ... | dim_n\n---------|--------|--------|--------|-----|-------\napple    |  0.25  | -0.17  | 0.08   | ... | 0.41\nbanana   |  0.22  | -0.13  | 0.09   | ... | 0.37\nlaptop   | -0.34  | 0.48   | 0.91   | ... | -0.03\n...\n</code></pre> <p>Each row is a word. Each column is a learned semantic feature. The model doesn\u2019t name these features\u2014they\u2019re abstract\u2014but they might (roughly) correspond to properties like \u201cis a fruit,\u201d \u201chas emotion,\u201d or \u201cis a location.\u201d</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math.html#do-we-need-to-understand-the-math-to-use-embeddings","title":"\ud83e\udd14 Do We Need to Understand the Math to Use Embeddings?","text":"<p>Not at all!</p> <p>You don\u2019t need to do any math to use word embeddings in your projects. Libraries like <code>gensim</code>, <code>spaCy</code>, and <code>transformers</code> handle it all for you. But having this intuition helps you appreciate what\u2019s happening under the hood\u2014and why embeddings are so effective.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html","title":"3.3 How Embeddings Capture Meaning","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#33-how-embeddings-capture-meaning","title":"3.3 How Embeddings Capture Meaning","text":"<p>Now that you understand what word embeddings are\u2014dense, learned vector representations of words\u2014it\u2019s time to dig deeper into how they capture meaning from real language. This is where the magic of embeddings really begins to shine.</p> <p>How is it possible that a model trained on nothing but raw text can discover that:</p> <ul> <li><code>\"Paris\"</code> is the capital of <code>\"France\"</code></li> <li><code>\"king\"</code> and <code>\"queen\"</code> differ by gender</li> <li><code>\"swimming\"</code> is related to <code>\"water\"</code> but not <code>\"fire\"</code>?</li> </ul> <p>The answer lies in a principle that\u2019s as old as computational linguistics itself: the distributional hypothesis.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#the-distributional-hypothesis","title":"\ud83e\udde0 The Distributional Hypothesis","text":"<p>At the heart of word embeddings is a simple but powerful idea:</p> <p>\u201cYou shall know a word by the company it keeps.\u201d \u2014 J.R. Firth, 1957</p> <p>This means that words that occur in similar contexts tend to have similar meanings.</p> <p>For example, take the following sentences:</p> <ul> <li>\u201cShe sipped a warm cup of tea.\u201d</li> <li>\u201cHe poured himself some hot coffee.\u201d</li> <li>\u201cThey drank herbal tea after dinner.\u201d</li> <li>\u201cShe grabbed a mug of coffee on the way to work.\u201d</li> </ul> <p>Here, tea and coffee appear in similar roles, with similar surrounding words. A machine that sees thousands of such examples can begin to recognize this pattern. Over time, the model learns to place tea and coffee close together in the embedding space.</p> <p>This is the magic: the model isn\u2019t told what these words mean, but it discovers their meaning from patterns in how they\u2019re used.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#context-drives-meaning","title":"\ud83d\udd04 Context Drives Meaning","text":"<p>Let\u2019s make this even more concrete.</p> <p>Imagine you\u2019re training a model, and you feed it this sentence:</p> <p>\u201cThe doctor examined the patient.\u201d</p> <p>Then another:</p> <p>\u201cThe surgeon performed the operation.\u201d</p> <p>And another:</p> <p>\u201cThe nurse monitored the vital signs.\u201d</p> <p>You\u2019ll notice that:</p> <ul> <li>Doctor, surgeon, and nurse all appear near words like \u201cpatient\u201d, \u201coperation\u201d, \u201cvital signs\u201d, \u201chospital\u201d</li> <li>These contextual clues help the model \u201crealize\u201d that these words belong to a shared category\u2014healthcare professionals</li> </ul> <p>That\u2019s how meaning forms. It\u2019s not from a dictionary. It\u2019s from data-driven observation.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#how-the-learning-happens-without-heavy-math","title":"\ud83d\udee0\ufe0f How the Learning Happens (Without Heavy Math)","text":"<p>Let\u2019s briefly look at how models like Word2Vec make this happen in practice.</p> <p>Suppose we have this sentence:</p> <p>\u201cThe cat sat on the mat.\u201d</p> <p>Let\u2019s say our target word is <code>\"sat\"</code>. The model picks a context window, say 2 words on either side:</p> <p>Context words: <code>\"The\"</code>, <code>\"cat\"</code>, <code>\"on\"</code>, <code>\"the\"</code> Target word: <code>\"sat\"</code></p> <p>The model\u2019s task is to either:</p> <ul> <li>Predict the target from the context (CBOW \u2013 Continuous Bag of Words)</li> <li>Or predict the context from the target (Skip-gram)</li> </ul> <p>Every time the model gets a prediction wrong, it adjusts the word vectors to get a bit closer to being right next time. Over millions of sentences, the model slowly pushes words used in similar contexts closer together in vector space.</p> <p>This is how the structure of language becomes the structure of space.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#embeddings-learn-more-than-just-similarity","title":"\ud83d\udce6 Embeddings Learn More Than Just Similarity","text":"<p>Because the model has access to massive amounts of data, it doesn\u2019t just learn that <code>\"king\"</code> is close to <code>\"queen\"</code>\u2014it learns how they relate. It starts to capture higher-level relationships like:</p> <ul> <li>Gender (man \u2192 woman, king \u2192 queen)</li> <li>Verb tense (walk \u2192 walked, run \u2192 ran)</li> <li>Nationality (Germany \u2192 German, Brazil \u2192 Brazilian)</li> </ul> <p>These patterns aren\u2019t hand-coded. They emerge naturally from the training process.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning.html#real-world-example","title":"\ud83d\udcca Real-World Example","text":"<p>Let\u2019s look at a real case. If we inspect vectors from a trained Word2Vec model, we might find:</p> <pre><code>similar(\"king\") \u2192 [\"queen\", \"monarch\", \"prince\", \"emperor\", \"ruler\"]\nsimilar(\"banana\") \u2192 [\"apple\", \"mango\", \"fruit\", \"pineapple\", \"grape\"]\nsimilar(\"run\") \u2192 [\"jog\", \"sprint\", \"race\", \"walk\", \"exercise\"]\n</code></pre> <p>These aren\u2019t guesses. They\u2019re learned from data. The embedding model has seen enough examples of how words are used to infer categories, associations, and themes.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html","title":"3.4.1 What Is Word2Vec?","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html#341-what-is-word2vec","title":"3.4.1 What Is Word2Vec?","text":"<p>In 2013, a research team at Google led by Tomas Mikolov introduced a model that would go on to revolutionize how we represent language in machine learning: Word2Vec.</p> <p>At a glance, Word2Vec might sound simple. It learns to represent words as vectors by looking at the words that surround them. But this simple idea unlocked something powerful: a way for machines to not just process language\u2014but begin to understand it.</p> <p>Let\u2019s unpack what Word2Vec is, why it was such a game changer, and what makes it so effective.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html#the-core-idea-learning-meaning-from-context","title":"\ud83e\udde0 The Core Idea: Learning Meaning from Context","text":"<p>We\u2019ve talked before about the distributional hypothesis, the idea that:</p> <p>Words that appear in similar contexts tend to have similar meanings.</p> <p>Word2Vec takes this principle and turns it into a learning task.</p> <p>Instead of manually assigning meaning to words, Word2Vec learns meaning by solving a prediction problem:</p> <ul> <li>Either predict a word based on the words around it</li> <li>Or predict the surrounding words based on a target word</li> </ul> <p>This is very different from older methods like Bag-of-Words or TF-IDF, which simply counted things. Word2Vec uses a small neural network to learn vector representations that are optimized for predicting context.</p> <p>The result? Embeddings that group similar words together, capture relationships, and work beautifully as input to more advanced models.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html#word2vec-isnt-just-a-modelits-a-framework","title":"\u2699\ufe0f Word2Vec Isn\u2019t Just a Model\u2014it\u2019s a Framework","text":"<p>Technically, Word2Vec isn\u2019t one single model. It\u2019s a framework with two different learning strategies:</p> <ul> <li>CBOW (Continuous Bag-of-Words): Predicts the target word from surrounding context words.</li> <li>Skip-gram: Predicts surrounding context words from a single target word.</li> </ul> <p>We\u2019ll explore both strategies in the next subsection, but for now, just understand that Word2Vec is all about turning context into prediction\u2014and in doing so, it creates meaningful word vectors.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html#why-word2vec-was-a-breakthrough","title":"\ud83d\udd0d Why Word2Vec Was a Breakthrough","text":"<p>Before Word2Vec, most NLP systems relied on:</p> <ul> <li>Manually engineered features</li> <li>High-dimensional sparse vectors</li> <li>Poor generalization for unseen words or small datasets</li> </ul> <p>Word2Vec offered a better way. With just a simple neural network, it could:</p> <ul> <li>Learn directly from raw text (no labels needed)</li> <li>Produce dense vectors (compact, continuous representations)</li> <li>Capture semantics: similarity, analogies, and relationships</li> </ul> <p>Even more impressively, Word2Vec was fast. It could train on billions of words in hours. This made it practical to build high-quality embeddings from massive datasets like Google News or Wikipedia.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F.html#output-the-embedding-matrix","title":"\ud83d\udd22 Output: The Embedding Matrix","text":"<p>After training, what you get from Word2Vec is:</p> <ul> <li>A large matrix of size (vocabulary size \u00d7 embedding dimension)</li> <li>Each row is a word\u2019s embedding (a learned vector)</li> </ul> <p>You can now:</p> <ul> <li>Retrieve the vector for any word</li> <li>Compare vectors to measure similarity</li> <li>Feed these vectors into downstream NLP tasks</li> </ul> <p>This matrix becomes your foundation for understanding language.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html","title":"3.4.2 CBOW and Skip Gram: Two Learning Strategies","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#342-cbow-and-skip-gram-two-learning-strategies","title":"3.4.2 CBOW and Skip-Gram: Two Learning Strategies","text":"<p>When we say \"Word2Vec,\" we\u2019re actually referring to two related models\u2014each with a slightly different way of learning from language. Both models share the same goal: to learn word embeddings that capture meaning. But they approach that goal from opposite directions.</p> <p>These two strategies are:</p> <ul> <li>CBOW (Continuous Bag-of-Words): Predict the word in the middle from the surrounding context.</li> <li>Skip-Gram: Predict the surrounding context words from the word in the middle.</li> </ul> <p>Let\u2019s explore each one step-by-step with examples, intuition, and diagrams (you can visualize them later using illustrations as placeholders).</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#cbow-predict-the-word-from-the-context","title":"\ud83e\uddf1 CBOW: Predict the Word from the Context","text":"<p>CBOW is like playing a guessing game:</p> <p>\"Can you guess the missing word if I give you the words around it?\"</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#example","title":"\ud83d\udccc Example","text":"<p>Take the sentence:</p> <p>\u201cThe cat sat on the mat.\u201d</p> <p>Suppose the target word is <code>\"sat\"</code> and we choose a window size of 2, which means we look at the 2 words before and after the target.</p> <p>The context becomes: <code>\"The\"</code>, <code>\"cat\"</code>, <code>\"on\"</code>, <code>\"the\"</code> CBOW\u2019s task is to predict the center word: <code>\"sat\"</code></p> <p>So:</p> <pre><code>Input:  [\"The\", \"cat\", \"on\", \"the\"]\nTarget: \"sat\"\n</code></pre> <p>The model processes the surrounding words and learns to predict the center word. To do this effectively, it must create vector representations of the context words that are informative enough to guess the target. This is how embeddings get learned.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#characteristics-of-cbow","title":"\u2705 Characteristics of CBOW","text":"<ul> <li>Faster to train because it averages context vectors.</li> <li>Works well for larger corpora.</li> <li>Tends to smooth over rare word representations (averaging effects).</li> <li>Trains on more data points (every target word appears in a context window).</li> </ul> <p>Absolutely! Here is the corrected Skip-Gram portion of Section 3.4.2, now written clearly and accurately in book-style format:</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#skip-gram-predict-the-context-from-the-word","title":"\ud83d\udd04 Skip-Gram: Predict the Context from the Word","text":"<p>If CBOW is about guessing the word in the middle, Skip-Gram flips that idea on its head. Instead of predicting the center word from its neighbors, Skip-Gram does the opposite:</p> <p>\u201cCan you guess the words around me if I give you the center word?\u201d</p> <p>This turns each word in the corpus into a kind of anchor point, and the model tries to learn which words are likely to appear nearby.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#example_1","title":"\ud83d\udccc Example","text":"<p>Consider the sentence:</p> <p>\u201cThe cat sat on the mat.\u201d</p> <p>Let\u2019s say our window size is 2. That means we look at two words on either side of the target word.</p> <p>Now let\u2019s take <code>\"sat\"</code> as our center word. The Skip-Gram model will try to predict its context:</p> <pre><code>Input:   \"sat\"  \nTargets: [\"The\", \"cat\", \"on\", \"the\"]\n</code></pre> <p>The model turns this into four training pairs:</p> <ul> <li>(\"sat\", \"The\")</li> <li>(\"sat\", \"cat\")</li> <li>(\"sat\", \"on\")</li> <li>(\"sat\", \"the\")</li> </ul> <p>So instead of predicting one word from multiple inputs (like CBOW does), Skip-Gram predicts multiple outputs from one input word.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#characteristics-of-skip-gram","title":"\u2705 Characteristics of Skip-Gram","text":"<ul> <li>Focuses on the target word and tries to predict surrounding words.</li> <li>Trains on more pairs, especially useful for rare words.</li> <li>Tends to produce more fine-grained and informative embeddings.</li> <li>Typically slower to train, since it creates more training examples than CBOW.</li> <li>Better for smaller datasets or when you care about learning rare word behavior.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#cbow-vs-skip-gram-side-by-side-summary","title":"\ud83d\udd0d CBOW vs Skip-Gram: Side-by-Side Summary","text":"Feature CBOW Skip-Gram Prediction Task Context \u2192 Target word Target word \u2192 Context words Training Speed Faster Slower Performance on Rare Words Weaker Better Data Efficiency More efficient Needs more training examples Use Case Large datasets, general purpose Smaller datasets, finer detail"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html#why-two-models","title":"\ud83e\udde0 Why Two Models?","text":"<p>Both models were designed to solve the same problem\u2014learning useful word embeddings\u2014but in slightly different ways. Which one you choose depends on:</p> <ul> <li>Your dataset size</li> <li>Whether you want to prioritize speed or detail</li> <li>Your specific task requirements</li> </ul> <p>Most modern applications use Skip-Gram, as it tends to yield richer word vectors, especially when working with fewer data.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html","title":"3.4.3 Training Word2Vec: How the Model Learns","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#343-training-word2vec-how-the-model-learns","title":"3.4.3 Training Word2Vec: How the Model Learns","text":"<p>Now that we understand what CBOW and Skip-Gram do at a high level\u2014predicting words from context and vice versa\u2014it\u2019s time to peek under the hood and see how Word2Vec actually learns these embeddings.</p> <p>This is where the abstract idea of \"vector representations\" becomes real. Behind the scenes, Word2Vec uses a shallow neural network to convert words into vectors that are good at prediction\u2014and, as a side effect, good at capturing meaning.</p> <p>Let\u2019s walk through how that happens in an intuitive, beginner-friendly way.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#the-word2vec-architecture-simplified","title":"\ud83e\udde0 The Word2Vec Architecture (Simplified)","text":"<p>Word2Vec uses a very simple neural network with just one hidden layer. It\u2019s one of the most lightweight neural models you\u2019ll encounter, yet incredibly effective.</p> <p>Here\u2019s the basic structure:</p> <pre><code>Input Layer \u2192 Hidden Layer (Embeddings) \u2192 Output Layer\n</code></pre> <p>Let\u2019s go through it using Skip-Gram as the example:</p> <ol> <li>The input is a one-hot encoded word.    For example, if your vocabulary has 10,000 words, the word <code>\"sat\"</code> would be a vector like:</li> </ol> <p><code>[0, 0, ..., 1, ..., 0]  # 10,000 dimensions, only 1 is \"on\"</code></p> <ol> <li> <p>The input is passed through a weight matrix (the hidden layer).    This matrix is actually what becomes your word embedding matrix.</p> </li> <li> <p>If your embedding size is 100, the weight matrix is of size 10,000 \u00d7 100.</p> </li> <li> <p>It maps the one-hot vector to a dense vector of 100 numbers.</p> </li> <li> <p>The output layer tries to predict the context words.    This involves comparing the hidden representation to all other word vectors, using something like a dot product or softmax.</p> </li> </ol>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#learning-by-prediction","title":"\ud83d\udd27 Learning by Prediction","text":"<p>The training goal is simple:</p> <p>Adjust the word vectors so that they become better at predicting the right context words.</p> <p>Each time the model makes a prediction, it checks:</p> <ul> <li>Were the predicted words close to the actual ones?</li> <li>If not, it adjusts the vectors\u2014nudging them in the right direction using gradient descent.</li> </ul> <p>This happens millions of times. Slowly, the word vectors shift in space:</p> <ul> <li>Words that appear in similar contexts move closer together.</li> <li>Rare or unrelated words drift further apart.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#lets-recap-that-flow-skip-gram-version","title":"\ud83d\udd01 Let\u2019s Recap That Flow (Skip-Gram Version)","text":"<ol> <li>Input word \u2192 one-hot vector (e.g., <code>\"sat\"</code>)</li> <li>Multiply by embedding matrix \u2192 get <code>\"sat\"</code>\u2019s embedding</li> <li>Multiply by another matrix to predict surrounding words</li> <li>Compare predictions with true context words</li> <li>Use the error to update both matrices via backpropagation</li> </ol> <p>After training:</p> <ul> <li>The first matrix (input \u2192 hidden) becomes your embedding matrix.</li> <li>You throw away the rest. You don\u2019t need the output layer anymore!</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#what-about-cbow","title":"\ud83d\udce5 What About CBOW?","text":"<p>In CBOW, you reverse the direction of prediction:</p> <ul> <li>You average the embeddings of the context words.</li> <li>Then you use that average vector to predict the center word.</li> </ul> <p>The learning process is otherwise similar:</p> <ul> <li>You pass the average through a layer.</li> <li>You get a prediction.</li> <li>You update the vectors to reduce the prediction error.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html#what-makes-this-effective","title":"\ud83c\udfaf What Makes This Effective?","text":"<p>You might be wondering: why does this simple model work so well?</p> <p>It\u2019s because:</p> <ul> <li>The model sees millions of examples of word usage.</li> <li>It optimizes the embeddings to capture useful generalizations.</li> <li>The embeddings are not hand-crafted\u2014they are discovered from real text.</li> </ul> <p>In doing so, the vectors come to reflect semantic properties:</p> <ul> <li><code>\"strong\"</code> and <code>\"powerful\"</code> end up close.</li> <li><code>\"man\"</code> - <code>\"king\"</code> + <code>\"woman\"</code> \u2248 <code>\"queen\"</code></li> </ul> <p>That\u2019s learned purely by solving this context prediction task over and over.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html","title":"3.4.4 Negative Sampling and Efficiency Tricks","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#344-negative-sampling-and-efficiency-tricks","title":"3.4.4 Negative Sampling and Efficiency Tricks","text":"<p>So far, we\u2019ve seen how Word2Vec learns word embeddings by predicting context words. But there\u2019s a catch\u2014real-world vocabularies are huge.</p> <p>Imagine trying to predict the correct context word out of a vocabulary of 100,000 words or more. At every training step, the model would need to:</p> <ul> <li>Compute scores for every word in the vocabulary.</li> <li>Apply the softmax function (which needs to sum over all those scores).</li> <li>Update gradients for every word in the output layer.</li> </ul> <p>That\u2019s... expensive. \ud83d\ude13</p> <p>This is where efficiency tricks like Negative Sampling come in. These techniques help us approximate the full softmax, allowing the model to train faster\u2014without sacrificing quality.</p> <p>Let\u2019s break it down intuitively.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#the-problem-softmax-over-a-huge-vocabulary","title":"\u26a0\ufe0f The Problem: Softmax Over a Huge Vocabulary","text":"<p>In the original Skip-Gram architecture, predicting the context word means computing:</p> <p>$$ P(\\text{word}i \\mid \\text{target}) = \\frac{e^{v_i \\cdot v{\\text{target}}}}{\\sum_{j=1}^{|V|} e^{v_j \\cdot v_{\\text{target}}}} $$</p> <p>That denominator sums over every single word in the vocabulary <code>V</code>. When <code>|V|</code> = 100,000 or more, this becomes infeasible.</p> <p>Imagine doing this for every word pair, in every window, in every sentence, for millions of sentences. Ouch.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#the-solution-negative-sampling","title":"\u2705 The Solution: Negative Sampling","text":"<p>Negative Sampling offers a simple, elegant workaround:</p> <p>Don\u2019t try to update all words. Just update a few \"negative\" words per training step.</p> <p>Here\u2019s how it works:</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#step-by-step","title":"\ud83d\udd27 Step-by-Step","text":"<ol> <li> <p>For each (target, context) positive pair (e.g., <code>\"sat\"</code> \u2192 <code>\"on\"</code>), we generate k negative samples: random words that are not true context words (e.g., <code>\"banana\"</code>, <code>\"explosion\"</code>, <code>\"cloud\"</code>).</p> </li> <li> <p>The model tries to:</p> </li> <li> <p>Increase the similarity between the target and the true context word</p> </li> <li> <p>Decrease the similarity between the target and the negative samples</p> </li> <li> <p>These are trained using a binary classification objective:</p> </li> <li> <p>Label the true pair as <code>1</code></p> </li> <li>Label the random pairs as <code>0</code></li> </ol> <p>So instead of training a huge softmax classifier, the model now just learns:</p> <ul> <li>\u201cThis is a valid pair\u201d (positive example)</li> <li>\u201cThis is a fake pair\u201d (negative example)</li> </ul> <p>Much cheaper. Much faster.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#an-analogy-security-checkpoint","title":"\u2699\ufe0f An Analogy: Security Checkpoint","text":"<p>Imagine you\u2019re at airport security. Normally, you'd have to check every single bag. That\u2019s what full softmax is doing\u2014exhaustive inspection.</p> <p>Negative sampling is like saying:</p> <ul> <li>\u201cI know what a dangerous item looks like.\u201d</li> <li>\u201cI\u2019ll just check this one good bag... and compare it to 5 suspicious-looking ones.\u201d</li> <li>\u201cIf the good bag looks less dangerous than the others, we\u2019re good!\u201d</li> </ul> <p>Efficient, effective, and gets the job done. \u2708\ufe0f</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#training-loop-with-negative-sampling-skip-gram","title":"\ud83d\udd01 Training Loop with Negative Sampling (Skip-Gram)","text":"<p>Let\u2019s say we\u2019re training on:</p> <pre><code>Input: \"sat\"\nTarget context: \"on\"\n</code></pre> <p>Now:</p> <ul> <li>Choose 4 random words: <code>\"banana\"</code>, <code>\"xylophone\"</code>, <code>\"river\"</code>, <code>\"furnace\"</code></li> </ul> <p>We now train the model to:</p> <ul> <li>Maximize similarity with <code>\"on\"</code> (label = 1)</li> <li>Minimize similarity with each negative sample (label = 0)</li> </ul> <p>This turns one prediction into five quick binary classifications.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#why-negative-sampling-works","title":"\ud83e\udde0 Why Negative Sampling Works","text":"<p>It works well because:</p> <ul> <li>Most word pairs don\u2019t co-occur, so randomly sampled pairs are likely to be wrong.</li> <li>Over time, the model learns to pull real context words together and push others apart.</li> </ul> <p>You don\u2019t need perfect classification over the whole vocab\u2014you just need to learn relative meaning.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html#other-efficiency-trick-hierarchical-softmax-optional","title":"\ud83d\udcda Other Efficiency Trick: Hierarchical Softmax (Optional)","text":"<p>An alternative to negative sampling is Hierarchical Softmax, which organizes words into a binary tree. Predictions become log-time instead of linear-time.</p> <p>While useful, it's a bit more complex, and negative sampling is far more popular due to its simplicity and speed.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html","title":"3.4.5 Practical Example: Training Word2Vec in Python","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#345-practical-example-training-word2vec-in-python","title":"3.4.5 Practical Example: Training Word2Vec in Python","text":"<p>Now that you understand how Word2Vec works conceptually\u2014CBOW vs Skip-Gram, learning word vectors, and speeding things up with negative sampling\u2014it\u2019s time to see it in action.</p> <p>In this section, we\u2019ll walk through:</p> <ul> <li>How to train your own Word2Vec model on a small dataset</li> <li>How to use a pre-trained model</li> <li>How to explore word similarities and analogies</li> </ul> <p>We\u2019ll use the popular <code>gensim</code> library, which offers a simple and efficient implementation of Word2Vec.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#setting-up","title":"\ud83d\udee0\ufe0f Setting Up","text":"<p>First, make sure <code>gensim</code> is installed:</p> <pre><code>pip install gensim\n</code></pre> <p>Now, let\u2019s import what we need:</p> <pre><code>from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\nnltk.download('punkt')\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#step-1-prepare-the-data","title":"\ud83d\udcda Step 1: Prepare the Data","text":"<p>We\u2019ll use a few sample sentences for training. In real use, you\u2019d use thousands or millions of sentences from real-world text.</p> <pre><code>sentences = [\n    \"The cat sat on the mat\",\n    \"The dog barked at the cat\",\n    \"Dogs and cats are common pets\",\n    \"She put the food on the mat\",\n    \"He loves his pet dog\"\n]\n\n# Tokenize the sentences into words\ntokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n</code></pre> <p>Here, we lowercase and tokenize each sentence. Gensim expects a list of word lists like this:</p> <pre><code>[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n ...]\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#step-2-train-the-word2vec-model","title":"\ud83e\udde0 Step 2: Train the Word2Vec Model","text":"<p>Let\u2019s use the Skip-Gram model (<code>sg=1</code>) with a small embedding size and a short context window:</p> <pre><code>model = Word2Vec(\n    sentences=tokenized_sentences,\n    vector_size=50,      # Size of word embeddings\n    window=2,            # Context window size\n    min_count=1,         # Include all words (no threshold)\n    sg=1,                # Use Skip-Gram (0 for CBOW)\n    negative=5,          # Use negative sampling\n    epochs=100           # More epochs for small dataset\n)\n</code></pre> <p>That\u2019s it! The model is trained and ready to use.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#step-3-explore-the-word-embeddings","title":"\ud83d\udd0d Step 3: Explore the Word Embeddings","text":"<p>Let\u2019s try finding similar words:</p> <pre><code>print(model.wv.most_similar(\"cat\"))\n</code></pre> <p>You might get something like:</p> <pre><code>[('dog', 0.87), ('mat', 0.72), ('sat', 0.66), ('barked', 0.61)]\n</code></pre> <p>Try a word that wasn\u2019t in the training data:</p> <pre><code>print(\"food\" in model.wv)  # Should return True if it was seen\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#step-4-look-at-a-word-vector","title":"\ud83e\uddee Step 4: Look at a Word Vector","text":"<p>Each word has a vector of 50 real numbers:</p> <pre><code>print(model.wv['cat'])\n</code></pre> <p>Output (truncated for readability):</p> <pre><code>array([ 0.034, -0.142, ..., 0.095], dtype=float32)\n</code></pre> <p>These vectors are what you\u2019d feed into downstream tasks or visualize.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#optional-save-and-reload-the-model","title":"\ud83e\udde0 Optional: Save and Reload the Model","text":"<pre><code>model.save(\"word2vec-demo.model\")\n# To load it later:\nloaded_model = Word2Vec.load(\"word2vec-demo.model\")\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#try-analogy-tasks-if-trained-on-larger-data","title":"\ud83d\udcca Try Analogy Tasks (If Trained on Larger Data)","text":"<p>With larger datasets, you can try analogies like:</p> <pre><code>model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n</code></pre> <p>This would ideally return something close to <code>\"queen\"</code>\u2014but only if the model has seen enough examples.</p> <p>In our small corpus, this likely won\u2019t work well, but it shows what\u2019s possible.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html#summary","title":"\u2705 Summary","text":"<p>In just a few lines of code, we:</p> <ul> <li>Tokenized sentences</li> <li>Trained a Word2Vec model (Skip-Gram with Negative Sampling)</li> <li>Explored word similarity and vector representations</li> </ul> <p>This simple workflow demonstrates how easy it is to build meaningful embeddings\u2014Word2Vec takes care of the heavy lifting.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html","title":"3.4.6 Strengths and Limitations of Word2Vec","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html#346-strengths-and-limitations-of-word2vec","title":"3.4.6 Strengths and Limitations of Word2Vec","text":"<p>Word2Vec was a major leap forward in how machines understand language\u2014but it\u2019s not perfect. Let\u2019s briefly look at what it does well, and where it falls short.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html#strengths","title":"\u2705 Strengths","text":"<ul> <li>Learns from raw text without needing any labels.</li> <li>Produces dense, low-dimensional embeddings that capture word meaning.</li> <li>Embeddings reflect semantic relationships and analogies (e.g., king - man + woman \u2248 queen).</li> <li>Fast and efficient\u2014especially with Negative Sampling.</li> <li>Outperforms traditional methods like TF-IDF in tasks like similarity, clustering, and classification.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Ignores word order in CBOW (and to some extent in Skip-Gram).</li> <li> <p>Context is fixed\u2014each word has one vector, regardless of meaning.</p> </li> <li> <p>For example, \u201cbank\u201d (river bank vs. money bank) is treated the same.</p> </li> <li>Doesn\u2019t handle out-of-vocabulary words unless retrained.</li> <li>Struggles with morphological variations (e.g., \u201crun\u201d, \u201crunning\u201d, \u201cran\u201d).</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html#why-this-matters","title":"\ud83d\udd04 Why This Matters","text":"<p>Word2Vec was a foundational step in the evolution of NLP. It proved that:</p> <ul> <li>Language meaning could be learned, not programmed.</li> <li>Words could be represented as vectors in space.</li> <li>And that these vectors could power better downstream models.</li> </ul> <p>However, its limitations opened the door to improvements:</p> <ul> <li>FastText (adds subword information to handle morphology)</li> <li>Contextual embeddings like ELMo and BERT, which generate different vectors depending on usage</li> </ul> <p>We\u2019ll explore these advanced methods in later chapters.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5%20Beyond%20Word2Vec%20%E2%80%93%20The%20Evolution%20of%20Word%20Embeddings.html","title":"3.5 Beyond Word2Vec \u2013 The Evolution of Word Embeddings","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5%20Beyond%20Word2Vec%20%E2%80%93%20The%20Evolution%20of%20Word%20Embeddings.html#35-beyond-word2vec-the-evolution-of-word-embeddings","title":"3.5 Beyond Word2Vec \u2013 The Evolution of Word Embeddings","text":"<p>When Word2Vec arrived on the scene, it changed the way we approached language in machine learning. It gave us a powerful, intuitive way to turn words into numbers\u2014and those numbers captured surprisingly deep relationships.</p> <p>But as with any breakthrough, new limitations appeared as the field evolved:</p> <ul> <li>What if a word has multiple meanings?</li> <li>What if the model sees a word it has never seen before?</li> <li>What if we need to understand subtle variations in word forms like \u201crun,\u201d \u201crunning,\u201d and \u201cran\u201d?</li> </ul> <p>Word2Vec gave us a solid foundation, but it still treated words as isolated units, assigning one vector per word, regardless of how it\u2019s used in a sentence.</p> <p>That\u2019s where the next generation of embeddings came in.</p> <p>In this section, we\u2019ll explore how researchers tackled these challenges with smarter, more flexible models:</p> <ul> <li>FastText improved Word2Vec by breaking words into character n-grams, allowing it to generalize to rare or unseen words.</li> <li>GloVe took a different route, combining global word co-occurrence statistics with vector training for richer representations.</li> <li>And finally, we\u2019ll get a glimpse of contextual embeddings\u2014like ELMo and BERT\u2014that changed everything by learning not just what a word means, but what it means right now, in this sentence, in this context.</li> </ul> <p>This section is the bridge between the classical world of static embeddings and the dynamic world of modern large language models. It shows how our understanding of words\u2014and how we represent them\u2014evolved rapidly over just a few years.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html","title":"3.5.1 Why Move Beyond Word2Vec?","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html#351-why-move-beyond-word2vec","title":"3.5.1 Why Move Beyond Word2Vec?","text":"<p>Word2Vec was a major step forward in turning words into meaningful vectors, but like all great innovations, it came with its own set of limitations. These limits didn\u2019t matter at first\u2014they were easy to overlook, especially compared to what came before. But as the field matured and expectations rose, cracks started to show.</p> <p>Let\u2019s unpack the core issues that pushed researchers to go beyond Word2Vec.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html#one-word-one-vector-always","title":"\ud83d\udd01 One Word, One Vector \u2014 Always","text":"<p>In Word2Vec, every word has exactly one vector, no matter how it's used.</p> <p>Take the word \u201cbank\u201d:</p> <ul> <li>In \u201cShe deposited money in the bank,\u201d it refers to a financial institution.</li> <li>In \u201cHe sat on the bank of the river,\u201d it means the edge of a river.</li> </ul> <p>Same spelling, completely different meaning.</p> <p>But Word2Vec doesn\u2019t know that. It assigns one vector to \u201cbank,\u201d blending both meanings. That means the model:</p> <ul> <li>Struggles with polysemy (words with multiple meanings)</li> <li>Can\u2019t distinguish usage based on context</li> </ul> <p>In natural language, context is everything. And Word2Vec, while good at generalizing, just isn\u2019t equipped to handle that kind of nuance.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html#no-understanding-of-subwords","title":"\u274c No Understanding of Subwords","text":"<p>Let\u2019s say you train Word2Vec on English text. It learns the word \u201crun,\u201d but never sees \u201crunning\u201d or \u201crunner.\u201d What happens?</p> <p>It treats each form of the word as completely different\u2014even though they clearly share meaning.</p> <p>This becomes a serious problem with:</p> <ul> <li>Rare words (appearing only a few times)</li> <li>Misspellings or typos</li> <li>Morphologically rich languages (e.g., Finnish, Turkish)</li> </ul> <p>Since Word2Vec sees words as atomic tokens, it can\u2019t break them down. It lacks the ability to say, \u201cAh, \u2018unhappiness\u2019 contains the root \u2018happy\u2019.\u201d</p> <p>This means the model has:</p> <ul> <li>Poor generalization to unseen words</li> <li>Redundant or sparse embeddings for similar word forms</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html#ignores-word-order-and-structure","title":"\ud83e\udde0 Ignores Word Order (and Structure)","text":"<p>CBOW, for instance, just averages surrounding word vectors. This is fast\u2014but it ignores the order of the words.</p> <p>Consider:</p> <ul> <li>\u201cDog bites man\u201d</li> <li>\u201cMan bites dog\u201d</li> </ul> <p>Word2Vec treats the context as a bag of words\u2014so these could look almost the same to the model. But clearly, the meaning has changed dramatically!</p> <p>While Skip-Gram preserves some directional sense, neither CBOW nor Skip-Gram captures sentence structure or syntax effectively.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html#static-shallow-context-blind","title":"\ud83d\udcac Static, Shallow, Context-Blind","text":"<p>To sum up, Word2Vec's weaknesses come down to three main points:</p> Weakness Description One vector per word Can\u2019t handle multiple meanings No subword awareness Struggles with new, rare, or morphologically complex words No context awareness Meaning doesn\u2019t adapt to how a word is used in a sentence <p>These shortcomings opened the door for the next wave of embedding techniques\u2014ones that could:</p> <ul> <li>Capture subword structure</li> <li>Incorporate global statistics</li> <li>And most importantly: adapt to context</li> </ul> <p>In the next sections, we\u2019ll look at how FastText, GloVe, and later contextual models tackled these problems and brought word embeddings to the next level.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html","title":"3.5.2 FastText \u2013 Words as Bags of Subwords","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#352-fasttext-words-as-bags-of-subwords","title":"3.5.2 FastText \u2013 Words as Bags of Subwords","text":"<p>Imagine trying to understand a language without knowing how to break down words like \u201crunning,\u201d \u201crunner,\u201d or \u201chappiness\u201d into parts. That\u2019s essentially what Word2Vec does\u2014it treats every word as a unique, indivisible object.</p> <p>FastText, developed by Facebook AI Research in 2016, offered a clever upgrade:</p> <p>\u201cLet\u2019s look inside the words.\u201d</p> <p>Instead of learning a vector for each word as a whole, FastText learns vectors for parts of words\u2014called character n-grams\u2014and then combines them to build word embeddings.</p> <p>This simple idea makes FastText smarter with rare words, better at generalizing, and more robust to spelling variations and new vocabulary.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#the-key-idea-subword-units","title":"\ud83d\udd20 The Key Idea: Subword Units","text":"<p>FastText represents each word as a collection of character-level n-grams. These are overlapping sequences of characters inside the word.</p> <p>For example, for the word <code>\"playing\"</code> and n-gram size 3, we might get:</p> <pre><code>&lt;pl, pla, lay, ayi, yin, ing, ng&gt;\n</code></pre> <p>(Note: FastText also uses special boundary symbols like <code>&lt;</code> and <code>&gt;</code> to distinguish word edges.)</p> <p>Each n-gram gets its own vector. To create a word vector:</p> <ul> <li>FastText adds up the vectors for all its n-grams.</li> <li>The final embedding represents not just the whole word, but its internal parts.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#why-this-matters","title":"\ud83d\udce6 Why This Matters","text":"<p>This design gives FastText a few powerful advantages over Word2Vec:</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#1-handles-rare-and-unseen-words","title":"\u2705 1. Handles Rare and Unseen Words","text":"<p>If the model has never seen the word <code>\"unhappiness\"</code> before, that\u2019s okay! As long as it has seen <code>\"happy\"</code>, <code>\"ness\"</code>, <code>\"un\"</code>, etc., it can still compose a reasonable embedding by combining subword vectors.</p> <p>This is especially valuable in:</p> <ul> <li>Small datasets</li> <li>Highly inflected languages</li> <li>Misspelled or made-up words (like names or slang)</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#2-learns-morphology-implicitly","title":"\u2705 2. Learns Morphology Implicitly","text":"<p>Words like <code>\"run\"</code>, <code>\"runs\"</code>, <code>\"running\"</code>, and <code>\"runner\"</code> will share many n-grams, so their vectors will naturally be close in space.</p> <p>This leads to better generalization and more compact embedding spaces.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#3-robust-to-typos-and-variants","title":"\u2705 3. Robust to Typos and Variants","text":"<p>Even if a user types <code>\"runnning\"</code> or <code>\"plaiyng\"</code>, FastText can still extract familiar subword patterns, making it less brittle.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#architecturally-speaking","title":"\ud83e\udde0 Architecturally Speaking","text":"<p>Internally, FastText extends the Skip-Gram model. The key difference is that it doesn\u2019t use a one-hot vector for the input word\u2014instead, it uses all the subword vectors to create the input.</p> <p>The rest of the training process (negative sampling, context prediction) works just like in Word2Vec.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#example-training-fasttext-in-python","title":"\ud83e\uddea Example: Training FastText in Python","text":"<p>Let\u2019s train FastText on a few simple sentences using <code>gensim</code>:</p> <pre><code>from gensim.models import FastText\nfrom nltk.tokenize import word_tokenize\n\nsentences = [\n    \"Dogs are running fast\",\n    \"A dog runs faster than a cat\",\n    \"Running is good exercise\"\n]\n\ntokenized = [word_tokenize(sent.lower()) for sent in sentences]\n\nmodel = FastText(\n    sentences=tokenized,\n    vector_size=50,\n    window=3,\n    min_count=1,\n    sg=1,            # Skip-Gram\n    epochs=50\n)\n\n# Try a rare word\nprint(model.wv['runnning'])  # Misspelled on purpose!\n</code></pre> <p>Even though <code>\"runnning\"</code> (with triple n\u2019s) wasn\u2019t seen, FastText can still build a vector using its subwords.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html#when-should-you-use-fasttext","title":"\ud83d\udcca When Should You Use FastText?","text":"<p>Use FastText when:</p> <ul> <li>You\u2019re dealing with morphologically rich languages</li> <li>Your data includes misspellings or rare words</li> <li>You want better generalization from smaller datasets</li> </ul> <p>It\u2019s slightly slower to train than Word2Vec but far more robust.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html","title":"3.5.3 GloVe \u2013 Global Vectors for Word Representation","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#353-glove-global-vectors-for-word-representation","title":"3.5.3 GloVe \u2013 Global Vectors for Word Representation","text":"<p>While Word2Vec and FastText focus on local context windows\u2014predicting words from nearby neighbors\u2014another school of thought asked:</p> <p>\u201cWhat if we looked at the big picture? What if we used global statistics of how words co-occur in an entire corpus?\u201d</p> <p>That\u2019s exactly what GloVe (Global Vectors for Word Representation) does. Developed by researchers at Stanford, GloVe blends the best of both worlds:</p> <ul> <li>The predictive power of neural embeddings (like Word2Vec)</li> <li>The global co-occurrence information found in traditional count-based methods (like TF-IDF)</li> </ul> <p>Let\u2019s see how it works\u2014and why it became another major success in the evolution of NLP.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#the-core-idea","title":"\ud83d\udce6 The Core Idea","text":"<p>At its heart, GloVe builds on a simple intuition:</p> <p>\u201cWords that appear in similar global contexts should have similar vector representations.\u201d</p> <p>But instead of predicting one word from another (like Word2Vec), GloVe looks at how often word pairs occur together across the entire corpus.</p> <p>It creates a co-occurrence matrix, where each cell tells you:</p> <ul> <li>How many times word i appears near word j in the corpus</li> </ul> <p>From this matrix, it learns word vectors that try to preserve the ratios of co-occurrence probabilities.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#a-bit-more-formally","title":"\ud83d\udd22 A Bit More Formally","text":"<p>Let\u2019s say the word \u201cice\u201d co-occurs often with \u201ccold\u201d and rarely with \u201csteam\u201d. Meanwhile, \u201cfire\u201d co-occurs more with \u201csteam\u201d and less with \u201ccold.\u201d</p> <p>GloVe doesn\u2019t just count these occurrences\u2014it looks at their ratios:</p> <p>$$ \\frac{P(\\text{cold} \\mid \\text{ice})}{P(\\text{cold} \\mid \\text{steam})} \\gg 1 $$</p> <p>It then tries to learn word vectors such that:</p> <ul> <li>Their dot product approximates the log of their co-occurrence count</li> <li>This setup ensures that semantic relationships are encoded geometrically</li> </ul> <p>The model is trained by minimizing a weighted least squares loss over all word pairs.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#why-glove-works-well","title":"\ud83d\udcca Why GloVe Works Well","text":"<p>Here\u2019s what makes GloVe unique:</p> Feature Description Global context Uses statistics from the entire corpus (not just a window) Efficient Pre-computes co-occurrence matrix, then trains Interpretable Word relationships emerge naturally in vector space <p>In fact, GloVe is famous for producing very clean vector spaces, where analogies like:</p> <pre><code>king - man + woman \u2248 queen\n</code></pre> <p>emerge even more clearly than in Word2Vec.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#using-pretrained-glove-vectors-in-python","title":"\ud83d\udee0\ufe0f Using Pretrained GloVe Vectors in Python","text":"<p>GloVe is often used with pretrained embeddings, such as the 100-dimensional vectors trained on Wikipedia and Gigaword.</p> <p>Let\u2019s see how to load and use them:</p> <pre><code>import gensim.downloader as api\n\n# Load GloVe vectors (alternative: load from a local .txt file)\nglove_model = api.load(\"glove-wiki-gigaword-100\")\n\n# Find most similar words to 'king'\nprint(glove_model.most_similar(\"king\"))\n</code></pre> <p>You can also do vector math:</p> <pre><code>result = glove_model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])\nprint(result[0])  # likely \"queen\"\n</code></pre>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#when-to-use-glove","title":"\ud83e\udd14 When to Use GloVe?","text":"<p>GloVe is great when:</p> <ul> <li>You want ready-to-use embeddings trained on large corpora</li> <li>You care about semantic similarity and analogy solving</li> <li>You\u2019re doing a task that doesn\u2019t need context-sensitive meaning (like classification)</li> </ul> <p>But GloVe has its own limits:</p> <ul> <li>Like Word2Vec, it gives one vector per word</li> <li>It still can\u2019t differentiate between meanings in different contexts</li> <li>It doesn\u2019t learn from task-specific data\u2014it\u2019s static</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html#wrap-up","title":"\ud83e\udde0 Wrap-up","text":"<p>GloVe showed that you don\u2019t need to use deep neural networks to learn good embeddings\u2014sometimes, word counts + smart math can go a long way.</p> <p>Still, like Word2Vec and FastText, GloVe is a static embedding method. No matter where or how a word appears, it gets the same vector.</p> <p>In the next section, we\u2019ll compare these three approaches side-by-side.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html","title":"3.5.4 Comparing Word2Vec, GloVe, and FastText","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html#354-comparing-word2vec-glove-and-fasttext","title":"3.5.4 Comparing Word2Vec, GloVe, and FastText","text":"<p>By now, you\u2019ve met the three giants of classical word embeddings:</p> <ul> <li>Word2Vec: Predicts words based on local context.</li> <li>FastText: Adds subword awareness to Word2Vec.</li> <li>GloVe: Learns embeddings using global co-occurrence statistics.</li> </ul> <p>Each of these methods has its own strengths, weaknesses, and best-use scenarios. Let\u2019s now put them side by side for a clear comparison.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html#comparison-table","title":"\ud83d\udcca Comparison Table","text":"Feature Word2Vec FastText GloVe Context Type Local (sliding window) Local (sliding window) Global (co-occurrence matrix) Architecture Neural (CBOW/Skip-Gram) Neural + Subword units Matrix factorization Subword Awareness \u274c No \u2705 Yes (character n-grams) \u274c No Handles OOV Words \u274c No \u2705 Yes (via subwords) \u274c No One Vector per Word? \u2705 Yes \u2705 Yes (composed from subwords) \u2705 Yes Training Speed Fast Slightly slower Fast (after matrix is built) Interpretability Medium Medium High (explicit co-occurrence) Pretrained Models? \u2705 Widely available \u2705 Available (less common) \u2705 Very popular Popular Use Cases General NLP, classification Multilingual, noisy data, typos Semantic tasks, analogy solving"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html#key-observations","title":"\ud83d\udd0d Key Observations","text":"<ul> <li>Word2Vec is fast, intuitive, and easy to train\u2014but blind to morphology and word structure.</li> <li>FastText extends Word2Vec to handle rare and new words better by breaking words into subword chunks.</li> <li>GloVe is better at capturing global relationships and is often preferred for tasks involving semantic similarity or analogies.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html#when-to-use-each","title":"\ud83c\udfaf When to Use Each?","text":"Use Case Best Model You want fast training on clean data Word2Vec You need to handle typos or rare words FastText You want to use pretrained vectors quickly GloVe You need high-quality analogies or global meaning GloVe You're working with morphologically rich languages FastText"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html#but-remember-theyre-all-static","title":"\ud83e\udde0 But Remember: They\u2019re All Static","text":"<p>All three models share one major limitation:</p> <p>They assign the same vector to a word, regardless of its context.</p> <p>This means:</p> <ul> <li>They can\u2019t distinguish \u201crock\u201d (music) vs. \u201crock\u201d (stone)</li> <li>They struggle with context shifts and ambiguity</li> <li>They miss the syntactic and semantic flow of a sentence</li> </ul> <p>And that\u2019s what the next generation of models set out to fix.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html","title":"3.6 Limitations of Word Embeddings \u2014 And What Comes Next","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#36-limitations-of-word-embeddings-and-what-comes-next","title":"3.6 Limitations of Word Embeddings \u2014 And What Comes Next","text":"<p>Word embeddings like Word2Vec, GloVe, and FastText helped transform raw text into numbers that machines could understand. They captured surprisingly rich relationships\u2014analogies like \u201cking - man + woman \u2248 queen\u201d amazed even researchers.</p> <p>But they\u2019re not the end of the story.</p> <p>As we used them more deeply, we began to notice cracks\u2014fundamental limitations that prevented them from fully modeling language like a human would.</p> <p>Let\u2019s walk through where word embeddings fall short.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#1-no-understanding-of-context","title":"\ud83e\udde0 1. No Understanding of Context","text":"<p>The word \u201cbank\u201d is a classic example:</p> <ul> <li>\u201cHe sat by the river bank.\u201d</li> <li>\u201cShe deposited cash in the savings bank.\u201d</li> </ul> <p>A human immediately knows these are different meanings. But to Word2Vec or GloVe, \u201cbank\u201d has one vector\u2014used in every situation.</p> <p>This is because traditional embeddings are context-free:</p> <p>A word always means the same thing, no matter where it appears.</p> <p>And that\u2019s just not how language works.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#2-static-and-frozen","title":"\ud83d\udce6 2. Static and Frozen","text":"<p>Once word embeddings are trained, they\u2019re static:</p> <ul> <li>\u201cRunning\u201d will always be mapped to the same vector.</li> <li>The model can\u2019t learn new meanings or adapt to changing usage.</li> <li>If the language or domain evolves (e.g., slang, memes, technical terms), your embeddings quickly become outdated.</li> </ul> <p>They don\u2019t grow with the task or adjust based on sentence structure.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#3-the-oov-problem","title":"\ud83d\udeab 3. The OOV Problem","text":"<p>OOV stands for Out-of-Vocabulary\u2014and it\u2019s a big issue in NLP.</p> <p>If your model never saw the word \u201ccryptocurrency\u201d during training, it has no idea what it is. Word2Vec and GloVe will simply fail.</p> <p>FastText helps a bit by using subwords, but even then, it's more of a patch than a full solution.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#4-no-sentence-level-understanding","title":"\ud83e\udde9 4. No Sentence-Level Understanding","text":"<p>Word embeddings operate at the individual word level. There\u2019s no mechanism for understanding:</p> <ul> <li>Sentence structure</li> <li>Grammar</li> <li>Long-range relationships between words</li> </ul> <p>So while \u201cthe cat sat on the mat\u201d and \u201cthe mat sat on the cat\u201d use the same words, the meaning flips\u2014but the embeddings stay nearly the same.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#what-comes-next-contextual-embeddings","title":"\ud83d\udd2e What Comes Next: Contextual Embeddings","text":"<p>To overcome these problems, a new generation of models emerged\u2014ones that don\u2019t assign words fixed vectors but dynamically adjust based on context.</p> <p>This new class is known as contextual embeddings, and it includes some of the most important models in modern NLP:</p> <ul> <li>ELMo (Embeddings from Language Models): Early contextual model using bidirectional LSTMs</li> <li>BERT (Bidirectional Encoder Representations from Transformers): Learns rich word representations by looking at both sides of a word</li> <li>GPT (Generative Pre-trained Transformer): Learns meaning through large-scale autoregressive prediction</li> </ul> <p>These models answer the question:</p> <p>\u201cWhat does this word mean in this sentence?\u201d</p> <p>They are deep, context-aware, and form the core of most modern language systems\u2014including the Large Language Models that power tools like ChatGPT.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html#a-tease-of-whats-ahead","title":"\ud83c\udf31 A Tease of What\u2019s Ahead","text":"<p>In the next chapter, we\u2019ll explore:</p> <ul> <li>What makes contextual embeddings different (and powerful)</li> <li>How models like BERT and GPT learn to understand sentences, paragraphs, and entire documents</li> <li>The transformer architecture that changed NLP forever</li> </ul> <p>We\u2019re moving from word-level understanding to language-level intelligence.</p> <p>The journey from words to intelligence is just beginning.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html","title":"Chapter 3: Word Embeddings and Dense Representations","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this chapter, the reader will be able to:</p> <ul> <li>Understand what word embeddings are and why they matter.</li> <li>See how embeddings solve the limitations of sparse representations like TF-IDF.</li> <li>Explore how models like Word2Vec, GloVe, and FastText learn embeddings.</li> <li>Visualize word relationships in vector space.</li> <li>Use pre-trained embeddings in code and analyze their structure.</li> <li>Recognize the limitations of static embeddings, and understand why contextual representations (like BERT or GPT) are needed.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#chapter-sections","title":"\ud83d\udcda Chapter Sections","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#31-introduction-from-counting-to-understanding","title":"3.1 Introduction: From Counting to Understanding","text":"<ul> <li>Reflect on the limitations of Bag-of-Words and TF-IDF: sparsity, lack of meaning.</li> <li>Introduce the need for dense, semantic representations.</li> <li>Build the intuitive idea that words appearing in similar contexts tend to have similar meanings.</li> <li>Set the stage for the chapter.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#32-what-are-word-embeddings","title":"3.2 What Are Word Embeddings?","text":"<ul> <li>Introduce word embeddings formally and intuitively.</li> <li>Explain that a word is represented as a dense vector of real numbers (e.g., 100\u2013300 dimensions).</li> <li>Discuss properties like semantic similarity, vector closeness, and continuous representation.</li> <li>Use simple analogies (e.g., \u201cwords on a map\u201d) to make it visual and relatable.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#33-how-embeddings-capture-meaning","title":"3.3 How Embeddings Capture Meaning","text":"<ul> <li>Explain how embeddings are learned from large corpora through co-occurrence and context.</li> <li>Discuss the distributional hypothesis (\u201cYou shall know a word by the company it keeps\u201d).</li> <li> <p>Show how embedding spaces reflect relationships:</p> </li> <li> <p>Clustering: animals, countries, emotions</p> </li> <li>Analogies: <code>king \u2013 man + woman \u2248 queen</code></li> <li>Use visualizations (t-SNE or PCA) and diagrams to show word groupings.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#34-word2vec-learning-embeddings-from-context","title":"3.4 Word2Vec: Learning Embeddings from Context","text":"<ul> <li>Introduce Word2Vec, its origin (Google, 2013), and significance.</li> <li> <p>Explain the two training strategies:</p> </li> <li> <p>CBOW: predict a word from its context</p> </li> <li>Skip-gram: predict the context from a word</li> <li>Dive into the architecture and training logic (without heavy math).</li> <li>Explain the role of context window, negative sampling, and why Word2Vec is unsupervised.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#35-glove-global-vectors-from-co-occurrence","title":"3.5 GloVe: Global Vectors from Co-occurrence","text":"<ul> <li>Explain the motivation behind GloVe (Stanford, 2014).</li> <li> <p>Compare to Word2Vec:</p> </li> <li> <p>Word2Vec: predictive, local</p> </li> <li>GloVe: count-based, global</li> <li>Describe how GloVe uses a co-occurrence matrix and learns embeddings by factorizing it.</li> <li>Show how it balances frequency with meaning.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#36-fasttext-going-beyond-words","title":"3.6 FastText: Going Beyond Words","text":"<ul> <li>Present the key innovation: subword embeddings using character n-grams.</li> <li> <p>Explain how this helps:</p> </li> <li> <p>Represent rare or misspelled words</p> </li> <li>Capture morphological structure (e.g., \u201crun\u201d, \u201crunning\u201d, \u201crunner\u201d)</li> <li>Discuss FastText\u2019s use in multilingual and noisy datasets.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#37-working-with-pretrained-embeddings-code-case-study","title":"3.7 Working with Pretrained Embeddings (Code + Case Study)","text":"<ul> <li>Introduce real-world usage of embeddings.</li> <li> <p>Use <code>gensim</code> or HuggingFace to:</p> </li> <li> <p>Load pre-trained Word2Vec, GloVe, or FastText models</p> </li> <li>Retrieve vectors</li> <li>Compute similarity</li> <li>Visualize clusters with PCA or t-SNE</li> <li>Build a simple classifier (e.g., sentiment analysis) using averaged word vectors.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#38-limitations-of-word-embeddings","title":"3.8 Limitations of Word Embeddings","text":"<ul> <li> <p>Discuss problems that embeddings don\u2019t solve:</p> </li> <li> <p>Context insensitivity: same vector for \u201cbank\u201d in different meanings</p> </li> <li>Static nature: embeddings don\u2019t change based on sentence</li> <li>Out-of-vocabulary (OOV): unknown words = no vector</li> <li>Use real examples to show failure cases.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives.html#39-from-static-to-contextual-the-next-step","title":"3.9 From Static to Contextual: The Next Step","text":"<ul> <li>Set up the transition to contextual embeddings (Chapter 4).</li> <li> <p>Briefly introduce the idea behind ELMo, BERT, and GPT:</p> </li> <li> <p>Vectors now depend on sentence context</p> </li> <li>Meaning is dynamic, not fixed</li> <li>End with a teaser: \u201cHow does a model know what 'Java' means in different sentences? That\u2019s the magic of contextual embeddings.\u201d</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html","title":"Chapter","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#chapter-3-word-embeddings-and-dense-representations","title":"Chapter 3: Word Embeddings and Dense Representations","text":"<p>This is the turning point in your reader\u2019s journey\u2014from basic NLP to the foundational ideas that power modern language models.</p>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this chapter, the reader will:</p> <ul> <li>Understand what word embeddings are and why they\u2019re essential.</li> <li>Learn how embeddings differ from classical approaches like TF-IDF.</li> <li>Explore popular embedding models: Word2Vec, GloVe, and FastText.</li> <li>See how embeddings capture semantic meaning and relationships.</li> <li>Use pre-trained embeddings in real code examples.</li> <li>Understand the limitations of word-level embeddings and the motivation for contextual embeddings (leading into Chapter 4).</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#chapter-structure-and-sections","title":"\ud83e\uddf1 Chapter Structure and Sections","text":""},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#31-introduction-from-counting-words-to-capturing-meaning","title":"3.1 Introduction: From Counting Words to Capturing Meaning","text":"<ul> <li>Recap the limitations of TF-IDF and BoW: sparsity, no meaning, no order.</li> <li>Introduce the idea of representing words in a dense, low-dimensional vector space.</li> <li>Explain the big idea: words with similar meanings have similar vectors.</li> <li>Set up the motivation for learning semantic word embeddings.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#32-what-are-word-embeddings","title":"3.2 What Are Word Embeddings?","text":"<ul> <li>Define word embeddings in simple terms.</li> <li>Explain how each word is mapped to a dense vector of real numbers.</li> <li>Compare sparse vs. dense vectors (visual or table).</li> <li>Use analogy: \u201cembeddings are to words what coordinates are to locations.\u201d</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#33-properties-of-embedding-spaces","title":"3.3 Properties of Embedding Spaces","text":"<ul> <li>Semantic closeness: similar words are close together.</li> <li>Direction and distance capture relationships (e.g., man \u2192 woman, king \u2192 queen).</li> <li>Introduce famous vector analogy:   <code>king - man + woman \u2248 queen</code></li> <li>Visual examples using 2D or 3D projected spaces (mention PCA or t-SNE).</li> <li>Discuss cosine similarity intuitively.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#34-word2vec-learning-embeddings-from-context","title":"3.4 Word2Vec: Learning Embeddings from Context","text":"<ul> <li>Introduce Word2Vec (by Google, 2013).</li> <li> <p>Explain the two architectures:</p> </li> <li> <p>CBOW (Continuous Bag of Words)</p> </li> <li>Skip-gram</li> <li>Intuition: \u201cYou shall know a word by the company it keeps.\u201d</li> <li>Explain how context windows work.</li> <li>Show simplified training process (without deep math).</li> <li>Highlight Word2Vec\u2019s unsupervised nature.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#35-glove-embeddings-from-global-co-occurrence","title":"3.5 GloVe: Embeddings from Global Co-occurrence","text":"<ul> <li>Introduce GloVe (Global Vectors for Word Representation by Stanford).</li> <li> <p>Explain how it differs from Word2Vec:</p> </li> <li> <p>Word2Vec is predictive</p> </li> <li>GloVe is count-based</li> <li>Intuition: builds a co-occurrence matrix and factorizes it.</li> <li>Pros and cons of this approach.</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#36-fasttext-subword-level-embeddings","title":"3.6 FastText: Subword-Level Embeddings","text":"<ul> <li>Explain how FastText improves over Word2Vec.</li> <li> <p>Adds subword (character n-gram) information:</p> </li> <li> <p>Helps with rare words, typos, and morphology.</p> </li> <li>Especially useful for morphologically rich languages (e.g., German, Hindi).</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#37-code-example-using-pretrained-word-embeddings","title":"3.7 Code Example: Using Pretrained Word Embeddings","text":"<ul> <li>Load pretrained vectors (Word2Vec, GloVe, FastText) using <code>gensim</code> or Hugging Face.</li> <li> <p>Show how to:</p> </li> <li> <p>Load a model</p> </li> <li>Get the vector for a word</li> <li>Measure similarity between words</li> <li>Visualize using <code>matplotlib</code> or <code>sklearn.manifold.TSNE</code></li> <li>Optional: small sentiment classifier using averaged word embeddings</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#38-limitations-of-word-embeddings","title":"3.8 Limitations of Word Embeddings","text":"<ul> <li> <p>No understanding of context:</p> </li> <li> <p>\u201cbank\u201d in \u201criver bank\u201d vs. \u201csavings bank\u201d has same vector</p> </li> <li>Embeddings are static once trained</li> <li>OOV (Out-of-vocabulary) problem: can\u2019t handle new words unless subword info is used</li> <li>No sense of sentence-level meaning or syntax</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#39-what-comes-next-contextual-embeddings","title":"3.9 What Comes Next: Contextual Embeddings","text":"<ul> <li>Preview of next chapter</li> <li>Mention BERT, ELMo, GPT as models that create context-aware embeddings</li> <li>Tease the idea: \u201cThe meaning of a word changes depending on the sentence.\u201d</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#optional-sidebarboxes-to-include","title":"\ud83d\udccc Optional Sidebar/Boxes to Include","text":"<ul> <li>\ud83d\udce6 Glossary: vector, cosine similarity, context window, co-occurrence</li> <li>\ud83e\uddea Try This: Use gensim to find top 5 similar words</li> <li>\ud83d\udca1 Visual Idea: Show 2D embedding clusters for related words (e.g., countries, sports, emotions)</li> </ul>"},{"location":"Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter.html#summary-and-takeaways","title":"\ud83d\udcd8 Summary and Takeaways","text":"<ul> <li>Word embeddings give us dense, meaningful representations of words.</li> <li>Word2Vec, GloVe, and FastText were revolutionary in NLP.</li> <li>These embeddings enable machines to understand relationships and similarities between words.</li> <li>But they\u2019re still limited\u2014they don\u2019t know about context.</li> <li>That sets the stage for the next big leap: contextual language models.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html","title":"Chapter","text":""},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#chapter-3-introduction-to-machine-learning-and-neural-networks","title":"\ud83d\udcd8 Chapter 3: Introduction to Machine Learning and Neural Networks","text":""},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#1-learning-objectives","title":"1. Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ul> <li>Understand how Machine Learning differs from traditional programming.</li> <li>Identify and define basic ML concepts such as features, labels, models, training, testing, and loss.</li> <li>Explain how simple neural networks function.</li> <li>Understand the importance of activation functions and how they add non-linearity.</li> <li>Describe how backpropagation and optimizers like SGD or Adam work.</li> <li>Implement a basic neural network using PyTorch.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#2-introduction","title":"2. Introduction","text":"<ul> <li>Why are machines learning?</li> <li>From symbolic rules to statistical learning: the shift in AI.</li> <li>The role of Machine Learning in modern AI systems, especially LLMs.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#3-core-sections","title":"3. Core Sections","text":""},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#31-traditional-programming-vs-machine-learning","title":"3.1 Traditional Programming vs. Machine Learning","text":"<ul> <li>\u201cIf-else\u201d vs. \u201clearn-from-data\u201d approaches.</li> <li>Analogy: Teaching a child rules vs. letting them learn from examples.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#32-what-is-machine-learning","title":"3.2 What is Machine Learning?","text":"<ul> <li>Defining ML in simple language.</li> <li>Types of ML: Supervised, Unsupervised, and Reinforcement Learning (brief overview with examples).</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#33-anatomy-of-a-machine-learning-problem","title":"3.3 Anatomy of a Machine Learning Problem","text":"<ul> <li>Features and labels explained with examples.</li> <li>The structure of a typical ML dataset.</li> <li>Model, predictions, and loss: key players in the ML pipeline.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#34-training-and-testing-a-model","title":"3.4 Training and Testing a Model","text":"<ul> <li>The idea of generalization.</li> <li>Why we split data: training, validation, and test sets.</li> <li>The concept of overfitting and underfitting, visualized.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#35-types-of-problems-ml-solves","title":"3.5 Types of Problems ML Solves","text":"<ul> <li>Classification vs. Regression (with intuitive examples).</li> <li>Examples from NLP: spam detection, sentiment analysis.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#36-from-linear-models-to-neural-networks","title":"3.6 From Linear Models to Neural Networks","text":"<ul> <li>Limitations of linear models.</li> <li>Need for multiple layers and non-linear boundaries.</li> <li>First intuition of a neural network.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#37-the-building-blocks-of-neural-networks","title":"3.7 The Building Blocks of Neural Networks","text":"<ul> <li>Neurons: mathematical units.</li> <li>Layers: input, hidden, and output.</li> <li>Visual representation of a simple feedforward network.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#38-activation-functions","title":"3.8 Activation Functions","text":"<ul> <li>Why linear models fall short.</li> <li>What activation functions do (intuitively and mathematically).</li> <li> <p>Common types:</p> </li> <li> <p><code>ReLU</code></p> </li> <li><code>Sigmoid</code></li> <li><code>Tanh</code></li> <li>(Optional) Mention of <code>Softmax</code> for output layers</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#39-forward-pass-and-predictions","title":"3.9 Forward Pass and Predictions","text":"<ul> <li>The data flow from input to output.</li> <li>How predictions are made.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#310-understanding-loss-functions","title":"3.10 Understanding Loss Functions","text":"<ul> <li>What is loss?</li> <li> <p>Examples of loss functions:</p> </li> <li> <p><code>MSE</code> (Mean Squared Error)</p> </li> <li><code>Cross-Entropy</code> (for classification)</li> <li>Why minimizing loss is the goal.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#311-backpropagation-how-learning-happens","title":"3.11 Backpropagation: How Learning Happens","text":"<ul> <li>Intuition: feedback and blame assignment.</li> <li>A beginner-friendly walkthrough of backpropagation.</li> <li>The role of gradients.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#312-optimizers-turning-gradients-into-learning","title":"3.12 Optimizers: Turning Gradients into Learning","text":"<ul> <li>What is an optimizer?</li> <li>How <code>SGD</code> and <code>Adam</code> help a model improve.</li> <li>Learning rate and its effects.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#313-training-loop-in-practice","title":"3.13 Training Loop in Practice","text":"<ul> <li>Epochs, batches, and iterations.</li> <li>Step-by-step flow of training a model.</li> <li>Monitoring and evaluating performance.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#314-code-walkthrough-training-a-tiny-neural-network-in-pytorch","title":"3.14 Code Walkthrough: Training a Tiny Neural Network in PyTorch","text":"<ul> <li>A fully commented beginner-level example.</li> <li>Code trains a network to solve a basic classification or regression task.</li> <li>Explanation of inputs, model, training steps, and outputs.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#315-visualizing-learning","title":"3.15 Visualizing Learning","text":"<ul> <li>Plotting loss over time.</li> <li>Optional: decision boundaries for 2D data.</li> <li>Visual idea of how the model learns better with each epoch.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#4-code-example-case-study","title":"4. Code Example / Case Study","text":"<ul> <li>Full code in a real environment with explanations.</li> <li>Emphasis on how each component maps to theory (data \u2192 model \u2192 loss \u2192 optimization).</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#5-summary","title":"5. Summary","text":"<ul> <li>A narrative wrap-up of all concepts in plain language.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#6-key-takeaways","title":"6. Key Takeaways","text":"<ul> <li>Bullet points summarizing main concepts, formulas, and intuition.</li> </ul>"},{"location":"Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html#7-quiz-exercises-optional","title":"7. Quiz / Exercises (Optional)","text":"<ul> <li>Concept checks: match-the-following, fill in the blanks.</li> <li>Practice: train the same model with different activation functions and observe.</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.1%20Introduction%20%E2%80%93%20Why%20Word%20Order%20and%20Memory%20Matter.html","title":"4.1 Introduction \u2013 Why Word Order and Memory Matter","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.1%20Introduction%20%E2%80%93%20Why%20Word%20Order%20and%20Memory%20Matter.html#41-introduction-why-word-order-and-memory-matter","title":"4.1 Introduction \u2013 Why Word Order and Memory Matter","text":"<p>In the previous chapter, we explored how words can be transformed into vectors\u2014dense numerical representations that capture the meaning of a word based on how it's used in language. Word2Vec, GloVe, and FastText gave us a powerful starting point: machines could now recognize that king and queen are related, or that bank and money often appear in similar contexts.</p> <p>But there\u2019s a big catch.</p> <p>All of these models treat words as if they have just one meaning, regardless of the sentence they appear in. For example, the word light can mean something you switch on in a room, something that's not heavy, or even a kind of beer. These are very different meanings\u2014and yet, traditional word embeddings give light a single vector. That\u2019s a problem.</p> <p>Even more importantly, these models ignore word order. Whether a sentence says \u201cthe cat chased the dog\u201d or \u201cthe dog chased the cat,\u201d the Bag-of-Words model\u2014and even Word2Vec to some extent\u2014would represent them nearly the same. That\u2019s clearly wrong. Word order and structure matter deeply in language.</p> <p>So how can we build models that understand this?</p> <p>Let\u2019s think like a machine for a moment. Imagine you\u2019re trying to understand a sentence by reading it one word at a time:</p> <p>\u201cThe man who wore a red hat\u2026\u201d</p> <p>At this point, what comes next? Maybe:</p> <p>\u201c\u2026sat down.\u201d \u201c\u2026walked away.\u201d \u201c\u2026was arrested.\u201d</p> <p>The correct prediction depends not just on the last word (hat), but also on the earlier part of the sentence (the man who wore\u2026). You need memory\u2014a way to carry forward information from earlier in the sentence to inform what comes next.</p> <p>This is where sequence models come in. They\u2019re designed to handle exactly this kind of problem\u2014by processing input one step at a time, remembering past information, and using it to make better predictions in the future.</p> <p>The first generation of such models in natural language processing were called Recurrent Neural Networks (RNNs). They marked a huge leap forward: for the first time, models could understand a sentence not just as a bag of words, but as a flow of meaning across time.</p> <p>In this chapter, we\u2019ll dive into how these models work. We\u2019ll explore RNNs and their improved variants\u2014LSTM and GRU\u2014and learn how they give machines a kind of memory. We\u2019ll see how they\u2019re trained to generate text, and how they opened the door to more advanced architectures like Transformers (which we\u2019ll explore in the next chapter).</p> <p>Let\u2019s begin our journey into sequence modeling\u2014starting with the core idea behind RNNs.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.1%20The%20Need%20for%20Sequential%20Processing.html","title":"4.2.1 The Need for Sequential Processing","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.1%20The%20Need%20for%20Sequential%20Processing.html#421-the-need-for-sequential-processing","title":"4.2.1 The Need for Sequential Processing","text":"<p>Imagine you\u2019re reading a sentence one word at a time. As you move from left to right, each new word builds on the ones before it:</p> <p>\u201cShe opened the door and saw a\u2026\u201d</p> <p>At this point, your brain is making predictions. A what? A cat? A surprise? A ghost?</p> <p>Now let\u2019s change just one word earlier in the sentence:</p> <p>\u201cShe locked the door and saw a\u2026\u201d</p> <p>That tiny change\u2014opened to locked\u2014can completely shift what we expect next. Human language is like that: deeply sensitive to order, timing, and context.</p> <p>So far, the models we\u2019ve discussed\u2014like Word2Vec or Bag-of-Words\u2014don't consider word order at all. To them, the sentence \u201cThe cat chased the mouse\u201d is just a bag of words: {cat, chased, the, mouse}. You could shuffle them around\u2014\u201cMouse chased the cat the\u201d\u2014and the model wouldn't notice the difference.</p> <p>Clearly, this is a big problem. In real language, order matters. A lot.</p> <p>Let\u2019s say we want to teach a machine to complete a sentence like this:</p> <p>\u201cThe stock market crashed because\u2026\u201d</p> <p>The right prediction depends on everything that came before. A machine needs to remember previous words and use that memory to understand the next ones. This is the fundamental idea behind sequence modeling.</p> <p>Now you might ask: why not just use a regular neural network and feed it the entire sentence? The issue is that standard feedforward networks expect fixed-size input. They process all inputs independently, without any sense of time or order. In short, they have no memory.</p> <p>That\u2019s where Recurrent Neural Networks, or RNNs, come in.</p> <p>RNNs were designed to model sequences\u2014text, speech, music, even stock prices\u2014by maintaining a sort of running memory of what came before. As each new word (or input) is read, the RNN updates its internal state, which is then used to process the next input. This allows it to build a contextual understanding, step by step.</p> <p>Think of it like reading a sentence with a highlighter in hand. Each new word influences what you highlight next. You don\u2019t start from scratch every time\u2014you carry your understanding forward.</p> <p>This ability to \u201cremember\u201d previous inputs is what makes RNNs such a natural fit for language. In the next section, we\u2019ll explore exactly how this memory works\u2014and how it's implemented using the hidden state of a recurrent neural network.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html","title":"4.2.2 The Core Idea of Recurrence","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html#422-the-core-idea-of-recurrence","title":"4.2.2 The Core Idea of Recurrence","text":"<p>At the heart of a Recurrent Neural Network (RNN) is a simple but powerful idea: what you\u2019ve already seen should help you understand what comes next.</p> <p>Let\u2019s break that down.</p> <p>In a standard feedforward neural network, you give the model an input, it processes that input, and then it produces an output. Done. It doesn\u2019t remember anything about what came before. That\u2019s fine for tasks where each input is independent\u2014like identifying whether an image contains a cat or not\u2014but it doesn\u2019t work well for language, where meaning depends on history.</p> <p>RNNs fix this by introducing a concept called a hidden state. Think of it as the model\u2019s memory. At each step in a sequence, the RNN updates this memory based on two things:</p> <ol> <li>The current input (the word it\u2019s reading now)</li> <li>The previous hidden state (its memory from the past)</li> </ol> <p>Together, these allow the RNN to form a new understanding of the sentence so far.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html#a-little-math-gently-introduced","title":"\ud83e\udde0 A Little Math (Gently Introduced)","text":"<p>Let\u2019s say we\u2019re processing a sentence word by word. At time step <code>t</code>, the RNN receives:</p> <ul> <li>The input word vector: $x_t$</li> <li>The previous hidden state: $h_{t-1}$</li> </ul> <p>It then computes the new hidden state $h_t$ like this:</p> <p>$$ h_t = \\tanh(W \\cdot x_t + U \\cdot h_{t-1} + b) $$</p> <p>Here\u2019s what that means:</p> <ul> <li>$W$ is a weight matrix that transforms the input</li> <li>$U$ is another matrix that transforms the hidden state</li> <li>$b$ is a bias term</li> <li>$\\tanh$ is the activation function that squashes values between \u20131 and 1</li> </ul> <p>Each time the RNN reads a new word, it blends that word with its current memory, updates its hidden state, and passes it forward.</p> <p>You can visualize it like this:</p> <pre><code>x_t \u2500\u2500\u25b6[ RNN Cell ]\u2500\u2500\u25b6 h_t\n       \u25b2       \u2502\n       \u2502       \u25bc\n     h_{t-1}  Output\n</code></pre> <p>Each RNN Cell has a loop inside it\u2014this is what allows it to carry memory from one step to the next. It\u2019s why we call it recurrent.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html#analogy-reading-a-story","title":"\ud83d\udcda Analogy: Reading a Story","text":"<p>Imagine reading a mystery novel. As each new clue is revealed, you update your mental model of who might be the culprit. That internal model\u2014the accumulation of everything you\u2019ve read so far\u2014is like the RNN\u2019s hidden state. Each new piece of information (a sentence or paragraph) modifies your understanding just a bit. You don\u2019t forget everything that came before\u2014you build on it.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html#why-it-matters","title":"\ud83e\udde9 Why It Matters","text":"<p>This structure allows the RNN to capture patterns over time. It can learn that \u201cThe cat sat on the\u201d is likely to be followed by \u201cmat,\u201d or that the phrase \u201cI am feeling very\u201d could lead to \u201chappy,\u201d \u201csad,\u201d or \u201ctired\u201d depending on the context.</p> <p>But while this idea of recurrence is powerful, it's not without flaws. As sequences grow longer, the RNN\u2019s memory begins to fade. It becomes harder for the model to carry meaningful signals across many steps\u2014a problem we\u2019ll explore in a later section on vanishing gradients.</p> <p>Before we get there, let\u2019s first look at how RNNs work when you process an entire sentence, one word at a time. That\u2019s what we\u2019ll cover next in 4.2.3: Unrolling an RNN Over Time.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html","title":"4.2.3 Unrolling an RNN Over Time","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html#423-unrolling-an-rnn-over-time","title":"4.2.3 Unrolling an RNN Over Time","text":"<p>So far, we\u2019ve treated the RNN like a black box: it takes an input, uses its memory (the hidden state), and produces a new hidden state. But what happens when we feed it an entire sequence\u2014like a sentence or paragraph?</p> <p>To understand that, we need to \u201cunroll\u201d the RNN.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html#what-does-unrolling-mean","title":"\ud83e\udde0 What Does \"Unrolling\" Mean?","text":"<p>An RNN is a recursive structure. That means it reuses the same logic (and the same weights) again and again\u2014once for each time step in the input.</p> <p>To visualize this, we can imagine the RNN stretched out in time. Each copy of the RNN processes one word and passes its hidden state to the next:</p> <pre><code>x\u2081 \u2500\u25b6 [ RNN Cell ] \u2500\u25b6 h\u2081\n        \u2193\nx\u2082 \u2500\u25b6 [ RNN Cell ] \u2500\u25b6 h\u2082\n        \u2193\nx\u2083 \u2500\u25b6 [ RNN Cell ] \u2500\u25b6 h\u2083\n        \u2193\n...     ...         ...\n</code></pre> <p>Each <code>[ RNN Cell ]</code> in this diagram is actually the same RNN, with the same parameters. It\u2019s not a deep network with many layers\u2014it\u2019s a repeated application of a single RNN unit.</p> <p>This is what we mean by weight sharing: all the cells in the unrolled diagram use the same set of weights $W, U, b$. That makes the model more efficient and helps it generalize better to sequences of varying lengths.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html#step-by-step-processing","title":"\ud83d\udd01 Step-by-Step Processing","text":"<p>Let\u2019s walk through an example sentence:</p> <p>\"I love cats\"</p> <ol> <li> <p>The first word, <code>\"I\"</code>, is converted to a vector $x_1$.    The RNN uses this and an initial hidden state $h_0$ (usually set to zeros) to compute $h_1$.</p> </li> <li> <p>The second word, <code>\"love\"</code>, is converted to $x_2$.    The RNN uses $x_2$ and the hidden state $h_1$ to compute $h_2$.</p> </li> <li> <p>The third word, <code>\"cats\"</code>, is processed using $x_3$ and $h_2$, producing $h_3$.</p> </li> </ol> <p>The final hidden state $h_3$ now contains a compressed summary of the entire sentence so far.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html#whats-so-special-about-this","title":"\ud83e\udde0 What\u2019s So Special About This?","text":"<p>This process is what allows the RNN to carry information forward through a sentence. It\u2019s not just looking at the current word\u2014it\u2019s combining the current word with what it has already learned.</p> <p>This is crucial for tasks like:</p> <ul> <li>Next word prediction: \"The dog barked at the ___\"</li> <li>Sentiment analysis: \"I absolutely hated the movie\"</li> <li>Sequence labeling: tagging each word with its part of speech</li> </ul> <p>At each time step, the model can output something\u2014like a predicted word or tag\u2014or wait until the end of the sentence and output a single decision.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html#backpropagation-through-time-bptt","title":"\ud83d\udd04 Backpropagation Through Time (BPTT)","text":"<p>When we train an RNN, we don\u2019t just update the weights after one time step\u2014we propagate the error backward through all time steps.</p> <p>This process is called Backpropagation Through Time, or BPTT. It\u2019s like backpropagation in regular neural networks, except we stretch it out over the entire sequence. This allows the model to learn from patterns that span many steps.</p> <p>However, this also introduces some serious challenges, which we\u2019ll explore next.</p> <p>As sequences get longer, the gradients that flow backward can start to shrink (or sometimes explode). This makes it hard for the model to remember information from early in the sequence. This problem is known as the vanishing gradient problem, and it\u2019s one of the biggest limitations of vanilla RNNs.</p> <p>In the next section, we\u2019ll understand this problem in more depth and see why it motivated the creation of more advanced architectures like LSTM and GRU.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html","title":"4.2.4 RNN in Practice \u2013 A Step by Step Example","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#424-rnn-in-practice-a-step-by-step-example","title":"4.2.4 RNN in Practice \u2013 A Step-by-Step Example","text":"<p>Let\u2019s now solidify everything we\u2019ve learned so far by walking through a working example. First, we\u2019ll see how the hidden states evolve as we process a sentence. Then, we\u2019ll implement a basic RNN from scratch in Python, without using any deep learning frameworks.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#example-i-love-cats","title":"\ud83d\udcda Example: \u201cI love cats\u201d","text":"<p>We\u2019ll process the sentence \u201cI love cats\u201d using an RNN.</p> <p>Assume the following:</p> <ul> <li>Our vocabulary is: <code>[\"I\", \"love\", \"cats\"]</code></li> <li> <p>We assign one-hot vectors:</p> </li> <li> <p><code>I     \u2192 [1, 0, 0]</code></p> </li> <li><code>love  \u2192 [0, 1, 0]</code></li> <li><code>cats \u2192 [0, 0, 1]</code></li> <li>Input size = 3 (because of 3 unique words)</li> <li>Hidden size = 2 (we\u2019ll keep this small for simplicity)</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#rnn-equations","title":"\ud83e\uddee RNN Equations","text":"<p>At each time step $t$, the RNN updates the hidden state using:</p> <p>$$ h_t = \\tanh(Wx_t + Uh_{t-1} + b) $$</p> <p>Where:</p> <ul> <li>$x_t$ is the current input vector</li> <li>$h_{t-1}$ is the previous hidden state</li> <li>$W$, $U$, and $b$ are trainable parameters</li> </ul> <p>We\u2019ll initialize everything manually and walk through the computation.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#python-implementation-from-scratch","title":"\ud83d\udcbb Python Implementation (from Scratch)","text":"<pre><code>import numpy as np\n\n# ---- Step 1: Define the vocabulary ----\nvocab = [\"I\", \"love\", \"cats\"]\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n\n# ---- Step 2: One-hot encoding ----\ndef one_hot(idx, size):\n    vec = np.zeros(size)\n    vec[idx] = 1.0\n    return vec\n\n# ---- Step 3: Initialize weights and hidden state ----\ninput_size = 3     # one-hot size\nhidden_size = 2    # size of the RNN memory\n\n# Weights (random small numbers)\nW = np.random.randn(hidden_size, input_size) * 0.1\nU = np.random.randn(hidden_size, hidden_size) * 0.1\nb = np.zeros(hidden_size)\n\n# Initial hidden state\nh_prev = np.zeros(hidden_size)\n\n# ---- Step 4: Input sentence ----\nsentence = [\"I\", \"love\", \"cats\"]\n\nprint(\"Step-by-step hidden states:\\n\")\n\nfor word in sentence:\n    x = one_hot(word_to_ix[word], input_size)  # input vector\n    h_t = np.tanh(np.dot(W, x) + np.dot(U, h_prev) + b)  # RNN update\n    print(f\"Word: {word:5}  | Hidden state: {h_t}\")\n    h_prev = h_t  # carry hidden state forward\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#whats-happening","title":"\ud83e\udde0 What\u2019s Happening?","text":"<p>At each step:</p> <ul> <li>The RNN reads the current word vector</li> <li>It combines that with the memory (<code>h_prev</code>)</li> <li>The result is a new hidden state that reflects the sentence so far</li> </ul> <p>By the end of the sentence, the final hidden state contains a summary of:</p> <p>\u201cI love cats\u201d</p> <p>If we were training a language model, the next step would be to use this hidden state to predict the next word. But even without prediction, you can see how the RNN builds memory step-by-step.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html#why-this-matters","title":"\ud83d\udd0d Why This Matters","text":"<p>This kind of \u201cmanual\u201d RNN helps demystify what\u2019s often treated as a black box. You can see clearly how:</p> <ul> <li>Each word updates the memory</li> <li>The model keeps a continuous state across time</li> <li>Everything is differentiable and trainable</li> </ul> <p>But\u2026 we also start to see some weaknesses.</p> <p>Even in this tiny example, what happens if we add five more words before \u201ccats\u201d? Will the memory still be strong enough to understand the context? Probably not.</p> <p>That\u2019s the core weakness of simple RNNs\u2014they struggle with long-term memory. And that\u2019s exactly where we\u2019re headed next.</p> <p>In the following section, we\u2019ll explore the vanishing gradient problem, a major limitation of vanilla RNNs, and why it led to the invention of more powerful units like LSTM and GRU.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html","title":"4.2.5: Strengths and Weaknesses of Vanilla RNNs","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html#425-strengths-and-weaknesses-of-vanilla-rnns","title":"4.2.5 Strengths and Weaknesses of Vanilla RNNs","text":"<p>By now, you\u2019ve seen how Recurrent Neural Networks (RNNs) can read a sentence one word at a time, carry memory from word to word, and gradually build a hidden representation of the entire sequence. That\u2019s already a huge improvement over traditional bag-of-words models or even Word2Vec embeddings, which ignore the order and structure of a sentence.</p> <p>So what are RNNs really good at? And where do they fall short?</p> <p>Let\u2019s break it down.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html#strengths-of-vanilla-rnns","title":"\u2705 Strengths of Vanilla RNNs","text":"<ol> <li>Handles Sequences of Variable Length</li> </ol> <p>RNNs don\u2019t require input of a fixed size like feedforward networks. You can feed them sentences that are 3 words long or 300 words long\u2014they\u2019ll process each step the same way, updating the hidden state as they go.</p> <ol> <li>Captures Temporal Dependencies</li> </ol> <p>RNNs introduce the critical idea of temporal modeling\u2014the meaning of a word depends on its neighbors. They can learn that \u201cgood morning\u201d is common, but \u201cgood elephant\u201d is not. This makes them suitable for a wide range of NLP tasks: next-word prediction, part-of-speech tagging, named entity recognition, and more.</p> <ol> <li>Parameter Sharing Across Time Steps</li> </ol> <p>Since RNNs use the same weights at every time step, they\u2019re efficient and generalizable. The same logic that processes word #1 can process word #50.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html#weaknesses-of-vanilla-rnns","title":"\u274c Weaknesses of Vanilla RNNs","text":"<ol> <li>Short-Term Memory Only</li> </ol> <p>While RNNs can remember what happened one or two steps ago, they quickly \u201cforget\u201d things as the sequence gets longer. For example, in the sentence:</p> <p>\u201cThe book that the student who the teacher liked was reading was long.\u201d</p> <p>A simple RNN might struggle to relate the first \u201cbook\u201d to the final \u201cwas long,\u201d because the relevant words are too far apart. This is a long-term dependency, and vanilla RNNs are not good at handling it.</p> <ol> <li>Vanishing Gradient Problem</li> </ol> <p>When training an RNN using backpropagation through time, gradients are passed backward across many steps. In long sequences, these gradients can become very small (vanish) or very large (explode). When gradients vanish, the model fails to learn long-range patterns.</p> <p>In simpler terms: the further back you try to remember, the weaker your memory becomes during training.</p> <ol> <li>Slow Training and Inference</li> </ol> <p>RNNs process inputs sequentially\u2014they can\u2019t parallelize across time steps. This makes them slower than newer architectures like Transformers, which we\u2019ll encounter soon.</p> <ol> <li>Difficult to Tune</li> </ol> <p>Vanilla RNNs can be unstable during training. Choosing the right learning rate, weight initialization, and sequence length often requires trial and error.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html#summary-rnns-are-a-good-start-but-not-enough","title":"\ud83d\udd01 Summary: RNNs Are a Good Start, But Not Enough","text":"<p>Vanilla RNNs were revolutionary when they first appeared. They introduced the key idea that language is sequential and that memory matters.</p> <p>But as we push toward modeling longer sequences and more complex dependencies, RNNs begin to break down.</p> <p>That\u2019s why researchers created improved versions of RNNs: the Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU). These models were designed specifically to fix the memory limitations of vanilla RNNs, while keeping the core idea of sequence modeling intact.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html","title":"4.3 The Problem of Vanishing Gradients","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#43-the-problem-of-vanishing-gradients","title":"4.3 The Problem of Vanishing Gradients","text":"<p>At this point, we've seen how Recurrent Neural Networks (RNNs) can pass information from one step to the next through a hidden state. This hidden state acts like a memory that gets updated every time a new word is read.</p> <p>But here\u2019s the problem: RNNs have a short memory. They might do well on short phrases like:</p> <p>\u201cThe cat sat on the mat.\u201d</p> <p>But they struggle with longer dependencies, like:</p> <p>\u201cThe cat, which had been missing for several days and was finally found near the old barn by the river, sat on the mat.\u201d</p> <p>The model might completely forget about \u201cthe cat\u201d by the time it reaches \u201csat on the mat\u201d.</p> <p>Why does this happen? To understand that, we need to take a peek under the hood and talk about one of the most important training challenges in deep learning: the vanishing gradient problem.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#a-quick-refresher-what-is-a-gradient","title":"\ud83e\udde0 A Quick Refresher: What Is a Gradient?","text":"<p>When training a neural network, we use a method called backpropagation to update its weights. This involves calculating a gradient\u2014a kind of signal that tells each weight how much it contributed to the error, and how it should change to improve.</p> <p>If the gradient is large, the weight gets updated significantly. If it\u2019s small, the update is tiny.</p> <p>This works great in short networks. But in deep networks\u2014or in long unrolled RNNs\u2014a single mistake at the end of the sequence has to travel backward through many time steps to adjust earlier weights.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#the-vanishing-gradient-problem","title":"\ud83e\uddef The Vanishing Gradient Problem","text":"<p>As this gradient flows backward through each step of the RNN, it often gets multiplied by small numbers (less than 1). After 10 or 20 steps, the gradient becomes so small that it\u2019s almost zero.</p> <p>It \u201cvanishes.\u201d</p> <p>This means the early parts of the sequence stop receiving useful updates during training. The model can\u2019t \u201clearn\u201d to remember what happened a long time ago.</p> <p>Imagine trying to pass a message down a line of 50 people by whispering it ear to ear. By the time it reaches the end, the message is barely recognizable. That\u2019s what happens to the gradient during backpropagation through time.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#the-exploding-gradient-problem","title":"\ud83d\udca5 The Exploding Gradient Problem","text":"<p>There's a twin problem called the exploding gradient\u2014when the gradient becomes too large instead of too small. This causes unstable updates and can crash the training.</p> <p>While exploding gradients can be controlled using techniques like gradient clipping, vanishing gradients are harder to fix.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#why-vanilla-rnns-struggle","title":"\ud83d\ude15 Why Vanilla RNNs Struggle","text":"<p>The recurrence formula of RNNs uses a nonlinear activation function like <code>tanh</code> or <code>sigmoid</code>. These functions squash outputs into a limited range, which contributes to shrinking gradients over time.</p> <p>So while RNNs can theoretically model long-term dependencies, in practice, they rarely do unless the sequence is very short or the pattern is very strong.</p> <p>This limitation sparked a breakthrough in neural network design\u2014architectures that could learn to remember more effectively, by controlling what to store and what to forget.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html#enter-lstm-and-gru","title":"\ud83d\udca1 Enter LSTM and GRU","text":"<p>To overcome these problems, researchers developed new types of RNN cells\u2014most notably the Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU).</p> <p>These models introduced gates that decide:</p> <ul> <li>What new information to add</li> <li>What old information to forget</li> <li>What to keep and carry forward</li> </ul> <p>By doing this, they solve the vanishing gradient problem and allow the network to capture long-range dependencies in text.</p> <p>In the next section, we\u2019ll explore the LSTM in detail: how it works, what it looks like inside, and how it enables machines to hold on to important information for longer periods.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html","title":"4.4.1 Motivation: Why Vanilla RNNs Need Help","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html#441-motivation-why-vanilla-rnns-need-help","title":"4.4.1 Motivation: Why Vanilla RNNs Need Help","text":"<p>Let\u2019s say you\u2019re reading a novel. In Chapter 1, a character named Anna is introduced as a violinist with a fear of flying. Chapters later, during a concert scene in Paris, this fear is suddenly relevant\u2014but only if you remembered it from the beginning.</p> <p>Human readers have no problem holding onto important facts across long stretches of text. But for standard RNNs, this is where things start to break down.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html#the-memory-struggle","title":"\ud83e\udd2f The Memory Struggle","text":"<p>As we saw earlier, vanilla RNNs update their hidden state at every time step based on the current input and the previous state. This is like keeping a mental note as you read each word in a sentence. In theory, the hidden state should store all the important information it\u2019s seen so far.</p> <p>In practice, however, that memory fades\u2014quickly.</p> <p>Here\u2019s why:</p> <ul> <li>Each time the RNN processes a new word, the hidden state gets updated.</li> <li>These updates are based on nonlinear functions (like <code>tanh</code>) that squash values into a small range.</li> <li>When you apply this over many time steps, the influence of earlier words becomes smaller and smaller\u2014until it\u2019s effectively gone.</li> </ul> <p>This is the vanishing gradient problem in action.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html#a-simple-example","title":"\ud83e\uddea A Simple Example","text":"<p>Take this sentence:</p> <p>\u201cThe cat that the dog chased across the garden ran away.\u201d</p> <p>To understand what ran away, the model needs to know the subject: The cat. But this was several words ago. A vanilla RNN is likely to lose track of it by the time it reaches the word ran.</p> <p>So while vanilla RNNs are technically \u201crecurrent,\u201d they\u2019re only really good at remembering the last 2\u20133 words. Beyond that, they tend to forget.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html#long-term-dependencies-a-real-need","title":"\ud83e\udde0 Long-Term Dependencies: A Real Need","text":"<p>Language is full of long-term dependencies. Here are just a few examples where long memory is essential:</p> <ul> <li>Subject-verb agreement across long clauses:</li> </ul> <p>\u201cThe collection of rare books was stolen.\u201d * Contextual meaning:</p> <p>\u201cI placed the cake on the table and left the room. When I came back, it was gone.\u201d (What was gone? The cake. From two sentences ago.) * Sentiment expressed early and reversed later:</p> <p>\u201cAt first, I thought the movie was boring, but then\u2026\u201d</p> <p>RNNs often miss these subtleties because they don\u2019t know how to hold onto critical details over long distances.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html#what-we-need-a-better-memory-system","title":"\ud83d\udca1 What We Need: A Better Memory System","text":"<p>What if we could give the model a memory it could choose to retain or forget?</p> <ul> <li>It could decide when something is important enough to remember.</li> <li>It could ignore irrelevant details.</li> <li>It could keep useful information across dozens\u2014or even hundreds\u2014of words.</li> </ul> <p>That\u2019s the exact motivation behind Long Short-Term Memory (LSTM) networks.</p> <p>In the next subsection, we\u2019ll open up the LSTM cell and look inside. You\u2019ll see how it uses gates to control memory in a way that vanilla RNNs never could.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html","title":"4.4.2 Inside the LSTM Cell","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#442-inside-the-lstm-cell","title":"4.4.2 Inside the LSTM Cell","text":"<p>If vanilla RNNs are forgetful readers, then LSTMs are thoughtful ones\u2014with a notepad in hand. They don\u2019t just react to each new word blindly. Instead, they ask themselves: Should I remember this? Should I ignore it? Should I update what I know?</p> <p>To do this, an LSTM uses a more sophisticated internal structure\u2014a memory cell guided by gates. These gates are what give the LSTM its power to remember relevant information and forget the rest.</p> <p>Let\u2019s step inside.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#two-kinds-of-memory","title":"\ud83e\udde0 Two Kinds of Memory","text":"<p>An LSTM maintains two pieces of memory at each time step:</p> <ol> <li>Cell State $c_t$: The long-term memory. This is the key to remembering information over many time steps.</li> <li>Hidden State $h_t$: The short-term memory, used for output and interactions with the rest of the network.</li> </ol> <p>The cell state acts like a conveyor belt\u2014it flows through the sequence largely unchanged, with small edits made along the way via the gates.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#the-three-gates","title":"\ud83d\udd10 The Three Gates","text":"<p>Each LSTM cell contains three gates. Think of each gate as a little controller that decides how much information passes through.</p> <p>Let\u2019s say we\u2019re processing word $x_t$ at time step $t$, and we already have the previous hidden state $h_{t-1}$ and previous cell state $c_{t-1}$.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#1-forget-gate-f_t","title":"1. Forget Gate $f_t$","text":"<p>This gate decides what parts of the old memory to forget.</p> <p>$$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$</p> <ul> <li>$\\sigma$ is the sigmoid function (outputs values between 0 and 1)</li> <li>If $f_t[i] = 0$, that part of the memory is erased</li> <li>If $f_t[i] = 1$, it\u2019s kept completely</li> </ul> <p>Analogy: You\u2019re reviewing old notes. You erase parts that aren\u2019t relevant anymore.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#2-input-gate-i_t-and-candidate-cell-state-tildec_t","title":"2. Input Gate $i_t$ and Candidate Cell State $\\tilde{c}_t$","text":"<p>These determine what new information to store.</p> <p>$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$</p> <p>$$ \\tilde{c}t = \\tanh(W_c x_t + U_c h{t-1} + b_c) $$</p> <ul> <li>$i_t$ controls how much of $\\tilde{c}_t$ (new info) gets added</li> <li>$\\tilde{c}_t$ is the candidate update, passed through <code>tanh</code></li> </ul> <p>Analogy: You hear something new. The input gate decides whether it\u2019s worth noting, and how strongly to add it to memory.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#3-update-the-cell-state-c_t","title":"3. Update the Cell State $c_t$","text":"<p>This is where memory gets updated:</p> <p>$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t $$</p> <ul> <li>$\\odot$ means element-wise multiplication</li> <li>We combine the old memory (after forgetting) with the new candidate memory (after filtering)</li> </ul> <p>This is the heart of the LSTM: a balance between remembering and updating.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#4-output-gate-o_t","title":"4. Output Gate $o_t$","text":"<p>Finally, the output gate decides what part of the memory to expose as the hidden state:</p> <p>$$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$</p> <p>$$ h_t = o_t \\odot \\tanh(c_t) $$</p> <p>This hidden state $h_t$ is what gets passed to the next LSTM cell and can be used for predictions.</p> <p>Analogy: You\u2019ve taken notes and processed them\u2014now you share a summary of what\u2019s relevant.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#recap-the-lstm-flow","title":"\ud83e\udde9 Recap: The LSTM Flow","text":"<ol> <li>Forget part of the old memory</li> <li>Input new information and update the memory</li> <li>Output the relevant parts for the next step</li> </ol>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html#why-this-works","title":"\ud83d\udd04 Why This Works","text":"<p>By using gates, LSTMs give the network control over what to remember and what to forget\u2014solving the core problem that plagues vanilla RNNs.</p> <ul> <li>Gradients can now flow through the cell state without vanishing, because memory updates are additive, not multiplicative.</li> <li>The network learns when to remember important signals and ignore noise.</li> </ul> <p>In short: LSTMs have learnable memory control.</p> <p>Next, we\u2019ll see how this cell operates step by step over a sequence\u2014and how it actually performs in practice.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html","title":"4.4.3 How LSTMs Process Sequences","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#443-how-lstms-process-sequences","title":"4.4.3 How LSTMs Process Sequences","text":"<p>Now that we\u2019ve seen the internal machinery of an LSTM cell\u2014gates, memory, and all\u2014it\u2019s time to see it in action. What happens when you give an LSTM a sentence, one word at a time?</p> <p>The beauty of LSTMs is not just in how they process a single word\u2014but in how they carry memory across time steps, carefully updating and retaining knowledge as needed.</p> <p>Let\u2019s walk through a simple example and trace the flow of information.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#example-the-dog-barked-loudly","title":"\ud83e\uddea Example: \u201cThe dog barked loudly\u201d","text":"<p>Suppose we want our model to process the sentence:</p> <p>\u201cThe dog barked loudly\u201d</p> <p>We feed the words into the LSTM one at a time. At each step, the LSTM takes in:</p> <ul> <li>The current input word (converted to a vector $x_t$)</li> <li>The previous hidden state $h_{t-1}$</li> <li>The previous cell state $c_{t-1}$</li> </ul> <p>It returns:</p> <ul> <li>A new hidden state $h_t$</li> <li>A new cell state $c_t$</li> </ul> <p>Let\u2019s visualize the timeline:</p> <pre><code>Time Step:     t=1        t=2        t=3         t=4\nInput:        \"The\"      \"dog\"     \"barked\"    \"loudly\"\n             ------     ------     -------     -------\nCell:        c\u2081         c\u2082         c\u2083          c\u2084\nHidden:      h\u2081         h\u2082         h\u2083          h\u2084\n</code></pre> <p>Each LSTM cell receives updated memory from the previous cell and decides:</p> <ul> <li>Should I keep the old memory?</li> <li>Should I overwrite it with new info?</li> <li>What part of it should I share outward?</li> </ul> <p>This continuous flow allows it to remember context that spans across several steps.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#step-by-step-flow-conceptually","title":"\ud83d\udccb Step-by-Step Flow (Conceptually)","text":"<ol> <li> <p>Input: \"The\" (t = 1)</p> </li> <li> <p>There's no prior context, so the model starts fresh.</p> </li> <li> <p>The input and forget gates decide what basic sentence structure info to store.</p> </li> <li> <p>Input: \"dog\" (t = 2)</p> </li> <li> <p>The model learns this is likely a subject noun.</p> </li> <li> <p>This word updates the memory to reflect who the sentence is about.</p> </li> <li> <p>Input: \"barked\" (t = 3)</p> </li> <li> <p>A verb appears.</p> </li> <li> <p>The LSTM now understands the subject is acting and updates memory accordingly.</p> </li> <li> <p>Input: \"loudly\" (t = 4)</p> </li> <li> <p>An adverb adds detail to the action.</p> </li> <li>The LSTM might store this as modifying the verb \u201cbarked\u201d and start preparing for the sentence end.</li> </ol>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#why-this-matters","title":"\ud83e\udde0 Why This Matters","text":"<p>At each point, the model\u2019s hidden state reflects not just the current word, but also everything it has learned so far. That\u2019s crucial for tasks like:</p> <ul> <li>Next word prediction: The LSTM uses its memory to guess what word might come next.</li> <li>Translation: The full sequence is processed before generating the translated version.</li> <li>Sentiment analysis: Early words like \u201cnot\u201d can flip the meaning of words like \u201cgood\u201d much later in the sentence.</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#output-at-each-step-optional","title":"\ud83e\uddee Output at Each Step (Optional)","text":"<p>Some tasks require output after each word (e.g., POS tagging), while others only need output at the end (e.g., sentiment classification).</p> <p>You can choose how to use the hidden states:</p> <ul> <li>Use all $h_1, h_2, h_3, h_4$</li> <li>Or just the final hidden state $h_4$</li> </ul> <p>This flexibility is one of the reasons LSTMs became so widely adopted.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html#summary","title":"\ud83e\udde9 Summary","text":"<ul> <li>LSTMs process sequences step by step, maintaining both short- and long-term memory.</li> <li>Gates control how memory is updated and used.</li> <li>The final hidden state contains a rich, context-aware summary of the entire sentence so far.</li> </ul> <p>Next, we\u2019ll explore when and why LSTMs are used in real-world NLP tasks\u2014and compare them to other models like GRUs.</p> <p>Shall we move on to Section 4.4.4: When and Why to Use LSTMs?</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html","title":"4.4.4 When and Why to Use LSTMs","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#444-when-and-why-to-use-lstms","title":"4.4.4 When and Why to Use LSTMs","text":"<p>By now, you\u2019ve seen how LSTMs fix a fundamental weakness in standard RNNs: memory loss. With their ability to remember what matters and forget what doesn\u2019t, LSTMs offer a robust tool for working with sequential data like language, time series, or audio.</p> <p>But when should you actually use them? What kinds of problems are LSTMs good at solving? And what trade-offs should you be aware of?</p> <p>Let\u2019s explore.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#when-lstms-are-a-great-choice","title":"\u2705 When LSTMs Are a Great Choice","text":"<p>LSTMs are especially useful when your task involves long-term dependencies\u2014where understanding something early in the sequence is crucial to interpreting what happens later.</p> <p>Here are some classic scenarios where LSTMs work well:</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#language-modeling-text-generation","title":"\ud83d\udd24 Language Modeling &amp; Text Generation","text":"<ul> <li>Predicting the next word in a sentence (like autocomplete)</li> <li>Generating new text in the style of Shakespeare, code, or recipes</li> <li>LSTMs can learn structure and rhythm in natural language over time</li> </ul> <p>Example: Input: \u201cOnce upon a time, there was a\u2026\u201d Output: \u201cprincess who lived in a faraway castle.\u201d</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#machine-translation","title":"\ud83c\udf10 Machine Translation","text":"<ul> <li>Translating a sentence from English to French, for example</li> <li>The LSTM reads the input sentence into a memory, then generates the output word-by-word</li> <li>Works well with encoder-decoder architectures (we\u2019ll see more of this later)</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#sentiment-analysis","title":"\ud83d\ude03 Sentiment Analysis","text":"<ul> <li>Classifying whether a review is positive or negative</li> <li>LSTMs can capture important cues like:</li> </ul> <p>\u201cI didn\u2019t like the movie.\u201d vs. \u201cI didn\u2019t like the movie, but the soundtrack was amazing.\u201d</p> <p>Notice how the second part can soften or reverse sentiment. LSTMs help catch this.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#speech-recognition-audio-processing","title":"\ud83d\udd0a Speech Recognition &amp; Audio Processing","text":"<ul> <li>Audio is a natural sequence, just like text</li> <li>LSTMs can map audio features to transcriptions over time</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#time-series-forecasting","title":"\ud83d\udcc9 Time-Series Forecasting","text":"<ul> <li>Stock prices, weather data, sensor streams</li> <li>Predicting the next value given historical trends</li> <li>LSTMs can learn cyclical patterns and anomalies over time</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#where-lstms-fall-short","title":"\u274c Where LSTMs Fall Short","text":"<p>Despite their strengths, LSTMs aren\u2019t perfect. Here are some limitations you should keep in mind:</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#slow-training-and-inference","title":"\ud83d\udc22 Slow Training and Inference","text":"<p>LSTMs process sequences step by step. That means they can\u2019t be easily parallelized like Transformers. On long texts, this can become slow.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#lots-of-parameters","title":"\ud83e\uddee Lots of Parameters","text":"<p>Each LSTM cell has multiple weight matrices (for each gate), so models can become heavy and take longer to converge.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#still-not-truly-contextual","title":"\ud83e\udde0 Still Not Truly Contextual","text":"<p>While LSTMs carry memory, the same word in different contexts often leads to similar representations. They're better than Word2Vec, but still limited in how deeply they understand nuanced meaning.</p> <p>Example: \u201cbank\u201d in river bank vs. savings bank \u2014 LSTM might still struggle here unless trained extensively.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#lstm-vs-other-models","title":"\ud83c\udd9a LSTM vs. Other Models","text":"Task Type LSTM RNN GRU Transformer Long-term memory \u2705 Good \u274c Poor \u2705 Good \u2705\u2705 Excellent Speed \u274c Slow \u2705 Faster \u2705 Faster \u2705\u2705 Fast (parallel) Simplicity \u274c Complex \u2705 Simple \u2705 Moderate \u274c Complex Contextual Embedding \u274c Limited \u274c None \u274c Limited \u2705\u2705 Yes"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html#summary","title":"\ud83e\udde9 Summary","text":"<p>LSTMs are a powerful tool when:</p> <ul> <li>Sequence matters</li> <li>Context is spread out over time</li> <li>You need memory and reasoning</li> </ul> <p>But as tasks become more complex, and the need for deep context grows, even LSTMs start to show their limits.</p> <p>That\u2019s why the field evolved further: toward Gated Recurrent Units (GRUs) for simplicity, and eventually to Transformers, which revolutionized NLP with parallelism and attention mechanisms.</p> <p>We\u2019ll look at GRUs next\u2014think of them as LSTMs with fewer moving parts.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html","title":"4.5.1 Motivation for GRUs","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html#451-motivation-for-grus","title":"4.5.1 Motivation for GRUs","text":"<p>Why LSTMs needed a simpler sibling</p> <p>By now, you\u2019ve seen how powerful Long Short-Term Memory (LSTM) networks can be. They solve the vanishing gradient problem. They learn to remember important information and forget the rest. They even manage to model long-term dependencies that vanilla RNNs can't.</p> <p>But power comes at a price.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html#the-complexity-of-lstms","title":"\ud83d\udc18 The Complexity of LSTMs","text":"<p>Take a moment to reflect on what happens inside an LSTM cell:</p> <ul> <li>Three separate gates (forget, input, output)</li> <li>A separate cell state and hidden state</li> <li>Multiple weight matrices for each gate</li> <li>Non-trivial flow of information at each step</li> </ul> <p>That's a lot of machinery. While it gives us control and flexibility, it also means:</p> <ul> <li>More parameters to train</li> <li>More computation per time step</li> <li>Slower training, especially on large datasets</li> <li>Higher risk of overfitting with small datasets</li> </ul> <p>For some tasks, that complexity is justified. But for others, it\u2019s like using a rocket ship to deliver pizza across the street.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html#can-we-simplify","title":"\ud83e\udde0 Can We Simplify?","text":"<p>This led researchers to ask a natural question:</p> <p>\"Can we design a simpler RNN architecture that retains the benefits of LSTM\u2014like remembering and forgetting\u2014but with fewer gates and less computation?\"</p> <p>The answer came in 2014, when Cho et al. introduced the Gated Recurrent Unit (GRU).</p> <p>GRUs took the core idea of gating\u2014controlling memory\u2014and reimagined it in a cleaner, more efficient design. Instead of three gates and two memory tracks, GRUs streamlined everything into:</p> <ul> <li>Just two gates</li> <li>A single hidden state (no separate cell state)</li> <li>Fewer parameters and faster training</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html#gru-the-bicycle-to-lstms-motorcycle","title":"\ud83d\udeb2 GRU: The Bicycle to LSTM\u2019s Motorcycle","text":"<p>Think of it this way:</p> <ul> <li>An RNN is like a person trying to remember everything by scribbling on the same notepad over and over.</li> <li>An LSTM is like a professional note-taker\u2014with highlighters, folders, and an intricate filing system.</li> <li>A GRU is like a minimalist\u2014smart, efficient, and carrying just one notebook and two pens.</li> </ul> <p>In many tasks, the GRU can go just as far as the LSTM\u2014without the extra baggage.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html#what-grus-keep-and-drop","title":"\ud83d\udd04 What GRUs Keep (and Drop)","text":"<p>GRUs keep:</p> <ul> <li>The idea of remembering relevant info across time steps</li> <li>Mechanisms to control how much past information to keep or forget</li> </ul> <p>GRUs drop:</p> <ul> <li>The separate memory cell $c_t$</li> <li>The output gate</li> </ul> <p>Instead, the GRU reuses and merges some of these concepts into a more compact and faster architecture.</p> <p>In the next section, we\u2019ll open up the GRU cell and examine its internal logic. You'll see how its two gates work together to manage memory\u2014beautifully, and simply.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html","title":"4.5.2 Inside the GRU Cell","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#452-inside-the-gru-cell","title":"4.5.2 Inside the GRU Cell","text":"<p>A simpler, elegant memory design</p> <p>If the LSTM is a memory system with three levers and two compartments, the Gated Recurrent Unit (GRU) simplifies everything down to just two levers and a single memory track. And yet, it still manages to remember and forget, learn and adapt\u2014often just as well.</p> <p>Let\u2019s look inside a GRU cell to understand how this streamlined mechanism works.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#one-memory-track-hidden-state-only","title":"\ud83e\udde0 One Memory Track: Hidden State Only","text":"<p>Unlike the LSTM, the GRU has no separate cell state. Instead, it keeps everything it needs in a single hidden state $h_t$. This hidden state is updated at each time step using two gates:</p> <ul> <li>The Update Gate $z_t$</li> <li>The Reset Gate $r_t$</li> </ul> <p>Let\u2019s break these down.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#update-gate-z_t-control-what-to-keep","title":"\ud83d\udd04 Update Gate $z_t$: Control What to Keep","text":"<p>The update gate determines how much of the past should be kept around and how much should be replaced by the new information.</p> <p>The formula is:</p> <p>$$ z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z) $$</p> <p>Where:</p> <ul> <li>$x_t$: input at time step $t$</li> <li>$h_{t-1}$: previous hidden state</li> <li>$\\sigma$: sigmoid activation (values between 0 and 1)</li> </ul> <p>Intuition: If $z_t \\approx 1$, we mostly keep the past memory. If $z_t \\approx 0$, we replace it with new information.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#reset-gate-r_t-control-how-to-mix-the-past","title":"\u267b\ufe0f Reset Gate $r_t$: Control How to Mix the Past","text":"<p>The reset gate decides how much of the past hidden state should be considered when generating new information.</p> <p>$$ r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r) $$</p> <p>This gate lets the GRU \"forget\" parts of the previous state when appropriate\u2014especially helpful in dealing with noise or irrelevant past input.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#candidate-hidden-state-tildeh_t-the-new-memory","title":"\u2699\ufe0f Candidate Hidden State $\\tilde{h}_t$: The New Memory","text":"<p>Once the gates are computed, the GRU calculates a candidate hidden state $\\tilde{h}_t$ based on the current input and a modified past:</p> <p>$$ \\tilde{h}t = \\tanh(W_h x_t + U_h (r_t \\odot h{t-1}) + b_h) $$</p> <ul> <li>$r_t \\odot h_{t-1}$: this is a filtered version of the past, shaped by the reset gate</li> <li>$\\tanh$: ensures the output values stay in a manageable range</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#final-step-update-the-hidden-state-h_t","title":"\ud83e\uddfe Final Step: Update the Hidden State $h_t$","text":"<p>The final hidden state is computed by interpolating between the old and new memory using the update gate:</p> <p>$$ h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$</p> <p>Interpretation:</p> <ul> <li>If the update gate $z_t$ is high \u2192 we mostly use the new information.</li> <li>If $z_t$ is low \u2192 we mostly retain the previous memory.</li> </ul> <p>This formula is elegant: it\u2019s just a weighted average of the past and present.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#analogy-thoughtful-updating","title":"\ud83d\udcda Analogy: Thoughtful Updating","text":"<p>Imagine you're editing a live document:</p> <ul> <li>The reset gate decides how much of the earlier draft to look at.</li> <li>The update gate decides whether to keep what you had or replace it with what you just wrote.</li> </ul> <p>In some sentences, you only tweak a few words (low update). In others, you rewrite everything (high update). The GRU behaves similarly.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html#summary-of-gru-flow","title":"\ud83d\udd01 Summary of GRU Flow","text":"<p>Here\u2019s a quick recap of the GRU\u2019s internal steps:</p> <ol> <li>Compute update and reset gates from input and past state.</li> <li>Use reset gate to filter old memory.</li> <li>Generate candidate memory from input and modified past.</li> <li>Use update gate to blend old and new memory into the new state.</li> </ol> <p>All of this happens within a single hidden state, which is passed along through time.</p> <p>In the next section, we\u2019ll compare this simplified design to the LSTM and explore when to use each one.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html","title":"4.5.3 Comparing GRU and LSTM","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#453-comparing-gru-and-lstm","title":"4.5.3 Comparing GRU and LSTM","text":"<p>Two memory networks, one important decision</p> <p>Now that we\u2019ve explored both the LSTM and the GRU, it\u2019s natural to ask:</p> <p>\u201cWhich one should I use?\u201d</p> <p>Both are designed to solve the same problem\u2014vanishing gradients and short-term memory in RNNs\u2014but they do it in slightly different ways. Let\u2019s compare them side by side to help you choose the right tool for your NLP (or sequence modeling) task.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#memory-design-two-states-vs-one","title":"\ud83e\udde0 Memory Design: Two States vs. One","text":"Feature LSTM GRU Memory Structure Has cell state and hidden state Only hidden state Complexity More gates, more parameters Simpler design Gates Forget, Input, Output (3) Update, Reset (2) Flexibility More control over memory flow Faster, fewer moving parts <ul> <li>LSTM offers fine-grained control over memory with separate states and multiple gates.</li> <li>GRU simplifies the architecture by combining gates and using just one state.</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#training-speed-and-efficiency","title":"\u26a1 Training Speed and Efficiency","text":"<p>Because GRUs have fewer parameters, they often:</p> <ul> <li>Train faster</li> <li>Require less memory</li> <li>Generalize better on smaller datasets</li> </ul> <p>This can be a big advantage when:</p> <ul> <li>You\u2019re training on limited hardware</li> <li>Your dataset isn\u2019t massive</li> <li>You care more about speed than precision</li> </ul> <p>However, LSTMs may outperform GRUs when trained on very large datasets or tasks requiring intricate temporal reasoning.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#empirical-performance","title":"\ud83e\uddea Empirical Performance","text":"<p>In practice, both models often perform similarly. Here\u2019s a rough idea based on research and real-world usage:</p> Task GRU vs. LSTM Text classification About the same Sentiment analysis (short) GRU can be better Translation / Language Mod. LSTM often better Time series prediction GRU slightly faster Long sequences with nuance LSTM may edge ahead <p>That said, performance always depends on:</p> <ul> <li>The dataset</li> <li>The task</li> <li>The training setup</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#when-should-you-choose-gru","title":"\ud83d\udee0\ufe0f When Should You Choose GRU?","text":"<p>Use GRU when you:</p> <ul> <li>Want a lighter, faster model</li> <li>Have limited data or compute</li> <li>Are building a prototype or trying many architectures</li> <li>Need a baseline model that performs well with less tuning</li> </ul> <p>Use LSTM when you:</p> <ul> <li>Have access to large data and computational power</li> <li>Need to capture subtle, long-range dependencies</li> <li>Are working on complex NLP pipelines like translation or summarization</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html#summary","title":"\ud83e\udde9 Summary","text":"<p>Both LSTM and GRU give you what vanilla RNNs cannot: learnable memory. They let your model decide what to remember and what to forget.</p> <ul> <li>LSTM is more powerful and flexible, but heavier.</li> <li>GRU is simpler and faster, often good enough\u2014and sometimes even better.</li> </ul> <p>In fact, many modern NLP systems try both and go with what performs better. If you\u2019re just getting started, the GRU is an excellent place to begin.</p> <p>Next, we\u2019ll look at when and where these gated recurrent models are used in real-world applications\u2014and what lies ahead with attention and transformers.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html","title":"4.5.4 When to Use GRUs","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#454-when-to-use-grus","title":"4.5.4 When to Use GRUs","text":"<p>A simpler solution for many sequence problems</p> <p>Now that we\u2019ve seen what makes GRUs special and how they compare to LSTMs, the big question is:</p> <p>\u201cWhen should I actually use a GRU?\u201d</p> <p>The short answer? More often than you might think. Let\u2019s explore where GRUs shine\u2014and where they may fall short.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#grus-are-a-great-choice-when","title":"\u2705 GRUs Are a Great Choice When\u2026","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#you-need-faster-training","title":"\ud83d\udd52 You Need Faster Training","text":"<p>GRUs have fewer gates, fewer parameters, and a smaller memory footprint than LSTMs. This makes them:</p> <ul> <li>Quicker to train</li> <li>Less resource-intensive</li> <li>Easier to deploy on smaller devices (e.g., phones, edge hardware)</li> </ul> <p>If you\u2019re working on a tight compute budget, GRUs can give you 80\u201390% of LSTM performance at 50\u201370% of the cost.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#you-have-a-small-or-medium-dataset","title":"\ud83d\udcca You Have a Small or Medium Dataset","text":"<p>GRUs generalize well even with less data, partly because they have fewer parameters and are less prone to overfitting. They\u2019re perfect for:</p> <ul> <li>Customer review classification</li> <li>Chatbots trained on modest data</li> <li>Early-stage prototypes of NLP systems</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#youre-building-a-lightweight-model","title":"\ud83c\udfd7\ufe0f You\u2019re Building a Lightweight Model","text":"<p>Need something quick and practical for:</p> <ul> <li>Sentiment classification?</li> <li>Intent recognition?</li> <li>Named Entity Recognition (NER)?</li> </ul> <p>A GRU often gets the job done without much tuning.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#example-use-cases-for-grus","title":"\ud83d\udd0d Example Use Cases for GRUs","text":"Application Why GRU Works Well Sentiment Analysis Short/medium-length inputs, fast feedback Speech Recognition Efficient sequence modeling, fast training Time-Series Prediction Captures patterns without needing full LSTM power Chatbots / QA Systems Fast, memory-efficient reasoning IoT / Edge NLP Fits into memory-constrained environments"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#when-not-to-use-grus","title":"\u274c When Not to Use GRUs","text":"<p>Despite their strengths, GRUs may not be the best fit when:</p> <ul> <li>You have very long-range dependencies in the input</li> </ul> <p>Example: A legal document where context from page 1 matters on page 5 * You need fine-grained memory control (e.g., translation or summarization) * You\u2019re working with Transformer-based architectures (which don\u2019t use GRUs or LSTMs at all)</p> <p>In these cases, LSTMs or Transformers may give better results, even if they take longer to train.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#tip-use-gru-as-a-baseline","title":"\ud83e\udde0 Tip: Use GRU as a Baseline","text":"<p>Many researchers and practitioners use GRUs as their first model for sequence tasks. Why?</p> <ul> <li>It\u2019s simple to implement (especially with libraries like PyTorch or Keras)</li> <li>It converges faster</li> <li>It gives you a solid baseline to compare more complex models against</li> </ul> <p>Once you\u2019ve seen how a GRU performs, you can always move up to LSTMs\u2014or even Transformers\u2014if needed.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html#summary","title":"\ud83e\udde9 Summary","text":"<ul> <li>GRUs are a fast, efficient alternative to LSTMs</li> <li>They perform well on most sequence tasks, especially when data or compute is limited</li> <li>Use them when you want to build quickly and iterate fast</li> <li>Save LSTMs for complex problems where fine-grained memory tracking is necessary</li> </ul> <p>Next, we\u2019ll wrap up the chapter with a section on language generation\u2014where you\u2019ll see RNNs, LSTMs, and GRUs in action, generating sentences word by word.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html","title":"4.6.1: Language Generation with RNNs","text":"<p>Wonderful! Here's Section 4.6.1: What Is Language Generation? written in our friendly, descriptive book tone\u2014perfect for helping readers grasp the intuition behind sequence-based text generation.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#461-what-is-language-generation","title":"4.6.1 What Is Language Generation?","text":"<p>Making machines speak, one word at a time</p> <p>Imagine sitting down to write a story. You don\u2019t start with the entire story in mind\u2014you begin with a word or two, then let the next ones flow based on what you\u2019ve already written. This is, in essence, what language generation is all about.</p> <p>At its core, language generation is the process by which a machine produces human-like text by predicting what comes next\u2014one token (word or character) at a time.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#the-sequential-nature-of-language","title":"\ud83d\udcd6 The Sequential Nature of Language","text":"<p>Language is inherently sequential. Words don\u2019t just appear randomly\u2014they follow rules and patterns. For example:</p> <p>\u201cThe cat sat on the\u2026\u201d</p> <p>Chances are, your brain automatically thinks of a word like mat, couch, or floor. That\u2019s because your mental language model has learned patterns from experience.</p> <p>A machine language model tries to do something similar: It predicts the next word given everything that came before.</p> <p>Formally, a language model tries to estimate:</p> <p>$$ P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1}) $$</p> <p>In plain English:</p> <p>The probability of a sentence is the product of each word\u2019s probability, conditioned on all the words before it.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#how-sequence-models-learn-to-generate","title":"\ud83e\udde0 How Sequence Models Learn to Generate","text":"<p>To generate language, a model like an RNN, GRU, or LSTM is trained to learn these patterns:</p> <ul> <li>It\u2019s fed a sequence of words during training (e.g., \u201cThe cat sat\u201d)</li> <li>It tries to predict the next word (e.g., \u201con\u201d)</li> <li>If it gets it wrong, it adjusts its weights using backpropagation</li> <li>Over time, it learns to anticipate common patterns of grammar and meaning</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#generation-vs-classification","title":"\ud83d\udde3\ufe0f Generation vs. Classification","text":"<p>So far in this chapter, we\u2019ve seen how sequence models classify things\u2014sentiment, intent, etc. But generation is different:</p> Classification Tasks Generation Tasks Input \u2192 Label Input \u2192 Output Sequence Sentiment: \u201cpositive\u201d Next word: \u201con\u201d POS tag: \u201cverb\u201d Complete sentence Short output Open-ended output <p>Language generation is more creative\u2014and open-ended. The model is free to produce anything it believes is likely, based on what it has seen so far.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#real-world-examples-of-language-generation","title":"\u270d\ufe0f Real-World Examples of Language Generation","text":"<p>You interact with language generation every day, often without realizing it:</p> <ul> <li>Autocomplete: your phone predicts the next word</li> <li>Chatbots: reply with relevant responses</li> <li>Story or poem generators: write in Shakespearean or sarcastic tone</li> <li>Machine translation: generates an entire translated sentence</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html#summary","title":"\ud83e\udde9 Summary","text":"<ul> <li>Language generation is the task of predicting the next word in a sequence.</li> <li>It\u2019s a probabilistic process where the model learns from patterns in real text.</li> <li>RNNs, GRUs, and LSTMs are all capable of generating text, though with limitations.</li> <li>This task is foundational for modern applications like chatbots, translators, and AI writing tools.</li> </ul> <p>In the next section, we\u2019ll dive deeper into how sequence models are trained to generate language\u2014and how they use techniques like teacher forcing to improve learning.</p> <p>Shall we move on to Section 4.6.2: Training a Recurrent Language Model?</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html","title":"4.6.2 Training a Recurrent Language Model","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#462-training-a-recurrent-language-model","title":"4.6.2 Training a Recurrent Language Model","text":"<p>Teaching machines to predict the next word</p> <p>Now that we understand the goal of language generation\u2014predicting the next word in a sequence\u2014let\u2019s explore how we train a model to do exactly that.</p> <p>Training a recurrent language model is like teaching a child to write by filling in blanks: show them a sentence, hide the last word, and ask them to guess what comes next. If they're wrong, they adjust based on feedback. Over time, they get better at spotting patterns.</p> <p>Machines learn the same way\u2014just faster, and with lots of data.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#step-1-preparing-the-dataset","title":"\ud83e\uddfe Step 1: Preparing the Dataset","text":"<p>To train a language model, we first need a text corpus\u2014a large collection of sentences. This is broken into sequences like so:</p> <p>Input: \u201cThe cat sat on the\u201d Target: \u201ccat sat on the mat\u201d</p> <p>Each training example is made of:</p> <ul> <li>An input sequence $(w_1, w_2, ..., w_{t-1})$</li> <li>A target sequence $(w_2, w_3, ..., w_t)$</li> </ul> <p>This way, the model learns to predict the next word at every time step.</p> <p>We also tokenize the text into integers or word embeddings (like Word2Vec or GloVe) before feeding it into the model.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#step-2-feeding-sequences-into-the-model","title":"\ud83d\udd01 Step 2: Feeding Sequences into the Model","text":"<p>We pass the input sequence word by word through the recurrent model\u2014RNN, GRU, or LSTM. At each time step $t$, the model computes a hidden state $h_t$, which captures all the information seen so far.</p> <p>Then it uses this hidden state to predict a probability distribution over the next word using a softmax layer.</p> <p>$$ P(w_t | w_1, ..., w_{t-1}) = \\text{softmax}(W \\cdot h_t + b) $$</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#step-3-comparing-predictions-with-reality","title":"\ud83c\udfaf Step 3: Comparing Predictions with Reality","text":"<p>The model's prediction is compared to the actual next word using a loss function\u2014typically categorical cross-entropy, which penalizes incorrect guesses and rewards correct ones.</p> <p>$$ \\mathcal{L} = -\\sum_{t} \\log P_{\\text{model}}(w_t) $$</p> <p>If the predicted word was \"dog\" but the correct word was \"mat\", the loss will be high. The model then adjusts its weights using backpropagation through time (BPTT).</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#teacher-forcing-helping-the-model-learn-faster","title":"\ud83d\udcda Teacher Forcing: Helping the Model Learn Faster","text":"<p>One trick used during training is called teacher forcing.</p> <p>Instead of feeding the model\u2019s own prediction as the next input (which may be wrong), we feed the actual word from the training data.</p> <p>Example: True input: \u201cThe cat sat\u201d True target: \u201ccat sat on\u201d</p> <p>Even if the model guesses \u201cThe cat ran\u201d, we still feed \u201csat\u201d as the next word, not \u201cran\u201d.</p> <p>This makes learning faster and more stable.</p> <p>Why it works:</p> <ul> <li>Prevents early errors from derailing the sequence</li> <li>Gives a clearer training signal for what should have come next</li> </ul> <p>Drawback: At inference time, teacher forcing isn\u2019t available\u2014you have to generate word-by-word using the model\u2019s own predictions. This mismatch can sometimes hurt performance.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html#summary","title":"\ud83e\udde9 Summary","text":"<ul> <li>Training a language model involves feeding sequences and predicting the next word at each step.</li> <li>Cross-entropy loss is used to measure how well the model matches the actual text.</li> <li>Teacher forcing accelerates training by using the true previous word instead of the model\u2019s guess.</li> <li>Once trained, the model can be used to generate text, one word at a time.</li> </ul> <p>In the next section, we\u2019ll explore how inference works: how to generate new sentences without ground-truth labels, using techniques like sampling and temperature control.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html","title":"4.6.3 Sampling and Generation During Inference","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#463-sampling-and-generation-during-inference","title":"4.6.3 Sampling and Generation During Inference","text":"<p>Letting the model speak for itself</p> <p>Training a language model is like teaching a student to complete sentences. But what happens when we let that student speak freely\u2014without telling them the next word?</p> <p>This is where inference comes in. During training, the model had access to the correct next word (via teacher forcing). But at inference time, we remove the training wheels. The model must predict the next word on its own, then use that prediction to guess the one after that\u2026 and so on.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#how-generation-works-at-inference","title":"\ud83d\udd01 How Generation Works at Inference","text":"<p>Let\u2019s say we give the model a prompt like:</p> <p>\u201cOnce upon a time\u201d</p> <p>Here\u2019s what happens next:</p> <ol> <li>Feed this sequence into the model.</li> <li>The model outputs a probability distribution over the vocabulary.</li> <li>We choose the next word based on that distribution.</li> <li>Append the chosen word to the input.</li> <li>Repeat the process until we reach a stop condition (e.g., max length, special token, or end punctuation).</li> </ol> <p>This process is called autoregressive generation\u2014the model generates one word at a time, using its own outputs to continue the sequence.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#sampling-strategies-how-to-pick-the-next-word","title":"\ud83c\udfb2 Sampling Strategies: How to Pick the Next Word","text":"<p>The model gives us a list of possible next words, each with a probability. But how do we decide which word to choose?</p> <p>There are several common strategies:</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#greedy-sampling","title":"\ud83d\udd39 Greedy Sampling","text":"<p>Always pick the word with the highest probability.</p> <ul> <li>\u2705 Simple and fast</li> <li>\u274c Can lead to dull or repetitive text</li> </ul> <p>\"I am a robot. I am a robot. I am a robot.\"</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#random-sampling","title":"\ud83d\udd39 Random Sampling","text":"<p>Pick the next word randomly based on the predicted probability distribution.</p> <ul> <li>\u2705 More creative and diverse</li> <li>\u274c Can produce incoherent or nonsensical results</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#top-k-sampling","title":"\ud83d\udd39 Top-k Sampling","text":"<p>Only consider the top k most likely words, then pick one randomly.</p> <ul> <li>\u2705 Controls randomness while keeping diversity</li> <li>Example: if <code>k=5</code>, sample from top 5 predicted words</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#top-p-sampling-nucleus-sampling","title":"\ud83d\udd39 Top-p Sampling (Nucleus Sampling)","text":"<p>Choose the smallest set of words whose combined probability exceeds p (like 0.9), then sample from that.</p> <ul> <li>\u2705 More flexible than top-k</li> <li>Adaptively adjusts based on the distribution\u2019s confidence</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#temperature","title":"\ud83d\udd39 Temperature","text":"<p>This controls the creativity of the sampling:</p> <ul> <li>High temperature (e.g. 1.5) \u2192 more randomness</li> <li>Low temperature (e.g. 0.3) \u2192 more confident, conservative choices</li> <li>Temperature of 1.0 is neutral</li> </ul> <p>Temperature modifies the probability distribution before sampling:</p> <p>$$ P_i^{\\text{adjusted}} = \\frac{e^{\\log P_i / T}}{\\sum_j e^{\\log P_j / T}} $$</p> <p>Where $T$ is the temperature.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#example-walkthrough","title":"\ud83e\uddfe Example Walkthrough","text":"<p>Let\u2019s say the model is given:</p> <p>\u201cThe weather today is\u201d</p> <p>And it predicts the following probabilities:</p> <ul> <li>sunny: 0.40</li> <li>cloudy: 0.35</li> <li>amazing: 0.10</li> <li>table: 0.01</li> </ul> <p>Greedy: \u2192 \u201csunny\u201d Top-k (k=2): \u2192 sample between \u201csunny\u201d and \u201ccloudy\u201d Top-p (p=0.8): \u2192 \u201csunny\u201d, \u201ccloudy\u201d, \u201camazing\u201d included High temperature: \u2192 might choose \u201camazing\u201d Low temperature: \u2192 probably chooses \u201csunny\u201d</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#ending-the-generation","title":"\ud83d\udccc Ending the Generation","text":"<p>You usually stop generating when:</p> <ul> <li>A special end token (like <code>&lt;eos&gt;</code>) is predicted</li> <li>A maximum length is reached (e.g., 50 tokens)</li> <li>The model generates a period or full stop</li> </ul> <p>This ensures the output doesn\u2019t go on forever.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html#summary","title":"\ud83e\udde9 Summary","text":"<ul> <li>Inference is where the model finally generates text on its own.</li> <li>Each new word is sampled based on a probability distribution.</li> <li>Sampling strategies (greedy, top-k, temperature) control creativity vs. coherence.</li> <li>Proper tuning of sampling parameters can make the difference between boring and brilliant outputs.</li> </ul> <p>In the next section, we\u2019ll put this into practice with a working code example that trains a small GRU model and generates sentences.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html","title":"4.6.4 Code Example \u2013 Generating Text with GRU","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#464-code-example-generating-text-with-gru","title":"4.6.4 Code Example \u2013 Generating Text with GRU","text":"<p>A small neural storyteller in action</p> <p>Let\u2019s put theory into practice! In this section, we\u2019ll build a simple character-level text generator using a GRU in PyTorch. The model will be trained on a small string corpus and will learn to generate new sequences one character at a time.</p> <p>This example won\u2019t create Shakespeare, but it will give you hands-on experience with:</p> <ul> <li>Preparing sequential data</li> <li>Building a GRU-based model</li> <li>Training it to predict the next character</li> <li>Generating new text at inference time</li> </ul>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#step-1-import-libraries-and-prepare-the-data","title":"\ud83d\udd27 Step 1: Import Libraries and Prepare the Data","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\n</code></pre> <pre><code># Sample training data (you can use any text)\ntext = \"hello world hello world\"\n\n# Create character vocabulary\nchars = sorted(set(text))\nchar2idx = {ch: idx for idx, ch in enumerate(chars)}\nidx2char = {idx: ch for ch, idx in char2idx.items()}\n\nvocab_size = len(chars)\n\n# Convert full text into a list of indices\ndata = [char2idx[c] for c in text]\n\n# Hyperparameters\nseq_length = 5\nhidden_size = 32\nembedding_dim = 16\nlr = 0.01\nepochs = 200\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#step-2-define-the-gru-language-model","title":"\ud83d\udce6 Step 2: Define the GRU Language Model","text":"<pre><code>class CharGRU(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        x = self.embedding(x)\n        out, hidden = self.gru(x, hidden)\n        logits = self.fc(out)\n        return logits, hidden\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#step-3-prepare-batches","title":"\ud83d\udd01 Step 3: Prepare Batches","text":"<pre><code>def get_batch(data, seq_length):\n    # Randomly pick a start index\n    start = random.randint(0, len(data) - seq_length - 1)\n    seq = data[start : start + seq_length]\n    target = data[start + 1 : start + seq_length + 1]\n    return torch.tensor(seq).unsqueeze(0), torch.tensor(target).unsqueeze(0)\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#step-4-train-the-model","title":"\ud83e\udde0 Step 4: Train the Model","text":"<pre><code>model = CharGRU(vocab_size, embedding_dim, hidden_size)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    x_batch, y_batch = get_batch(data, seq_length)\n    logits, _ = model(x_batch)\n    loss = loss_fn(logits.view(-1, vocab_size), y_batch.view(-1))\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#step-5-generate-text-from-the-trained-model","title":"\u270d\ufe0f Step 5: Generate Text from the Trained Model","text":"<pre><code>def generate_text(model, start_text, gen_length=50, temperature=1.0):\n    model.eval()\n    input_seq = torch.tensor([char2idx[ch] for ch in start_text]).unsqueeze(0)\n    hidden = None\n    output_text = start_text\n\n    for _ in range(gen_length):\n        logits, hidden = model(input_seq, hidden)\n        logits = logits[:, -1, :] / temperature  # Get last time step\n        probs = torch.softmax(logits, dim=-1)\n        next_idx = torch.multinomial(probs, num_samples=1).item()\n        next_char = idx2char[next_idx]\n        output_text += next_char\n        input_seq = torch.tensor([[next_idx]])\n\n    return output_text\n</code></pre> <pre><code># Try generating text\nprint(generate_text(model, start_text=\"hell\", gen_length=40))\n</code></pre>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html#summary","title":"\ud83e\udde9 Summary","text":"<p>In this code example, we:</p> <ul> <li>Created a GRU-based character model using PyTorch</li> <li>Trained it to predict the next character in a simple sequence</li> <li>Used it to generate new text, character by character</li> <li>Saw how temperature controls creativity during sampling</li> </ul> <p>This example is small by design\u2014but the same structure applies when training on larger datasets or switching from characters to words.</p> <p>Next, we\u2019ll look at the limitations of RNN-based generation models\u2014and how attention and transformers came to improve upon them.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html","title":"4.6.5 Challenges and Limitations of RNN based Generation","text":""},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#465-challenges-and-limitations-of-rnn-based-generation","title":"4.6.5 Challenges and Limitations of RNN-based Generation","text":"<p>Why vanilla sequence models struggle to scale</p> <p>By now, we\u2019ve seen how Recurrent Neural Networks\u2014especially with GRUs and LSTMs\u2014can learn to generate text, word by word or character by character. They can write short poems, answer questions, and even mimic basic sentence structure.</p> <p>But as promising as RNN-based generation may seem, it has some major limitations\u2014especially when the sequences get long, the context gets rich, or the logic becomes complex.</p> <p>Let\u2019s explore why RNNs often fall short.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#1-struggles-with-long-term-dependencies","title":"\u23f3 1. Struggles with Long-Term Dependencies","text":"<p>RNNs process sequences one token at a time, passing memory from one step to the next. While GRUs and LSTMs improved the ability to remember past words, there\u2019s still a limit.</p> <p>For example:</p> <p>\"The dog that chased the cat that ran across the garden barked.\"</p> <p>To correctly interpret or generate such a sentence, the model must remember what subject it\u2019s dealing with several steps later. RNNs can lose this thread over long distances, especially if the sentence is complex or contains multiple nested clauses.</p> <p>Even LSTMs, which are designed to combat this issue, start to lose fidelity as sequences grow longer.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#2-sequential-bottleneck","title":"\ud83d\udd01 2. Sequential Bottleneck","text":"<p>RNNs operate strictly in sequence\u2014one time step must be processed before the next. This means:</p> <ul> <li>They cannot be parallelized easily on GPUs</li> <li>They take longer to train and to generate output</li> <li>The model becomes less efficient for very long sequences</li> </ul> <p>In contrast, newer models like Transformers can look at all positions simultaneously, making them far more efficient.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#3-exposure-bias","title":"\ud83e\uddea 3. Exposure Bias","text":"<p>During training, we often use teacher forcing: we feed the model the correct word from the training set. But during generation, the model must feed itself its own predictions\u2014mistakes and all.</p> <p>This leads to a problem called exposure bias:</p> <p>One bad prediction early on \u2192 weird context \u2192 worse predictions \u2192 gibberish</p> <p>Since the model is rarely trained to handle its own errors, they snowball during generation.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#4-repetition-and-lack-of-coherence","title":"\ud83c\udf00 4. Repetition and Lack of Coherence","text":"<p>RNN-generated text often suffers from:</p> <ul> <li>Repetitive phrases (\"I am a robot. I am a robot...\")</li> <li>Short-sightedness (can\u2019t plan a full sentence or story)</li> <li>Drift in meaning (starts on one topic, ends somewhere else)</li> </ul> <p>This is partly due to limited memory and the model\u2019s inability to reason globally about the full sequence.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#5-no-real-attention","title":"\u274c 5. No Real Attention","text":"<p>RNNs treat each past word equally (or rely only on hidden states). They don\u2019t have a way to say:</p> <p>\u201cThis specific past word is especially important right now.\u201d</p> <p>They lack attention\u2014a mechanism that helps later models like Transformers focus on the most relevant parts of a sequence.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html#summary","title":"\ud83e\udde9 Summary","text":"<p>While RNNs (and their gated cousins like LSTMs and GRUs) have been foundational in NLP, they face serious limitations:</p> <ul> <li>Hard to scale to long or complex text</li> <li>Slow and sequential in nature</li> <li>Prone to error accumulation</li> <li>Struggle to retain and use precise context</li> </ul> <p>These limitations paved the way for a new idea: What if a model could look at the entire sentence\u2014or even the entire paragraph\u2014all at once?</p> <p>In the next chapter, we\u2019ll step into the world of attention and contextual embeddings\u2014the powerful innovations that made Transformers, BERT, and GPT possible.</p>"},{"location":"Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/RNN.html","title":"RNN","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[3]: Copied! <pre># ---- Step 1: Define the vocabulary ----\nvocab = [\"I\", \"love\", \"cats\"]\nword_to_ix = {word: i for i, word in enumerate(vocab)}\nword_to_ix\n</pre> # ---- Step 1: Define the vocabulary ---- vocab = [\"I\", \"love\", \"cats\"] word_to_ix = {word: i for i, word in enumerate(vocab)} word_to_ix  Out[3]: <pre>{'I': 0, 'love': 1, 'cats': 2}</pre> In\u00a0[4]: Copied! <pre># ---- Step 2: One-hot encoding ----\ndef one_hot(idx, size):\n    vec = np.zeros(size)\n    vec[idx] = 1.0\n    return vec\n</pre> # ---- Step 2: One-hot encoding ---- def one_hot(idx, size):     vec = np.zeros(size)     vec[idx] = 1.0     return vec  In\u00a0[5]: Copied! <pre># ---- Step 3: Initialize weights and hidden state ----\ninput_size = 3     # one-hot size\nhidden_size = 2    # size of the RNN memory\n</pre> # ---- Step 3: Initialize weights and hidden state ---- input_size = 3     # one-hot size hidden_size = 2    # size of the RNN memory  In\u00a0[8]: Copied! <pre># Weights (random small numbers)\nW = np.random.randn(hidden_size, input_size) * 0.1\nU = np.random.randn(hidden_size, hidden_size) * 0.1\nb = np.zeros(hidden_size)\n\n# Initial hidden state\nh_prev = np.zeros(hidden_size)\n\nW, U, b, h_prev\n</pre> # Weights (random small numbers) W = np.random.randn(hidden_size, input_size) * 0.1 U = np.random.randn(hidden_size, hidden_size) * 0.1 b = np.zeros(hidden_size)  # Initial hidden state h_prev = np.zeros(hidden_size)  W, U, b, h_prev Out[8]: <pre>(array([[-0.04229369,  0.06272946,  0.02243791],\n        [-0.09266698, -0.01642113,  0.11307286]]),\n array([[-0.02324619, -0.06041382],\n        [-0.05441866, -0.05885374]]),\n array([0., 0.]),\n array([0., 0.]))</pre> In\u00a0[9]: Copied! <pre># ---- Step 4: Input sentence ----\nsentence = [\"I\", \"love\", \"cats\"]\n\nprint(\"Step-by-step hidden states:\\n\")\n\nfor word in sentence:\n    x = one_hot(word_to_ix[word], input_size)  # input vector\n    h_t = np.tanh(np.dot(W, x) + np.dot(U, h_prev) + b)  # RNN update\n    print(f\"Word: {word:5}  | Hidden state: {h_t}\")\n    h_prev = h_t  # carry hidden state forward\n</pre>   # ---- Step 4: Input sentence ---- sentence = [\"I\", \"love\", \"cats\"]  print(\"Step-by-step hidden states:\\n\")  for word in sentence:     x = one_hot(word_to_ix[word], input_size)  # input vector     h_t = np.tanh(np.dot(W, x) + np.dot(U, h_prev) + b)  # RNN update     print(f\"Word: {word:5}  | Hidden state: {h_t}\")     h_prev = h_t  # carry hidden state forward <pre>Step-by-step hidden states:\n\nWord: I      | Hidden state: [-0.04226849 -0.09240264]\nWord: love   | Hidden state: [ 0.06918374 -0.00868247]\nWord: cats   | Hidden state: [0.02135095 0.10937961]\n</pre> In\u00a0[10]: Copied! <pre># ---- Step 5: Final hidden state ----\nprint(\"\\nFinal hidden state:\", h_prev)\n# This is the final hidden state after processing the entire sentence.\n# It can be used for further tasks like classification or sequence generation.\n# The hidden state captures the context of the input sequence.\n</pre> # ---- Step 5: Final hidden state ---- print(\"\\nFinal hidden state:\", h_prev) # This is the final hidden state after processing the entire sentence. # It can be used for further tasks like classification or sequence generation. # The hidden state captures the context of the input sequence.           <pre>\nFinal hidden state: [0.02135095 0.10937961]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}