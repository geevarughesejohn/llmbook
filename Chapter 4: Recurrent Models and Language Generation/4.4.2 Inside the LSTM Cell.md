

## **4.4.2 Inside the LSTM Cell**

If vanilla RNNs are forgetful readers, then LSTMs are thoughtful ones‚Äîwith a notepad in hand. They don‚Äôt just react to each new word blindly. Instead, they ask themselves: *Should I remember this? Should I ignore it? Should I update what I know?*

To do this, an LSTM uses a more sophisticated internal structure‚Äîa **memory cell** guided by **gates**. These gates are what give the LSTM its power to remember relevant information and forget the rest.

Let‚Äôs step inside.

---

### üß† Two Kinds of Memory

An LSTM maintains two pieces of memory at each time step:

1. **Cell State** $c_t$: The long-term memory. This is the key to remembering information over many time steps.
2. **Hidden State** $h_t$: The short-term memory, used for output and interactions with the rest of the network.

The cell state acts like a conveyor belt‚Äîit flows through the sequence largely unchanged, with small edits made along the way via the **gates**.

---

### üîê The Three Gates

Each LSTM cell contains **three gates**. Think of each gate as a little controller that decides how much information passes through.

Let‚Äôs say we‚Äôre processing word $x_t$ at time step $t$, and we already have the previous hidden state $h_{t-1}$ and previous cell state $c_{t-1}$.

#### 1. **Forget Gate** $f_t$

This gate decides what parts of the old memory to forget.

$$
f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
$$

* $\sigma$ is the sigmoid function (outputs values between 0 and 1)
* If $f_t[i] = 0$, that part of the memory is erased
* If $f_t[i] = 1$, it‚Äôs kept completely

**Analogy**: You‚Äôre reviewing old notes. You erase parts that aren‚Äôt relevant anymore.

---

#### 2. **Input Gate** $i_t$ and Candidate Cell State $\tilde{c}_t$

These determine **what new information** to store.

$$
i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
$$

$$
\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
$$

* $i_t$ controls **how much** of $\tilde{c}_t$ (new info) gets added
* $\tilde{c}_t$ is the candidate update, passed through `tanh`

**Analogy**: You hear something new. The input gate decides whether it‚Äôs worth noting, and how strongly to add it to memory.

---

#### 3. **Update the Cell State** $c_t$

This is where memory gets updated:

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

* $\odot$ means element-wise multiplication
* We combine the **old memory** (after forgetting) with the **new candidate memory** (after filtering)

This is the heart of the LSTM: a balance between remembering and updating.

---

#### 4. **Output Gate** $o_t$

Finally, the output gate decides what part of the memory to expose as the hidden state:

$$
o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
$$

$$
h_t = o_t \odot \tanh(c_t)
$$

This hidden state $h_t$ is what gets passed to the next LSTM cell and can be used for predictions.

**Analogy**: You‚Äôve taken notes and processed them‚Äînow you share a summary of what‚Äôs relevant.

---

### üß© Recap: The LSTM Flow

1. **Forget** part of the old memory
2. **Input** new information and update the memory
3. **Output** the relevant parts for the next step

---

### üîÑ Why This Works

By using **gates**, LSTMs give the network control over what to remember and what to forget‚Äîsolving the core problem that plagues vanilla RNNs.

* Gradients can now flow through the cell state **without vanishing**, because memory updates are additive, not multiplicative.
* The network **learns when to remember** important signals and **ignore noise**.

In short: LSTMs have **learnable memory control**.

Next, we‚Äôll see how this cell operates step by step over a sequence‚Äîand how it actually performs in practice.


