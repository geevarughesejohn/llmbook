

## **4.2.5 Strengths and Weaknesses of Vanilla RNNs**

By now, you‚Äôve seen how Recurrent Neural Networks (RNNs) can read a sentence one word at a time, carry memory from word to word, and gradually build a hidden representation of the entire sequence. That‚Äôs already a huge improvement over traditional bag-of-words models or even Word2Vec embeddings, which ignore the order and structure of a sentence.

So what are RNNs really good at? And where do they fall short?

Let‚Äôs break it down.

---

### ‚úÖ **Strengths of Vanilla RNNs**

1. **Handles Sequences of Variable Length**

   RNNs don‚Äôt require input of a fixed size like feedforward networks. You can feed them sentences that are 3 words long or 300 words long‚Äîthey‚Äôll process each step the same way, updating the hidden state as they go.

2. **Captures Temporal Dependencies**

   RNNs introduce the critical idea of **temporal modeling**‚Äîthe meaning of a word depends on its neighbors. They can learn that ‚Äúgood morning‚Äù is common, but ‚Äúgood elephant‚Äù is not. This makes them suitable for a wide range of NLP tasks: next-word prediction, part-of-speech tagging, named entity recognition, and more.

3. **Parameter Sharing Across Time Steps**

   Since RNNs use the same weights at every time step, they‚Äôre efficient and generalizable. The same logic that processes word #1 can process word #50.

---

### ‚ùå **Weaknesses of Vanilla RNNs**

1. **Short-Term Memory Only**

   While RNNs can remember what happened one or two steps ago, they quickly ‚Äúforget‚Äù things as the sequence gets longer. For example, in the sentence:

   > ‚ÄúThe book that the student who the teacher liked was reading was long.‚Äù

   A simple RNN might struggle to relate the first ‚Äúbook‚Äù to the final ‚Äúwas long,‚Äù because the relevant words are too far apart. This is a **long-term dependency**, and vanilla RNNs are not good at handling it.

2. **Vanishing Gradient Problem**

   When training an RNN using **backpropagation through time**, gradients are passed backward across many steps. In long sequences, these gradients can become **very small** (vanish) or **very large** (explode). When gradients vanish, the model fails to learn long-range patterns.

   > In simpler terms: the further back you try to remember, the weaker your memory becomes during training.

3. **Slow Training and Inference**

   RNNs process inputs **sequentially**‚Äîthey can‚Äôt parallelize across time steps. This makes them slower than newer architectures like Transformers, which we‚Äôll encounter soon.

4. **Difficult to Tune**

   Vanilla RNNs can be unstable during training. Choosing the right learning rate, weight initialization, and sequence length often requires trial and error.

---

### üîÅ Summary: RNNs Are a Good Start, But Not Enough

Vanilla RNNs were revolutionary when they first appeared. They introduced the key idea that **language is sequential** and that **memory matters**.

But as we push toward modeling **longer sequences** and more **complex dependencies**, RNNs begin to break down.

That‚Äôs why researchers created improved versions of RNNs: the **Long Short-Term Memory (LSTM)** and the **Gated Recurrent Unit (GRU)**. These models were designed specifically to fix the memory limitations of vanilla RNNs, while keeping the core idea of sequence modeling intact.

