
## **4.2.1 The Need for Sequential Processing**

Imagine you’re reading a sentence one word at a time. As you move from left to right, each new word builds on the ones before it:

> “She opened the door and saw a…”

At this point, your brain is making predictions. A what? A cat? A surprise? A ghost?

Now let’s change just one word earlier in the sentence:

> “She locked the door and saw a…”

That tiny change—*opened* to *locked*—can completely shift what we expect next. Human language is like that: deeply sensitive to **order**, **timing**, and **context**.

So far, the models we’ve discussed—like Word2Vec or Bag-of-Words—don't consider word order at all. To them, the sentence “The cat chased the mouse” is just a bag of words: {cat, chased, the, mouse}. You could shuffle them around—“Mouse chased the cat the”—and the model wouldn't notice the difference.

Clearly, this is a big problem. In real language, **order matters**. A lot.

Let’s say we want to teach a machine to complete a sentence like this:

> “The stock market crashed because…”

The right prediction depends on *everything* that came before. A machine needs to **remember** previous words and use that memory to understand the next ones. This is the fundamental idea behind **sequence modeling**.

Now you might ask: why not just use a regular neural network and feed it the entire sentence? The issue is that standard feedforward networks expect fixed-size input. They process all inputs independently, without any sense of time or order. In short, they have **no memory**.

That’s where **Recurrent Neural Networks**, or **RNNs**, come in.

RNNs were designed to model sequences—**text**, **speech**, **music**, even **stock prices**—by maintaining a sort of running memory of what came before. As each new word (or input) is read, the RNN updates its internal state, which is then used to process the next input. This allows it to build a contextual understanding, step by step.

Think of it like reading a sentence with a highlighter in hand. Each new word influences what you highlight next. You don’t start from scratch every time—you carry your understanding forward.

This ability to “remember” previous inputs is what makes RNNs such a natural fit for language. In the next section, we’ll explore exactly how this memory works—and how it's implemented using the hidden state of a recurrent neural network.
