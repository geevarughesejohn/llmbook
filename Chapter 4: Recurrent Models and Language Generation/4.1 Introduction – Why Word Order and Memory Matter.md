

## **4.1 Introduction – Why Word Order and Memory Matter**

In the previous chapter, we explored how words can be transformed into vectors—dense numerical representations that capture the meaning of a word based on how it's used in language. Word2Vec, GloVe, and FastText gave us a powerful starting point: machines could now recognize that *king* and *queen* are related, or that *bank* and *money* often appear in similar contexts.

But there’s a big catch.

All of these models treat words as if they have just **one meaning**, regardless of the sentence they appear in. For example, the word *light* can mean something you switch on in a room, something that's not heavy, or even a kind of beer. These are very different meanings—and yet, traditional word embeddings give *light* a single vector. That’s a problem.

Even more importantly, these models **ignore word order**. Whether a sentence says “the cat chased the dog” or “the dog chased the cat,” the Bag-of-Words model—and even Word2Vec to some extent—would represent them nearly the same. That’s clearly wrong. Word order and structure matter deeply in language.

So how can we build models that understand this?

Let’s think like a machine for a moment. Imagine you’re trying to understand a sentence by reading it one word at a time:

> “The man who wore a red hat…”

At this point, what comes next? Maybe:

> “…sat down.”
> “…walked away.”
> “…was arrested.”

The correct prediction depends not just on the last word (*hat*), but also on the earlier part of the sentence (*the man who wore…*). You need **memory**—a way to carry forward information from earlier in the sentence to inform what comes next.

This is where **sequence models** come in. They’re designed to handle exactly this kind of problem—by processing input one step at a time, remembering past information, and using it to make better predictions in the future.

The first generation of such models in natural language processing were called **Recurrent Neural Networks (RNNs)**. They marked a huge leap forward: for the first time, models could understand a sentence not just as a bag of words, but as a **flow of meaning across time**.

In this chapter, we’ll dive into how these models work. We’ll explore RNNs and their improved variants—**LSTM** and **GRU**—and learn how they give machines a kind of memory. We’ll see how they’re trained to **generate text**, and how they opened the door to more advanced architectures like **Transformers** (which we’ll explore in the next chapter).

Let’s begin our journey into sequence modeling—starting with the core idea behind RNNs.


