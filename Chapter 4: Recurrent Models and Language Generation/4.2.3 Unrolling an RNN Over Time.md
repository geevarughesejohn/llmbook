

## **4.2.3 Unrolling an RNN Over Time**

So far, weâ€™ve treated the RNN like a black box: it takes an input, uses its memory (the hidden state), and produces a new hidden state. But what happens when we feed it an entire **sequence**â€”like a sentence or paragraph?

To understand that, we need to â€œunrollâ€ the RNN.

---

### ðŸ§  What Does "Unrolling" Mean?

An RNN is a **recursive structure**. That means it reuses the same logic (and the same weights) again and againâ€”once for each time step in the input.

To visualize this, we can imagine the RNN **stretched out in time**. Each copy of the RNN processes one word and passes its hidden state to the next:

```
xâ‚ â”€â–¶ [ RNN Cell ] â”€â–¶ hâ‚
        â†“
xâ‚‚ â”€â–¶ [ RNN Cell ] â”€â–¶ hâ‚‚
        â†“
xâ‚ƒ â”€â–¶ [ RNN Cell ] â”€â–¶ hâ‚ƒ
        â†“
...     ...         ...
```

Each `[ RNN Cell ]` in this diagram is actually the **same RNN**, with the same parameters. Itâ€™s not a deep network with many layersâ€”itâ€™s a **repeated** application of a single RNN unit.

This is what we mean by **weight sharing**: all the cells in the unrolled diagram use the same set of weights $W, U, b$. That makes the model more efficient and helps it generalize better to sequences of varying lengths.

---

### ðŸ” Step-by-Step Processing

Letâ€™s walk through an example sentence:

> "I love cats"

1. The first word, `"I"`, is converted to a vector $x_1$.
   The RNN uses this and an initial hidden state $h_0$ (usually set to zeros) to compute $h_1$.

2. The second word, `"love"`, is converted to $x_2$.
   The RNN uses $x_2$ and the hidden state $h_1$ to compute $h_2$.

3. The third word, `"cats"`, is processed using $x_3$ and $h_2$, producing $h_3$.

The final hidden state $h_3$ now contains a compressed summary of the entire sentence so far.

---

### ðŸ§  Whatâ€™s So Special About This?

This process is what allows the RNN to **carry information forward** through a sentence. Itâ€™s not just looking at the current wordâ€”itâ€™s combining the current word with what it has already learned.

This is crucial for tasks like:

* **Next word prediction**: "The dog barked at the \_\_\_"
* **Sentiment analysis**: "I absolutely hated the movie"
* **Sequence labeling**: tagging each word with its part of speech

At each time step, the model can output somethingâ€”like a predicted word or tagâ€”or wait until the end of the sentence and output a single decision.

---

### ðŸ”„ Backpropagation Through Time (BPTT)

When we train an RNN, we donâ€™t just update the weights after one time stepâ€”we **propagate the error backward** through **all time steps**.

This process is called **Backpropagation Through Time**, or **BPTT**. Itâ€™s like backpropagation in regular neural networks, except we stretch it out over the entire sequence. This allows the model to learn from patterns that span many steps.

However, this also introduces some serious challenges, which weâ€™ll explore next.

As sequences get longer, the gradients that flow backward can start to **shrink** (or sometimes explode). This makes it hard for the model to remember information from early in the sequence. This problem is known as the **vanishing gradient problem**, and itâ€™s one of the biggest limitations of vanilla RNNs.

In the next section, weâ€™ll understand this problem in more depth and see why it motivated the creation of more advanced architectures like **LSTM** and **GRU**.

