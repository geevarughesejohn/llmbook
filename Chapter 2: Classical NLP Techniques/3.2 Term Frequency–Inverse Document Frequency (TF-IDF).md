
## **2.3.2 Term Frequencyâ€“Inverse Document Frequency (TF-IDF)**

---

### ğŸ§  Why Not Just Count Words?

The Bag-of-Words model is a great starting point. Itâ€™s simple, fast, and often surprisingly effective. But as you saw earlier, it treats **all words as equally important**â€”and thatâ€™s a problem.

Imagine youâ€™re building a document classifier, and every document starts with the phrase:

> â€œThis document is aboutâ€¦â€

The words *â€œthisâ€*, *â€œisâ€*, and *â€œaboutâ€* will show up in almost every document. As a result, theyâ€™ll have high countsâ€”but theyâ€™re not really helpful in telling documents apart.

What we want is a way to **give more importance to informative words** and **less importance to common, boring ones**.

Thatâ€™s exactly what **TF-IDF** does.

---

### ğŸ“ What is TF-IDF?

TF-IDF stands for:

* **Term Frequency (TF)** â€“ How often a word appears in a document
* **Inverse Document Frequency (IDF)** â€“ How rare the word is across all documents

By combining these two ideas, TF-IDF gives higher scores to **important words** (frequent in one document, rare in others), and lower scores to **common words** (frequent everywhere).

---

### ğŸ”¢ Letâ€™s Break It Down

#### âœ… Term Frequency (TF)

This measures how often a word shows up in a document. The more frequent, the more importantâ€”*within that document*.

$$
\text{TF}(w, d) = \frac{\text{Number of times } w \text{ appears in } d}{\text{Total number of words in } d}
$$

So if â€œmachineâ€ appears **3 times** in a 100-word document, its TF is `3 / 100 = 0.03`.

---

#### âœ… Inverse Document Frequency (IDF)

This measures how **rare** a word is across all documents in the dataset. The fewer documents it appears in, the higher its score.

$$
\text{IDF}(w) = \log\left(\frac{N}{1 + n_w}\right)
$$

Where:

* $N$ is the total number of documents
* $n_w$ is the number of documents containing the word $w$
* Adding 1 prevents division by zero

So if â€œlearningâ€ appears in **2 out of 10 documents**, its IDF is:

$$
\log\left(\frac{10}{1 + 2}\right) = \log\left(\frac{10}{3}\right) \approx 0.52
$$

> ğŸ” Common words like â€œisâ€ or â€œtheâ€ will have **very low IDF** scores. Thatâ€™s the magic of TF-IDFâ€”it automatically **downweights** them.

---

#### âœ… TF-IDF = TF Ã— IDF

To get the final score for a word in a document, multiply its **TF** and **IDF**.

$$
\text{TF-IDF}(w, d) = \text{TF}(w, d) \times \text{IDF}(w)
$$

This score is high when:

* The word is **frequent in one document**, and
* **Rare in the overall dataset**

---

### ğŸ” Mini Example

Letâ€™s use the same three documents again:

```text
Doc 1: "I love NLP"
Doc 2: "I love machine learning"
Doc 3: "NLP is fun"
```

#### Step 1: Count Term Frequencies

* â€œloveâ€ appears in Doc 1 and Doc 2: TF = 1 / 3 = 0.33 (for each)
* â€œmachineâ€ appears only in Doc 2: TF = 1 / 4 = 0.25

#### Step 2: Compute IDF

* Total documents = 3
* â€œloveâ€ appears in 2 docs â†’ IDF = log(3 / (1 + 2)) = log(1) = 0.0
* â€œmachineâ€ appears in 1 doc â†’ IDF = log(3 / (1 + 1)) = log(1.5) â‰ˆ 0.405

#### Step 3: Final TF-IDF Scores

* â€œloveâ€ â†’ 0.33 Ã— 0.0 = 0.0 âœ… *common word, downweighted*
* â€œmachineâ€ â†’ 0.25 Ã— 0.405 â‰ˆ 0.101 âœ… *rare, boosted*

> ğŸ’¡ So TF-IDF gives **high scores to unique, meaningful words**â€”and **low scores to generic, frequent words**.

---

### ğŸ§‘â€ğŸ’» Code Example: TF-IDF in Python

Letâ€™s implement TF-IDF using `TfidfVectorizer` from Scikit-learn:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

docs = [
    "I love NLP",
    "I love machine learning",
    "NLP is fun"
]

# Create and fit the TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(docs)

# Convert to readable format
import pandas as pd
df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
print(df)
```

#### ğŸ“¤ Output

|       | fun   | is    | learning | love  | machine | nlp   |
| ----- | ----- | ----- | -------- | ----- | ------- | ----- |
| Doc 1 | 0     | 0     | 0        | 0.707 | 0       | 0.707 |
| Doc 2 | 0     | 0     | 0.577    | 0.577 | 0.577   | 0     |
| Doc 3 | 0.577 | 0.577 | 0        | 0     | 0       | 0.577 |

Notice:

* Words like **â€œloveâ€** appear in multiple documents â†’ **lower scores**
* Words like **â€œfunâ€**, **â€œmachineâ€**, and **â€œlearningâ€** are more specific â†’ **higher scores**

---

### âš–ï¸ Comparison with Bag-of-Words

| Feature                | Bag-of-Words     | TF-IDF                     |
| ---------------------- | ---------------- | -------------------------- |
| Uses raw word counts   | âœ… Yes            | âŒ No                       |
| Weights rare terms     | âŒ No             | âœ… Yes                      |
| Penalizes common terms | âŒ No             | âœ… Yes                      |
| Captures meaning       | âŒ No             | âŒ Still limited            |
| Output vectors         | Sparse, high-dim | Sparse, weighted, high-dim |

---

### ğŸ¯ When to Use TF-IDF

Use TF-IDF when:

* You want to highlight unique or rare words in your text
* Youâ€™re doing tasks like **document classification**, **search engines**, or **information retrieval**
* You want to improve performance over basic BoW without jumping into deep learning


