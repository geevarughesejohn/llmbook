
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../1%20Introduction%3A%20What%20is%20Classical%20NLP%3F/">
      
      
        <link rel="next" href="../3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>2 Text Preprocessing: Cleaning Up Language - From Words to Intelligence: A Complete Guide to Large Language Models</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#22-text-preprocessing-cleaning-up-language" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="From Words to Intelligence: A Complete Guide to Large Language Models" class="md-header__button md-logo" aria-label="From Words to Intelligence: A Complete Guide to Large Language Models" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            From Words to Intelligence: A Complete Guide to Large Language Models
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2 Text Preprocessing: Cleaning Up Language
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="From Words to Intelligence: A Complete Guide to Large Language Models" class="md-nav__button md-logo" aria-label="From Words to Intelligence: A Complete Guide to Large Language Models" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    From Words to Intelligence: A Complete Guide to Large Language Models
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 1: The Language Modeling Problem  
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1: The Language Modeling Problem  
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.1%20learning%20objective/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 1: The Language Modeling Problem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.2%20Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.2 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.3 What Does It Mean to “Model” Language?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.4 A Brief History of Language Modeling: Rules → Statistics → Neural Nets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.5 Why Is Modeling Language So Difficult?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.6 Code Example: A Tiny Predictive Language Model (with N Grams)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.7%20Summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.7 Summary
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.8%20Key%20Takeaways/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.8 Key Takeaways
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.9 Quiz Exercises
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/ngrms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ngrms
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 2: Classical NLP Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Chapter 2: Classical NLP Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1%20Introduction%3A%20What%20is%20Classical%20NLP%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1 Introduction: What is Classical NLP?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    2 Text Preprocessing: Cleaning Up Language
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    2 Text Preprocessing: Cleaning Up Language
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#22-text-preprocessing-cleaning-up-language" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Text Preprocessing: Cleaning Up Language
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Text Preprocessing: Cleaning Up Language">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-do-we-need-to-preprocess-text" class="md-nav__link">
    <span class="md-ellipsis">
      🧹 Why Do We Need to Preprocess Text?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#221-basic-text-cleaning-steps" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 2.2.1 Basic Text Cleaning Steps
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔧 2.2.1 Basic Text Cleaning Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-lowercasing" class="md-nav__link">
    <span class="md-ellipsis">
      1. Lowercasing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-removing-punctuation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Removing Punctuation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-removing-stopwords" class="md-nav__link">
    <span class="md-ellipsis">
      3. Removing Stopwords
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-tokenization-breaking-text-into-pieces" class="md-nav__link">
    <span class="md-ellipsis">
      ✂️ 2.2.2 Tokenization: Breaking Text into Pieces
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✂️ 2.2.2 Tokenization: Breaking Text into Pieces">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Word Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentence-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-stemming-and-lemmatization" class="md-nav__link">
    <span class="md-ellipsis">
      🌱 2.2.3 Stemming and Lemmatization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🌱 2.2.3 Stemming and Lemmatization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stemming-a-crude-cutter" class="md-nav__link">
    <span class="md-ellipsis">
      Stemming – A Crude Cutter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemmatization-a-smarter-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Lemmatization – A Smarter Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-part-of-speech-pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      🔠 2.2.4 Part-of-Speech (POS) Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#figure-basic-nlp-preprocessing-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Figure: Basic NLP Preprocessing Pipeline
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-look-stemming-lemmatization-pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 Detailed Look: Stemming, Lemmatization &amp; POS Tagging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔍 Detailed Look: Stemming, Lemmatization &amp; POS Tagging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stemming-cutting-without-thinking-too-much" class="md-nav__link">
    <span class="md-ellipsis">
      🌿 Stemming – Cutting Without Thinking Too Much
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🌿 Stemming – Cutting Without Thinking Too Much">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works-internally" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How it works internally:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-breakdown" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example Breakdown:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemmatization-cutting-with-grammar-and-a-dictionary" class="md-nav__link">
    <span class="md-ellipsis">
      📚 Lemmatization – Cutting with Grammar and a Dictionary
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📚 Lemmatization – Cutting with Grammar and a Dictionary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works-internally_1" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How it works internally:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-breakdown_1" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example Breakdown:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#part-of-speech-pos-tagging-labeling-words-with-their-roles" class="md-nav__link">
    <span class="md-ellipsis">
      🏷️ Part-of-Speech (POS) Tagging – Labeling Words with Their Roles
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🏷️ Part-of-Speech (POS) Tagging – Labeling Words with Their Roles">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-pos-taggers-work-internally" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How POS Taggers Work Internally
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-rule-based-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      1. Rule-Based Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-statistical-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      2. Statistical Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-neural-tagging-modern" class="md-nav__link">
    <span class="md-ellipsis">
      3. Neural Tagging (Modern)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-with-nltk" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example with NLTK:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-differences" class="md-nav__link">
    <span class="md-ellipsis">
      Summary of Differences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3 Why Machines Need Numbers for clarity?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3.1%20The%20Bag-of-Words%20%28BoW%29%20Model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.1 The Bag of Words (BoW) Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2 Term Frequency–Inverse Document Frequency (TF IDF)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 TF IDF: Sparse Vectors and High Dimensionality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4%20Classical%20NLP%20Pipelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4 Classical NLP Pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5 Limitations of Rule Based and Statistical Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../E%3AA%20Classical%20Pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:A Classical Pipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../E%3ABoW%20with%20Scikit-learn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:BoW with Scikit learn
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../E%3ATF-IDF/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:TF IDF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3: Word Embeddings and Dense Representations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
     Code Example: Using Pretrained Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.1%20What%20Is%20a%20Word%20Embedding%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.1 What Is a Word Embedding?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.2%20Sparse%20vs.%20Dense%20Representations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.2 Sparse vs. Dense Representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.3%20How%20Embeddings%20Represent%20Similarity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.3 How Embeddings Represent Similarity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.4 The Shape of Meaning: Embedding Space
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.2.5%20A%20Glimpse%20at%20the%20Math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.5 A Glimpse at the Math
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.3%20How%20Embeddings%20Capture%20Meaning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 How Embeddings Capture Meaning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.1%20What%20Is%20Word2Vec%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.1 What Is Word2Vec?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.2 CBOW and Skip Gram: Two Learning Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.3 Training Word2Vec: How the Model Learns
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.4 Negative Sampling and Efficiency Tricks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.5 Practical Example: Training Word2Vec in Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.6 Strengths and Limitations of Word2Vec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5%20Beyond%20Word2Vec%20%E2%80%93%20The%20Evolution%20of%20Word%20Embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5 Beyond Word2Vec – The Evolution of Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.1 Why Move Beyond Word2Vec?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.2 FastText – Words as Bags of Subwords
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.3 GloVe – Global Vectors for Word Representation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.4 Comparing Word2Vec, GloVe, and FastText
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.6 Limitations of Word Embeddings — And What Comes Next
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/Learning%20Objectives/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/chapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 3E: Introduction to Machine Learning and Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3E: Introduction to Machine Learning and Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 4: Recurrent Models and Language Generation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Chapter 4: Recurrent Models and Language Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.1%20Introduction%20%E2%80%93%20Why%20Word%20Order%20and%20Memory%20Matter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.1 Introduction – Why Word Order and Memory Matter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.1%20The%20Need%20for%20Sequential%20Processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.1 The Need for Sequential Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.2 The Core Idea of Recurrence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.3 Unrolling an RNN Over Time
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.4 RNN in Practice – A Step by Step Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.5: Strengths and Weaknesses of Vanilla RNNs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.3 The Problem of Vanishing Gradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.1 Motivation: Why Vanilla RNNs Need Help
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.2 Inside the LSTM Cell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.3 How LSTMs Process Sequences
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.4 When and Why to Use LSTMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.1 Motivation for GRUs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.2 Inside the GRU Cell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.3 Comparing GRU and LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.4 When to Use GRUs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.1: Language Generation with RNNs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.2 Training a Recurrent Language Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.3 Sampling and Generation During Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.4 Code Example – Generating Text with GRU
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.5 Challenges and Limitations of RNN based Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/RNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#22-text-preprocessing-cleaning-up-language" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Text Preprocessing: Cleaning Up Language
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Text Preprocessing: Cleaning Up Language">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-do-we-need-to-preprocess-text" class="md-nav__link">
    <span class="md-ellipsis">
      🧹 Why Do We Need to Preprocess Text?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#221-basic-text-cleaning-steps" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 2.2.1 Basic Text Cleaning Steps
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔧 2.2.1 Basic Text Cleaning Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-lowercasing" class="md-nav__link">
    <span class="md-ellipsis">
      1. Lowercasing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-removing-punctuation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Removing Punctuation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-removing-stopwords" class="md-nav__link">
    <span class="md-ellipsis">
      3. Removing Stopwords
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-tokenization-breaking-text-into-pieces" class="md-nav__link">
    <span class="md-ellipsis">
      ✂️ 2.2.2 Tokenization: Breaking Text into Pieces
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✂️ 2.2.2 Tokenization: Breaking Text into Pieces">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Word Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentence-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-stemming-and-lemmatization" class="md-nav__link">
    <span class="md-ellipsis">
      🌱 2.2.3 Stemming and Lemmatization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🌱 2.2.3 Stemming and Lemmatization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stemming-a-crude-cutter" class="md-nav__link">
    <span class="md-ellipsis">
      Stemming – A Crude Cutter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemmatization-a-smarter-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Lemmatization – A Smarter Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-part-of-speech-pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      🔠 2.2.4 Part-of-Speech (POS) Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#figure-basic-nlp-preprocessing-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Figure: Basic NLP Preprocessing Pipeline
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-look-stemming-lemmatization-pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 Detailed Look: Stemming, Lemmatization &amp; POS Tagging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔍 Detailed Look: Stemming, Lemmatization &amp; POS Tagging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stemming-cutting-without-thinking-too-much" class="md-nav__link">
    <span class="md-ellipsis">
      🌿 Stemming – Cutting Without Thinking Too Much
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🌿 Stemming – Cutting Without Thinking Too Much">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works-internally" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How it works internally:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-breakdown" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example Breakdown:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemmatization-cutting-with-grammar-and-a-dictionary" class="md-nav__link">
    <span class="md-ellipsis">
      📚 Lemmatization – Cutting with Grammar and a Dictionary
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📚 Lemmatization – Cutting with Grammar and a Dictionary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works-internally_1" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How it works internally:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-breakdown_1" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example Breakdown:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#part-of-speech-pos-tagging-labeling-words-with-their-roles" class="md-nav__link">
    <span class="md-ellipsis">
      🏷️ Part-of-Speech (POS) Tagging – Labeling Words with Their Roles
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🏷️ Part-of-Speech (POS) Tagging – Labeling Words with Their Roles">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-pos-taggers-work-internally" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How POS Taggers Work Internally
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-rule-based-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      1. Rule-Based Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-statistical-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      2. Statistical Tagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-neural-tagging-modern" class="md-nav__link">
    <span class="md-ellipsis">
      3. Neural Tagging (Modern)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-with-nltk" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Example with NLTK:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-differences" class="md-nav__link">
    <span class="md-ellipsis">
      Summary of Differences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>2 Text Preprocessing: Cleaning Up Language</h1>

<p>Great! Let’s dive into the heart of classical NLP by beginning with one of its most important and practical components:</p>
<hr />
<h2 id="22-text-preprocessing-cleaning-up-language"><strong>2.2 Text Preprocessing: Cleaning Up Language</strong></h2>
<hr />
<h3 id="why-do-we-need-to-preprocess-text">🧹 Why Do We Need to Preprocess Text?</h3>
<p>Human language is messy.</p>
<p>We speak in slang, write with typos, and use punctuation, emojis, or abbreviations in all kinds of unpredictable ways. Computers, unfortunately, aren’t great at handling this mess—at least not without help.</p>
<p>Before any analysis or modeling can be done, we need to <strong>clean and prepare</strong> the text. This is what <strong>text preprocessing</strong> is all about: transforming raw, unstructured text into a cleaner, more structured format that a machine can understand.</p>
<p>Think of it like preparing vegetables before cooking. You need to wash, peel, and chop them into usable pieces before they go into the pan. Preprocessing text is no different.</p>
<hr />
<h3 id="221-basic-text-cleaning-steps">🔧 2.2.1 Basic Text Cleaning Steps</h3>
<p>Here are some common operations in the text preprocessing toolbox:</p>
<h4 id="1-lowercasing"><strong>1. Lowercasing</strong></h4>
<p>Most NLP systems treat <code>Python</code>, <code>PYTHON</code>, and <code>python</code> as different words. But in most cases, we want to treat them the same.</p>
<pre><code class="language-python">text = &quot;Python is Great!&quot;
text = text.lower()  # Output: &quot;python is great!&quot;
</code></pre>
<h4 id="2-removing-punctuation"><strong>2. Removing Punctuation</strong></h4>
<p>Punctuation marks don’t always carry meaning in basic NLP tasks. For simplicity, we often strip them away.</p>
<pre><code class="language-python">import string

text = &quot;Hello, world! How's everything?&quot;
text = text.translate(str.maketrans('', '', string.punctuation))
# Output: &quot;Hello world Hows everything&quot;
</code></pre>
<h4 id="3-removing-stopwords"><strong>3. Removing Stopwords</strong></h4>
<p><strong>Stopwords</strong> are common words like <em>“the”</em>, <em>“is”</em>, and <em>“in”</em> that occur frequently but carry little meaning in isolation. Removing them can reduce noise in the data.</p>
<pre><code class="language-python">from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

text = &quot;This is a simple sentence.&quot;
tokens = word_tokenize(text.lower())
filtered = [word for word in tokens if word not in stopwords.words('english')]
# Output: ['simple', 'sentence']
</code></pre>
<blockquote>
<p>📌 <em>Note: In more advanced models, stopwords are often kept, but for classical NLP, removing them is common.</em></p>
</blockquote>
<hr />
<h3 id="222-tokenization-breaking-text-into-pieces">✂️ 2.2.2 Tokenization: Breaking Text into Pieces</h3>
<p>Imagine you're a chef chopping a vegetable into pieces—that’s what <strong>tokenization</strong> does to text.</p>
<p>It breaks a paragraph or sentence into smaller parts called <strong>tokens</strong>—usually <strong>words</strong>, sometimes <strong>sentences</strong> or even <strong>characters</strong>.</p>
<h4 id="word-tokenization"><strong>Word Tokenization</strong></h4>
<p>Splitting a sentence into individual words.</p>
<pre><code class="language-python">from nltk.tokenize import word_tokenize

text = &quot;Natural Language Processing is fun!&quot;
tokens = word_tokenize(text)
# Output: ['Natural', 'Language', 'Processing', 'is', 'fun', '!']
</code></pre>
<h4 id="sentence-tokenization"><strong>Sentence Tokenization</strong></h4>
<p>Splitting a paragraph into sentences.</p>
<pre><code class="language-python">from nltk.tokenize import sent_tokenize

paragraph = &quot;Hello world. NLP is interesting. Let's learn more!&quot;
sentences = sent_tokenize(paragraph)
# Output: ['Hello world.', 'NLP is interesting.', &quot;Let's learn more!&quot;]
</code></pre>
<blockquote>
<p>🧠 <em>Tokenization is the first step in almost every NLP task. Without it, a computer wouldn’t know what pieces of text to work with.</em></p>
</blockquote>
<hr />
<h3 id="223-stemming-and-lemmatization">🌱 2.2.3 Stemming and Lemmatization</h3>
<p>After tokenization, we often want to reduce words to their <strong>base form</strong>, so that <em>“running”</em>, <em>“runs”</em>, and <em>“ran”</em> all point to the same root idea.</p>
<p>There are two ways to do this:</p>
<h4 id="stemming-a-crude-cutter"><strong>Stemming</strong> – A Crude Cutter</h4>
<p>Stemming is like chopping off parts of words using <strong>rules</strong>, without caring much for grammar. It may not produce real words.</p>
<pre><code class="language-python">from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
words = [&quot;running&quot;, &quot;runner&quot;, &quot;ran&quot;]
stems = [stemmer.stem(word) for word in words]
# Output: ['run', 'runner', 'ran']
</code></pre>
<blockquote>
<p>⚠️ Notice that “runner” stays the same and “ran” isn’t stemmed to “run”.</p>
</blockquote>
<h4 id="lemmatization-a-smarter-approach"><strong>Lemmatization</strong> – A Smarter Approach</h4>
<p>Lemmatization uses a <strong>dictionary</strong> and <strong>part-of-speech tags</strong> to return the correct base word, called the <strong>lemma</strong>.</p>
<pre><code class="language-python">from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize(&quot;running&quot;, pos=&quot;v&quot;)  # Output: 'run'
lemmatizer.lemmatize(&quot;better&quot;, pos=&quot;a&quot;)   # Output: 'good'
</code></pre>
<blockquote>
<p>✅ Lemmatization is generally better for real applications, but it’s slower and requires more information.</p>
</blockquote>
<hr />
<h3 id="224-part-of-speech-pos-tagging">🔠 2.2.4 Part-of-Speech (POS) Tagging</h3>
<p><strong>Part-of-speech tagging</strong> assigns each word a role in the sentence: is it a <strong>noun</strong>, <strong>verb</strong>, <strong>adjective</strong>, or something else?</p>
<p>Why is this useful?</p>
<ul>
<li>It helps lemmatizers decide the correct root form.</li>
<li>It enables rule-based text analysis.</li>
<li>It helps in parsing and understanding sentence structure.</li>
</ul>
<pre><code class="language-python">import nltk
from nltk import pos_tag, word_tokenize

sentence = &quot;The quick brown fox jumps over the lazy dog&quot;
tokens = word_tokenize(sentence)
pos_tags = pos_tag(tokens)
# Output: [('The', 'DT'), ('quick', 'JJ'), ..., ('dog', 'NN')]
</code></pre>
<blockquote>
<p>🧾 POS tags like <code>NN</code> (noun), <code>VB</code> (verb), <code>JJ</code> (adjective) are based on the <strong>Penn Treebank</strong> tagging scheme.</p>
</blockquote>
<hr />
<h3 id="figure-basic-nlp-preprocessing-pipeline">📊 Figure: Basic NLP Preprocessing Pipeline</h3>
<p>Let’s visualize what we’ve just learned with a simple diagram:</p>
<pre><code>Raw Text
   ↓
Lowercase → Remove Punctuation → Tokenize
   ↓                   ↓
Stopword Removal   POS Tagging
   ↓                   ↓
Stemming / Lemmatization
   ↓
Cleaned Tokens
</code></pre>
<blockquote>
<p><em>This pipeline is the foundation of many NLP tasks—from search engines to chatbots. Master it, and you’re well on your way.</em></p>
</blockquote>
<h2 id="detailed-look-stemming-lemmatization-pos-tagging">🔍 <strong>Detailed Look: Stemming, Lemmatization &amp; POS Tagging</strong></h2>
<hr />
<h3 id="stemming-cutting-without-thinking-too-much">🌿 <strong>Stemming – Cutting Without Thinking Too Much</strong></h3>
<p>Stemming is like using a <strong>machete</strong> to chop off the ends of words. It works by applying <strong>predefined rules</strong> that remove common suffixes (like <code>-ing</code>, <code>-ed</code>, <code>-ly</code>) to reduce a word to its <strong>stem</strong>—not necessarily a real word, just a root form.</p>
<h4 id="how-it-works-internally">⚙️ How it works internally:</h4>
<p>Take the <strong>Porter Stemmer</strong>, one of the most popular stemmers.</p>
<p>It applies a sequence of steps like:</p>
<ol>
<li>
<p><strong>Step 1</strong>: Remove plurals and past participles:</p>
</li>
<li>
<p><code>caresses → caress</code>, <code>ponies → poni</code>, <code>ties → ti</code></p>
</li>
<li>
<p><strong>Step 2</strong>: Remove common suffixes:</p>
</li>
<li>
<p><code>running → run</code>, <code>hopping → hop</code></p>
</li>
<li>
<p><strong>Step 3</strong>: Apply transformation rules:</p>
</li>
<li>
<p><code>national → nation</code>, <code>relational → relation</code></p>
</li>
<li>
<p><strong>Step 4+</strong>: Continue trimming based on patterns until a minimal root is found.</p>
</li>
</ol>
<p>Each rule is hand-crafted using pattern matching (like regular expressions) and applied in a fixed order.</p>
<h4 id="example-breakdown">🧠 Example Breakdown:</h4>
<pre><code class="language-python">from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
print(stemmer.stem(&quot;studies&quot;))  # Output: &quot;studi&quot;
print(stemmer.stem(&quot;studying&quot;)) # Output: &quot;studi&quot;
print(stemmer.stem(&quot;study&quot;))    # Output: &quot;studi&quot;
</code></pre>
<p>Even though all three words mean similar things, <strong>“study”</strong> is turned into <strong>“studi”</strong>, which is not a valid word. That’s okay for a stemmer—it’s not aiming for perfect grammar, just consistency in representation.</p>
<hr />
<h3 id="lemmatization-cutting-with-grammar-and-a-dictionary">📚 <strong>Lemmatization – Cutting with Grammar and a Dictionary</strong></h3>
<p>Lemmatization is more intelligent than stemming. It tries to reduce a word to its <strong>lemma</strong>—the actual dictionary form of the word.</p>
<p>It doesn’t just strip suffixes blindly; instead, it <strong>looks up the word in a lexicon</strong> and considers its <strong>part of speech (POS)</strong>.</p>
<h4 id="how-it-works-internally_1">⚙️ How it works internally:</h4>
<ol>
<li><strong>Token Identification</strong>: Take the word and its POS (noun, verb, etc.).</li>
<li><strong>Morphological Analysis</strong>: Use rules to figure out if the word can be inflected or transformed.</li>
<li><strong>Dictionary Lookup</strong>: Check a large database (like WordNet) for the base form.</li>
<li><strong>Return Lemma</strong>: If a match is found, return the base form. Otherwise, return the original word.</li>
</ol>
<h4 id="example-breakdown_1">🧠 Example Breakdown:</h4>
<pre><code class="language-python">from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize(&quot;better&quot;, pos=&quot;a&quot;))  # Output: &quot;good&quot;
print(lemmatizer.lemmatize(&quot;running&quot;, pos=&quot;v&quot;)) # Output: &quot;run&quot;
</code></pre>
<p>Without the POS:</p>
<pre><code class="language-python">print(lemmatizer.lemmatize(&quot;running&quot;))  # Output: &quot;running&quot;
</code></pre>
<blockquote>
<p>Without knowing whether "running" is a <strong>verb</strong> or a <strong>noun</strong>, the lemmatizer defaults to treating it as a noun—which doesn't change.</p>
</blockquote>
<p>That’s why <strong>lemmatization often needs POS tagging first</strong>. Which brings us to…</p>
<hr />
<h3 id="part-of-speech-pos-tagging-labeling-words-with-their-roles">🏷️ <strong>Part-of-Speech (POS) Tagging – Labeling Words with Their Roles</strong></h3>
<p><strong>POS tagging</strong> is the task of assigning a part of speech to each word in a sentence: is it a <strong>noun</strong>, <strong>verb</strong>, <strong>adjective</strong>, etc.?</p>
<p>This matters because the <strong>same word can play different roles</strong>:</p>
<ul>
<li>“<em>He can </em><em>book</em><em> a room.</em>” → <em>Verb</em></li>
<li>“<em>Read a good </em><em>book</em><em>.</em>” → <em>Noun</em></li>
</ul>
<h4 id="how-pos-taggers-work-internally">⚙️ How POS Taggers Work Internally</h4>
<p>There are three main approaches:</p>
<hr />
<h4 id="1-rule-based-tagging">1. <strong>Rule-Based Tagging</strong></h4>
<ul>
<li>Uses hand-written grammar rules.</li>
<li>For example: If a word ends in <strong>-ly</strong>, it’s probably an adverb.</li>
<li>Example Rule:
  <code>"if word ends in 'ing' and follows a verb → likely a gerund"</code></li>
</ul>
<p>✅ Simple, interpretable
❌ Fragile, limited coverage</p>
<hr />
<h4 id="2-statistical-tagging">2. <strong>Statistical Tagging</strong></h4>
<ul>
<li>Uses probabilistic models trained on labeled data (like the <strong>Penn Treebank</strong>).</li>
<li>
<p>Example: Hidden Markov Models (HMMs)</p>
</li>
<li>
<p>Model how likely each word is to follow another.</p>
</li>
<li>Use <strong>transition probabilities</strong> between tags and <strong>emission probabilities</strong> of words given a tag.</li>
</ul>
<pre><code class="language-text">Previous tag: DT (determiner)
Word: 'book'
→ Probabilities:
   NN (noun) = 0.75
   VB (verb) = 0.25
</code></pre>
<p>✅ Adaptable to many domains
❌ Requires training data</p>
<hr />
<h4 id="3-neural-tagging-modern">3. <strong>Neural Tagging (Modern)</strong></h4>
<ul>
<li>Uses deep learning (e.g., BiLSTM, Transformers).</li>
<li>Captures both <strong>context</strong> and <strong>word semantics</strong>.</li>
<li>Often uses pretrained word embeddings or contextual embeddings.</li>
</ul>
<p>✅ Highly accurate
❌ Overkill for classical NLP</p>
<hr />
<h4 id="example-with-nltk">🧠 Example with NLTK:</h4>
<pre><code class="language-python">import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk import pos_tag, word_tokenize

sentence = &quot;The old man the boats.&quot;
tokens = word_tokenize(sentence)
tags = pos_tag(tokens)
# Output: [('The', 'DT'), ('old', 'JJ'), ('man', 'VB'), ('the', 'DT'), ('boats', 'NNS')]
</code></pre>
<p>In this grammatically tricky sentence, <strong>“man”</strong> is correctly tagged as a <strong>verb</strong>, not a noun—showing that context matters!</p>
<hr />
<h3 id="summary-of-differences">Summary of Differences</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Stemming</th>
<th>Lemmatization</th>
<th>POS Tagging</th>
</tr>
</thead>
<tbody>
<tr>
<td>Goal</td>
<td>Crude root extraction</td>
<td>Meaningful base word</td>
<td>Identify word type (noun, etc.)</td>
</tr>
<tr>
<td>Method</td>
<td>Rule-based suffix stripping</td>
<td>Dictionary + grammar rules</td>
<td>Rule-based, statistical, or neural</td>
</tr>
<tr>
<td>Accuracy</td>
<td>Low to medium</td>
<td>High (if POS given)</td>
<td>Varies by method</td>
</tr>
<tr>
<td>Output Example</td>
<td>"studying" → "studi"</td>
<td>"studying" → "study"</td>
<td>"run" → VB (verb), NN (noun)</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>