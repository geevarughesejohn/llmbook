Awesome! Here‚Äôs a **simple, beginner-friendly code example** to help demystify the basics of language modeling using a small **n-gram model** in Python.

We‚Äôll build a tiny model that predicts the next word based on the previous word‚Äîcalled a **bigram model**.

---

## 6. **Code Example: A Tiny Predictive Language Model (with N-Grams)**

Before deep learning came into the picture, language models were often built using **n-grams**‚Äîsequences of *n* words. For example:

* A **unigram** model predicts words based on how often they occur.
* A **bigram** model predicts the next word based on the **previous** one.
* A **trigram** model uses the previous **two** words, and so on.

Let‚Äôs build a **bigram model** from scratch to see how this works.

---

### üêç Python Code: Simple Bigram Model

```python
import random
from collections import defaultdict

# Our tiny corpus (training data)
corpus = [
    "I love deep learning",
    "I love machine learning",
    "deep learning is fun",
    "machine learning is powerful"
]

# Step 1: Tokenize sentences into word pairs (bigrams)
bigram_model = defaultdict(list)

for sentence in corpus:
    words = sentence.split()
    for i in range(len(words) - 1):
        prefix = words[i]
        next_word = words[i + 1]
        bigram_model[prefix].append(next_word)

# Step 2: Predict the next word given a previous word
def predict_next_word(previous_word):
    possible_words = bigram_model.get(previous_word, [])
    if not possible_words:
        return "<no prediction>"
    return random.choice(possible_words)

# Step 3: Try the model
test_word = "learning"
predicted = predict_next_word(test_word)
print(f"Given '{test_word}', the model predicts: '{predicted}'")
```

---

### üß† What‚Äôs Happening Here?

1. **Tokenization**: We split each sentence into words and collect bigram pairs like ("I", "love") or ("machine", "learning").

2. **Model Creation**: For each word, we store a list of words that follow it in the training data.

3. **Prediction**: Given a word, we randomly pick one of its likely next words based on what it saw during training.

> üí° **Note**: This is a toy model. Real language models learn *probabilities* and generalize better‚Äîbut this example shows the *core idea* behind sequence prediction.

---

### üì• Sample Output

```
Given 'learning', the model predicts: 'is'
```

Try changing `test_word = "I"` or `"deep"` to see how it reacts.

---

In the next section, we‚Äôll summarize what we‚Äôve learned and connect it to the bigger picture of modern LLMs.

Shall we move on to **7. Summary**?
