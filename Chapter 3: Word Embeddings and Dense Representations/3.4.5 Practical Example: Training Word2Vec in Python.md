## **3.4.5 Practical Example: Training Word2Vec in Python**

Now that you understand how Word2Vec works conceptually‚ÄîCBOW vs Skip-Gram, learning word vectors, and speeding things up with negative sampling‚Äîit‚Äôs time to see it **in action**.

In this section, we‚Äôll walk through:

* How to train your own Word2Vec model on a small dataset
* How to use a pre-trained model
* How to explore word similarities and analogies

We‚Äôll use the popular `gensim` library, which offers a simple and efficient implementation of Word2Vec.

---

### üõ†Ô∏è Setting Up

First, make sure `gensim` is installed:

```bash
pip install gensim
```

Now, let‚Äôs import what we need:

```python
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')
```

---

### üìö Step 1: Prepare the Data

We‚Äôll use a few sample sentences for training. In real use, you‚Äôd use thousands or millions of sentences from real-world text.

```python
sentences = [
    "The cat sat on the mat",
    "The dog barked at the cat",
    "Dogs and cats are common pets",
    "She put the food on the mat",
    "He loves his pet dog"
]

# Tokenize the sentences into words
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]
```

Here, we lowercase and tokenize each sentence. Gensim expects a list of word lists like this:

```python
[['the', 'cat', 'sat', 'on', 'the', 'mat'],
 ['the', 'dog', 'barked', 'at', 'the', 'cat'],
 ...]
```

---

### üß† Step 2: Train the Word2Vec Model

Let‚Äôs use the Skip-Gram model (`sg=1`) with a small embedding size and a short context window:

```python
model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=50,      # Size of word embeddings
    window=2,            # Context window size
    min_count=1,         # Include all words (no threshold)
    sg=1,                # Use Skip-Gram (0 for CBOW)
    negative=5,          # Use negative sampling
    epochs=100           # More epochs for small dataset
)
```

That‚Äôs it! The model is trained and ready to use.

---

### üîç Step 3: Explore the Word Embeddings

Let‚Äôs try finding **similar words**:

```python
print(model.wv.most_similar("cat"))
```

You might get something like:

```
[('dog', 0.87), ('mat', 0.72), ('sat', 0.66), ('barked', 0.61)]
```

Try a **word that wasn‚Äôt in the training data**:

```python
print("food" in model.wv)  # Should return True if it was seen
```

---

### üßÆ Step 4: Look at a Word Vector

Each word has a vector of 50 real numbers:

```python
print(model.wv['cat'])
```

Output (truncated for readability):

```
array([ 0.034, -0.142, ..., 0.095], dtype=float32)
```

These vectors are what you‚Äôd feed into downstream tasks or visualize.

---

### üß† Optional: Save and Reload the Model

```python
model.save("word2vec-demo.model")
# To load it later:
loaded_model = Word2Vec.load("word2vec-demo.model")
```

---

### üìä Try Analogy Tasks (If Trained on Larger Data)

With larger datasets, you can try analogies like:

```python
model.wv.most_similar(positive=["king", "woman"], negative=["man"])
```

This would ideally return something close to `"queen"`‚Äîbut only if the model has seen enough examples.

In our small corpus, this likely won‚Äôt work well, but it shows what‚Äôs possible.

---

### ‚úÖ Summary

In just a few lines of code, we:

* Tokenized sentences
* Trained a Word2Vec model (Skip-Gram with Negative Sampling)
* Explored word similarity and vector representations

This simple workflow demonstrates how easy it is to build meaningful embeddings‚ÄîWord2Vec takes care of the heavy lifting.

