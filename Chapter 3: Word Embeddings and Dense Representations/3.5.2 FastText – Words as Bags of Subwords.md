
## **3.5.2 FastText â€“ Words as Bags of Subwords**

Imagine trying to understand a language without knowing how to break down words like â€œrunning,â€ â€œrunner,â€ or â€œhappinessâ€ into parts. Thatâ€™s essentially what Word2Vec doesâ€”it treats every word as a unique, indivisible object.

**FastText**, developed by Facebook AI Research in 2016, offered a clever upgrade:

> **â€œLetâ€™s look inside the words.â€**

Instead of learning a vector for each word as a whole, FastText learns vectors for **parts of words**â€”called **character n-grams**â€”and then combines them to build word embeddings.

This simple idea makes FastText **smarter with rare words**, **better at generalizing**, and **more robust** to spelling variations and new vocabulary.

---

### ğŸ”  The Key Idea: Subword Units

FastText represents each word as a collection of **character-level n-grams**. These are overlapping sequences of characters inside the word.

For example, for the word `"playing"` and n-gram size 3, we might get:

```
<pl, pla, lay, ayi, yin, ing, ng>
```

(Note: FastText also uses special boundary symbols like `<` and `>` to distinguish word edges.)

Each n-gram gets its own vector. To create a word vector:

* FastText **adds up the vectors** for all its n-grams.
* The final embedding represents not just the whole word, but its internal parts.

---

### ğŸ“¦ Why This Matters

This design gives FastText a few powerful advantages over Word2Vec:

#### âœ… 1. Handles Rare and Unseen Words

If the model has never seen the word `"unhappiness"` before, thatâ€™s okay!
As long as it has seen `"happy"`, `"ness"`, `"un"`, etc., it can still **compose a reasonable embedding** by combining subword vectors.

This is especially valuable in:

* **Small datasets**
* **Highly inflected languages**
* **Misspelled or made-up words** (like names or slang)

#### âœ… 2. Learns Morphology Implicitly

Words like `"run"`, `"runs"`, `"running"`, and `"runner"` will share many n-grams, so their vectors will naturally be **close in space**.

This leads to better generalization and **more compact** embedding spaces.

#### âœ… 3. Robust to Typos and Variants

Even if a user types `"runnning"` or `"plaiyng"`, FastText can still **extract familiar subword patterns**, making it less brittle.

---

### ğŸ§  Architecturally Speaking

Internally, FastText extends the **Skip-Gram** model.
The key difference is that it doesnâ€™t use a one-hot vector for the input wordâ€”instead, it uses all the subword vectors to create the input.

The rest of the training process (negative sampling, context prediction) works just like in Word2Vec.

---

### ğŸ§ª Example: Training FastText in Python

Letâ€™s train FastText on a few simple sentences using `gensim`:

```python
from gensim.models import FastText
from nltk.tokenize import word_tokenize

sentences = [
    "Dogs are running fast",
    "A dog runs faster than a cat",
    "Running is good exercise"
]

tokenized = [word_tokenize(sent.lower()) for sent in sentences]

model = FastText(
    sentences=tokenized,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1,            # Skip-Gram
    epochs=50
)

# Try a rare word
print(model.wv['runnning'])  # Misspelled on purpose!
```

Even though `"runnning"` (with triple nâ€™s) wasnâ€™t seen, FastText can still **build a vector** using its subwords.

---

### ğŸ“Š When Should You Use FastText?

Use FastText when:

* Youâ€™re dealing with **morphologically rich languages**
* Your data includes **misspellings or rare words**
* You want better **generalization** from smaller datasets

Itâ€™s slightly slower to train than Word2Vec but **far more robust**.


