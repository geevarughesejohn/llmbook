
## **3.5.4 Comparing Word2Vec, GloVe, and FastText**

By now, you’ve met the three giants of classical word embeddings:

* **Word2Vec**: Predicts words based on local context.
* **FastText**: Adds subword awareness to Word2Vec.
* **GloVe**: Learns embeddings using global co-occurrence statistics.

Each of these methods has its own strengths, weaknesses, and best-use scenarios. Let’s now put them side by side for a clear comparison.

---

### 📊 Comparison Table

| Feature                  | **Word2Vec**                | **FastText**                    | **GloVe**                       |
| ------------------------ | --------------------------- | ------------------------------- | ------------------------------- |
| **Context Type**         | Local (sliding window)      | Local (sliding window)          | Global (co-occurrence matrix)   |
| **Architecture**         | Neural (CBOW/Skip-Gram)     | Neural + Subword units          | Matrix factorization            |
| **Subword Awareness**    | ❌ No                        | ✅ Yes (character n-grams)       | ❌ No                            |
| **Handles OOV Words**    | ❌ No                        | ✅ Yes (via subwords)            | ❌ No                            |
| **One Vector per Word?** | ✅ Yes                       | ✅ Yes (composed from subwords)  | ✅ Yes                           |
| **Training Speed**       | Fast                        | Slightly slower                 | Fast (after matrix is built)    |
| **Interpretability**     | Medium                      | Medium                          | High (explicit co-occurrence)   |
| **Pretrained Models?**   | ✅ Widely available          | ✅ Available (less common)       | ✅ Very popular                  |
| **Popular Use Cases**    | General NLP, classification | Multilingual, noisy data, typos | Semantic tasks, analogy solving |

---

### 🔍 Key Observations

* **Word2Vec** is fast, intuitive, and easy to train—but blind to morphology and word structure.
* **FastText** extends Word2Vec to handle rare and new words better by breaking words into subword chunks.
* **GloVe** is better at capturing **global relationships** and is often preferred for tasks involving semantic similarity or analogies.

---

### 🎯 When to Use Each?

| Use Case                                           | Best Model   |
| -------------------------------------------------- | ------------ |
| You want fast training on clean data               | **Word2Vec** |
| You need to handle typos or rare words             | **FastText** |
| You want to use pretrained vectors quickly         | **GloVe**    |
| You need high-quality analogies or global meaning  | **GloVe**    |
| You're working with morphologically rich languages | **FastText** |

---

### 🧠 But Remember: They’re All Static

All three models share one major limitation:

> **They assign the same vector to a word, regardless of its context.**

This means:

* They can’t distinguish “rock” (music) vs. “rock” (stone)
* They struggle with context shifts and ambiguity
* They miss the syntactic and semantic flow of a sentence

And that’s what the next generation of models set out to fix.

