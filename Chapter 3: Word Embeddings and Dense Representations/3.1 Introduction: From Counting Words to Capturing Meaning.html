
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://geevarughesejohn.github.io/llmbook/Chapter%203%3A%20Word%20Embeddings%20and%20Dense%20Representations/3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html">
      
      
        <link rel="prev" href="%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html">
      
      
        <link rel="next" href="3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Chapter 3: Word Embeddings and Dense Representations - From Words to Intelligence: A Complete Guide to Large Language Models</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-3-word-embeddings-and-dense-representations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="From Words to Intelligence: A Complete Guide to Large Language Models" class="md-header__button md-logo" aria-label="From Words to Intelligence: A Complete Guide to Large Language Models" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            From Words to Intelligence: A Complete Guide to Large Language Models
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 3: Word Embeddings and Dense Representations
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="From Words to Intelligence: A Complete Guide to Large Language Models" class="md-nav__button md-logo" aria-label="From Words to Intelligence: A Complete Guide to Large Language Models" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    From Words to Intelligence: A Complete Guide to Large Language Models
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 1: The Language Modeling Problem  
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1: The Language Modeling Problem  
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.1%20learning%20objective.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 1: The Language Modeling Problem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.2%20Introduction.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.2 Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.3%20What%20Does%20It%20Mean%20to%20%E2%80%9CModel%E2%80%9D%20Language%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.3 What Does It Mean to ‚ÄúModel‚Äù Language?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.4%20A%20Brief%20History%20of%20Language%20Modeling%3A%20Rules%20%E2%86%92%20Statistics%20%E2%86%92%20Neural%20Nets.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.4 A Brief History of Language Modeling: Rules ‚Üí Statistics ‚Üí Neural Nets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.5%20Why%20Is%20Modeling%20Language%20So%20Difficult%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.5 Why Is Modeling Language So Difficult?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.6%20Code%20Example%3A%20A%20Tiny%20Predictive%20Language%20Model%20%28with%20N-Grams%29.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.6 Code Example: A Tiny Predictive Language Model (with N Grams)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.7%20Summary.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.7 Summary
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.8%20Key%20Takeaways.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.8 Key Takeaways
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/1.9%20Quiz%20Exercises.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1.9 Quiz Exercises
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%201%3A%20The%20Language%20Modeling%20Problem%20%20/ngrms.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ngrms
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 2: Classical NLP Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Chapter 2: Classical NLP Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/1%20Introduction%3A%20What%20is%20Classical%20NLP%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1 Introduction: What is Classical NLP?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/2%20Text%20Preprocessing%3A%20Cleaning%20Up%20Language.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2 Text Preprocessing: Cleaning Up Language
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/3%20Why%20Machines%20Need%20Numbers%20for%20clarity%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3 Why Machines Need Numbers for clarity?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/3.1%20The%20Bag-of-Words%20%28BoW%29%20Model.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.1 The Bag of Words (BoW) Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/3.2%20Term%20Frequency%E2%80%93Inverse%20Document%20Frequency%20%28TF-IDF%29.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2 Term Frequency‚ÄìInverse Document Frequency (TF IDF)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/3.3%20TF-IDF%3A%20Sparse%20Vectors%20and%20High%20Dimensionality.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 TF IDF: Sparse Vectors and High Dimensionality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/4%20Classical%20NLP%20Pipelines.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4 Classical NLP Pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/5%20Limitations%20of%20Rule-Based%20and%20Statistical%20Methods.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5 Limitations of Rule Based and Statistical Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/E%3AA%20Classical%20Pipeline.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:A Classical Pipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/E%3ABoW%20with%20Scikit-learn.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:BoW with Scikit learn
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/E%3ATF-IDF.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    E:TF IDF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%202%3A%20Classical%20NLP%20Techniques/chapter.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3: Word Embeddings and Dense Representations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="%20Code%20Example%3A%20Using%20Pretrained%20Word%20Embeddings.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
     Code Example: Using Pretrained Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="3.1%20Introduction%3A%20From%20Counting%20Words%20to%20Capturing%20Meaning.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ Learning Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-sections" class="md-nav__link">
    <span class="md-ellipsis">
      üìö Chapter Sections
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üìö Chapter Sections">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-introduction-from-counting-to-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Introduction: From Counting to Understanding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-what-are-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 What Are Word Embeddings?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-how-embeddings-capture-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 How Embeddings Capture Meaning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-word2vec-learning-embeddings-from-context" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Word2Vec: Learning Embeddings from Context
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-glove-global-vectors-from-co-occurrence" class="md-nav__link">
    <span class="md-ellipsis">
      3.5 GloVe: Global Vectors from Co-occurrence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-fasttext-going-beyond-words" class="md-nav__link">
    <span class="md-ellipsis">
      3.6 FastText: Going Beyond Words
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-working-with-pretrained-embeddings-code-case-study" class="md-nav__link">
    <span class="md-ellipsis">
      3.7 Working with Pretrained Embeddings (Code + Case Study)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#38-limitations-of-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      3.8 Limitations of Word Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#39-from-static-to-contextual-the-next-step" class="md-nav__link">
    <span class="md-ellipsis">
      3.9 From Static to Contextual: The Next Step
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-introduction-from-counting-to-understanding_1" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Introduction: From Counting to Understanding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 Introduction: From Counting to Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-limits-of-counting" class="md-nav__link">
    <span class="md-ellipsis">
      üß± The Limits of Counting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-problem-of-sparse-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      üßä The Problem of Sparse Vectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#words-without-context" class="md-nav__link">
    <span class="md-ellipsis">
      üé≠ Words Without Context
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-need-for-better-representations" class="md-nav__link">
    <span class="md-ellipsis">
      üå± The Need for Better Representations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.2.1%20What%20Is%20a%20Word%20Embedding%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.1 What Is a Word Embedding?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.2.2%20Sparse%20vs.%20Dense%20Representations.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.2 Sparse vs. Dense Representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.2.3%20How%20Embeddings%20Represent%20Similarity.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.3 How Embeddings Represent Similarity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.2.4%20The%20Shape%20of%20Meaning%3A%20Embedding%20Space.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.4 The Shape of Meaning: Embedding Space
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.2.5%20A%20Glimpse%20at%20the%20Math.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.2.5 A Glimpse at the Math
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.3%20How%20Embeddings%20Capture%20Meaning.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.3 How Embeddings Capture Meaning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.1%20What%20Is%20Word2Vec%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.1 What Is Word2Vec?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.2%20CBOW%20and%20Skip-Gram%3A%20Two%20Learning%20Strategies.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.2 CBOW and Skip Gram: Two Learning Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.3%20Training%20Word2Vec%3A%20How%20the%20Model%20Learns.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.3 Training Word2Vec: How the Model Learns
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.4%20Negative%20Sampling%20and%20Efficiency%20Tricks.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.4 Negative Sampling and Efficiency Tricks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.5%20Practical%20Example%3A%20Training%20Word2Vec%20in%20Python.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.5 Practical Example: Training Word2Vec in Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.4.6%20Strengths%20and%20Limitations%20of%20Word2Vec.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.4.6 Strengths and Limitations of Word2Vec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.5%20Beyond%20Word2Vec%20%E2%80%93%20The%20Evolution%20of%20Word%20Embeddings.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5 Beyond Word2Vec ‚Äì The Evolution of Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.5.1%20Why%20Move%20Beyond%20Word2Vec%3F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.1 Why Move Beyond Word2Vec?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.5.2%20FastText%20%E2%80%93%20Words%20as%20Bags%20of%20Subwords.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.2 FastText ‚Äì Words as Bags of Subwords
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.5.3%20GloVe%20%E2%80%93%20Global%20Vectors%20for%20Word%20Representation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.3 GloVe ‚Äì Global Vectors for Word Representation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.5.4%20Comparing%20Word2Vec%2C%20GloVe%2C%20and%20FastText.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.5.4 Comparing Word2Vec, GloVe, and FastText
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="3.6%20Limitations%20of%20Word%20Embeddings%20%E2%80%94%20And%20What%20Comes%20Next.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3.6 Limitations of Word Embeddings ‚Äî And What Comes Next
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="Learning%20Objectives.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 3: Word Embeddings and Dense Representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="chapter.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 3E: Introduction to Machine Learning and Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3E: Introduction to Machine Learning and Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%203E%3A%20Introduction%20to%20Machine%20Learning%20and%20Neural%20Networks/chapter.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 4: Recurrent Models and Language Generation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Chapter 4: Recurrent Models and Language Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.1%20Introduction%20%E2%80%93%20Why%20Word%20Order%20and%20Memory%20Matter.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.1 Introduction ‚Äì Why Word Order and Memory Matter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.1%20The%20Need%20for%20Sequential%20Processing.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.1 The Need for Sequential Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.2%20The%20Core%20Idea%20of%20Recurrence.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.2 The Core Idea of Recurrence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.3%20Unrolling%20an%20RNN%20Over%20Time.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.3 Unrolling an RNN Over Time
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.4%20RNN%20in%20Practice%20%E2%80%93%20A%20Step-by-Step%20Example.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.4 RNN in Practice ‚Äì A Step by Step Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.2.5%3A%20Strengths%20and%20Weaknesses%20of%20Vanilla%20RNNs.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.2.5: Strengths and Weaknesses of Vanilla RNNs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.3%20The%20Problem%20of%20Vanishing%20Gradients.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.3 The Problem of Vanishing Gradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.1%20Motivation%3A%20Why%20Vanilla%20RNNs%20Need%20Help.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.1 Motivation: Why Vanilla RNNs Need Help
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.2%20Inside%20the%20LSTM%20Cell.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.2 Inside the LSTM Cell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.3%20How%20LSTMs%20Process%20Sequences.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.3 How LSTMs Process Sequences
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.4.4%20When%20and%20Why%20to%20Use%20LSTMs.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.4.4 When and Why to Use LSTMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.1%20Motivation%20for%20GRUs.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.1 Motivation for GRUs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.2%20Inside%20the%20GRU%20Cell.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.2 Inside the GRU Cell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.3%20Comparing%20GRU%20and%20LSTM.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.3 Comparing GRU and LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.5.4%20When%20to%20Use%20GRUs.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.5.4 When to Use GRUs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.1%3A%20Language%20Generation%20with%20RNNs.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.1: Language Generation with RNNs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.2%20Training%20a%20Recurrent%20Language%20Model.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.2 Training a Recurrent Language Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.3%20Sampling%20and%20Generation%20During%20Inference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.3 Sampling and Generation During Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.4%20Code%20Example%20%E2%80%93%20Generating%20Text%20with%20GRU.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.4 Code Example ‚Äì Generating Text with GRU
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/4.6.5%20Challenges%20and%20Limitations%20of%20RNN-based%20Generation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4.6.5 Challenges and Limitations of RNN based Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Chapter%204%3A%20Recurrent%20Models%20and%20Language%20Generation/RNN.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ Learning Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-sections" class="md-nav__link">
    <span class="md-ellipsis">
      üìö Chapter Sections
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üìö Chapter Sections">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-introduction-from-counting-to-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Introduction: From Counting to Understanding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-what-are-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 What Are Word Embeddings?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-how-embeddings-capture-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 How Embeddings Capture Meaning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-word2vec-learning-embeddings-from-context" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Word2Vec: Learning Embeddings from Context
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-glove-global-vectors-from-co-occurrence" class="md-nav__link">
    <span class="md-ellipsis">
      3.5 GloVe: Global Vectors from Co-occurrence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-fasttext-going-beyond-words" class="md-nav__link">
    <span class="md-ellipsis">
      3.6 FastText: Going Beyond Words
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-working-with-pretrained-embeddings-code-case-study" class="md-nav__link">
    <span class="md-ellipsis">
      3.7 Working with Pretrained Embeddings (Code + Case Study)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#38-limitations-of-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      3.8 Limitations of Word Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#39-from-static-to-contextual-the-next-step" class="md-nav__link">
    <span class="md-ellipsis">
      3.9 From Static to Contextual: The Next Step
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-introduction-from-counting-to-understanding_1" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Introduction: From Counting to Understanding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 Introduction: From Counting to Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-limits-of-counting" class="md-nav__link">
    <span class="md-ellipsis">
      üß± The Limits of Counting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-problem-of-sparse-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      üßä The Problem of Sparse Vectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#words-without-context" class="md-nav__link">
    <span class="md-ellipsis">
      üé≠ Words Without Context
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-need-for-better-representations" class="md-nav__link">
    <span class="md-ellipsis">
      üå± The Need for Better Representations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="chapter-3-word-embeddings-and-dense-representations"><strong>Chapter 3: Word Embeddings and Dense Representations</strong></h1>
<hr />
<h2 id="learning-objectives"><strong>üéØ Learning Objectives</strong></h2>
<p>By the end of this chapter, the reader will be able to:</p>
<ul>
<li>Understand what <strong>word embeddings</strong> are and why they matter.</li>
<li>See how embeddings solve the limitations of sparse representations like TF-IDF.</li>
<li>Explore how models like <strong>Word2Vec</strong>, <strong>GloVe</strong>, and <strong>FastText</strong> learn embeddings.</li>
<li>Visualize word relationships in vector space.</li>
<li>Use pre-trained embeddings in code and analyze their structure.</li>
<li>Recognize the limitations of static embeddings, and understand why <strong>contextual representations</strong> (like BERT or GPT) are needed.</li>
</ul>
<hr />
<h2 id="chapter-sections"><strong>üìö Chapter Sections</strong></h2>
<hr />
<h3 id="31-introduction-from-counting-to-understanding"><strong>3.1 Introduction: From Counting to Understanding</strong></h3>
<ul>
<li>Reflect on the limitations of Bag-of-Words and TF-IDF: sparsity, lack of meaning.</li>
<li>Introduce the need for dense, semantic representations.</li>
<li>Build the intuitive idea that <strong>words appearing in similar contexts tend to have similar meanings</strong>.</li>
<li>Set the stage for the chapter.</li>
</ul>
<hr />
<h3 id="32-what-are-word-embeddings"><strong>3.2 What Are Word Embeddings?</strong></h3>
<ul>
<li>Introduce <strong>word embeddings</strong> formally and intuitively.</li>
<li>Explain that a word is represented as a <strong>dense vector</strong> of real numbers (e.g., 100‚Äì300 dimensions).</li>
<li>Discuss properties like <strong>semantic similarity</strong>, <strong>vector closeness</strong>, and <strong>continuous representation</strong>.</li>
<li>Use simple analogies (e.g., ‚Äúwords on a map‚Äù) to make it visual and relatable.</li>
</ul>
<hr />
<h3 id="33-how-embeddings-capture-meaning"><strong>3.3 How Embeddings Capture Meaning</strong></h3>
<ul>
<li>Explain how embeddings are learned from large corpora through co-occurrence and context.</li>
<li>Discuss the <strong>distributional hypothesis</strong> (‚ÄúYou shall know a word by the company it keeps‚Äù).</li>
<li>
<p>Show how embedding spaces reflect relationships:</p>
</li>
<li>
<p>Clustering: animals, countries, emotions</p>
</li>
<li>Analogies: <code>king ‚Äì man + woman ‚âà queen</code></li>
<li>Use visualizations (t-SNE or PCA) and diagrams to show word groupings.</li>
</ul>
<hr />
<h3 id="34-word2vec-learning-embeddings-from-context"><strong>3.4 Word2Vec: Learning Embeddings from Context</strong></h3>
<ul>
<li>Introduce <strong>Word2Vec</strong>, its origin (Google, 2013), and significance.</li>
<li>
<p>Explain the <strong>two training strategies</strong>:</p>
</li>
<li>
<p><strong>CBOW</strong>: predict a word from its context</p>
</li>
<li><strong>Skip-gram</strong>: predict the context from a word</li>
<li>Dive into the <strong>architecture and training logic</strong> (without heavy math).</li>
<li>Explain the role of context window, negative sampling, and why Word2Vec is unsupervised.</li>
</ul>
<hr />
<h3 id="35-glove-global-vectors-from-co-occurrence"><strong>3.5 GloVe: Global Vectors from Co-occurrence</strong></h3>
<ul>
<li>Explain the motivation behind GloVe (Stanford, 2014).</li>
<li>
<p>Compare to Word2Vec:</p>
</li>
<li>
<p>Word2Vec: predictive, local</p>
</li>
<li>GloVe: count-based, global</li>
<li>Describe how GloVe uses a <strong>co-occurrence matrix</strong> and learns embeddings by factorizing it.</li>
<li>Show how it balances frequency with meaning.</li>
</ul>
<hr />
<h3 id="36-fasttext-going-beyond-words"><strong>3.6 FastText: Going Beyond Words</strong></h3>
<ul>
<li>Present the key innovation: <strong>subword embeddings</strong> using character n-grams.</li>
<li>
<p>Explain how this helps:</p>
</li>
<li>
<p>Represent rare or misspelled words</p>
</li>
<li>Capture morphological structure (e.g., ‚Äúrun‚Äù, ‚Äúrunning‚Äù, ‚Äúrunner‚Äù)</li>
<li>Discuss FastText‚Äôs use in multilingual and noisy datasets.</li>
</ul>
<hr />
<h3 id="37-working-with-pretrained-embeddings-code-case-study"><strong>3.7 Working with Pretrained Embeddings (Code + Case Study)</strong></h3>
<ul>
<li>Introduce real-world usage of embeddings.</li>
<li>
<p>Use <code>gensim</code> or HuggingFace to:</p>
</li>
<li>
<p>Load pre-trained Word2Vec, GloVe, or FastText models</p>
</li>
<li>Retrieve vectors</li>
<li>Compute similarity</li>
<li>Visualize clusters with PCA or t-SNE</li>
<li>Build a simple classifier (e.g., sentiment analysis) using averaged word vectors.</li>
</ul>
<hr />
<h3 id="38-limitations-of-word-embeddings"><strong>3.8 Limitations of Word Embeddings</strong></h3>
<ul>
<li>
<p>Discuss problems that embeddings don‚Äôt solve:</p>
</li>
<li>
<p><strong>Context insensitivity</strong>: same vector for ‚Äúbank‚Äù in different meanings</p>
</li>
<li><strong>Static nature</strong>: embeddings don‚Äôt change based on sentence</li>
<li><strong>Out-of-vocabulary (OOV)</strong>: unknown words = no vector</li>
<li>Use real examples to show failure cases.</li>
</ul>
<hr />
<h3 id="39-from-static-to-contextual-the-next-step"><strong>3.9 From Static to Contextual: The Next Step</strong></h3>
<ul>
<li>Set up the transition to contextual embeddings (Chapter 4).</li>
<li>
<p>Briefly introduce the idea behind <strong>ELMo</strong>, <strong>BERT</strong>, and <strong>GPT</strong>:</p>
</li>
<li>
<p>Vectors now depend on sentence context</p>
</li>
<li>Meaning is dynamic, not fixed</li>
<li>End with a teaser: <strong>‚ÄúHow does a model know what 'Java' means in different sentences? That‚Äôs the magic of contextual embeddings.‚Äù</strong></li>
</ul>
<hr />
<p>Would you like me to now begin writing <strong>Section 3.1</strong> in the same clear, descriptive, and structured teaching style as <strong>2.3.1 / 2.3.2</strong>?</p>
<p>Great ‚Äî let‚Äôs now write <strong>Section 3.1: Introduction: From Counting to Understanding</strong> in the same descriptive, warm, beginner-friendly style as <strong>Sections 2.3.1 / 2.3.2</strong> from Chapter 2.</p>
<hr />
<h2 id="31-introduction-from-counting-to-understanding_1"><strong>3.1 Introduction: From Counting to Understanding</strong></h2>
<p>In the early stages of working with text data, we focused on turning words into numbers‚Äîa necessary first step in making language accessible to machines. Classical techniques like <strong>Bag-of-Words</strong> and <strong>TF-IDF</strong> helped us do that. They allowed us to convert entire sentences or documents into vectors based on how often certain words appear.</p>
<p>At first, this seems like progress. Machines can now process text numerically, and that‚Äôs useful for tasks like spam detection, topic classification, or keyword matching.</p>
<p>But let‚Äôs pause for a moment and ask:
<strong>Do these models really understand language?</strong></p>
<p>The answer, unfortunately, is no.</p>
<hr />
<h3 id="the-limits-of-counting">üß± The Limits of Counting</h3>
<p>Imagine you have two short sentences:</p>
<ul>
<li>‚ÄúI love sunny beaches.‚Äù</li>
<li>‚ÄúWarm coastlines make me happy.‚Äù</li>
</ul>
<p>If you feed these into a Bag-of-Words model, it will treat each sentence as a collection of individual words. Since ‚Äúbeaches‚Äù and ‚Äúcoastlines‚Äù are different tokens, the model sees no similarity between them‚Äîeven though you and I know they mean nearly the same thing.</p>
<p>TF-IDF adds a layer of intelligence by adjusting scores based on how common or rare a word is across many documents. But even with TF-IDF, the model doesn‚Äôt know that ‚Äúhappy‚Äù and ‚Äúlove‚Äù are both positive emotions, or that ‚Äúsunny‚Äù and ‚Äúwarm‚Äù might be describing similar settings.</p>
<p>In short, these methods <strong>count</strong>, but they don‚Äôt <strong>understand</strong>.</p>
<p>And there are deeper problems too.</p>
<hr />
<h3 id="the-problem-of-sparse-vectors">üßä The Problem of Sparse Vectors</h3>
<p>Classical vector models create huge, high-dimensional vectors‚Äîoften tens or hundreds of thousands of dimensions long‚Äîwhere each dimension corresponds to a word in the vocabulary.</p>
<p>Most of the time, these vectors are <strong>sparse</strong>, meaning they‚Äôre mostly zeros. If a sentence only uses a handful of words from a massive vocabulary, the vector representation will be nearly empty.</p>
<p>Sparse vectors:</p>
<ul>
<li>Waste memory and computing power</li>
<li>Make it hard for machine learning models to generalize</li>
<li>Carry no sense of meaning or structure</li>
</ul>
<p>It‚Äôs like trying to understand someone‚Äôs personality by looking at which words they use, without considering how or why they use them.</p>
<hr />
<h3 id="words-without-context">üé≠ Words Without Context</h3>
<p>Another big issue is <strong>polysemy</strong>‚Äîwhen a single word has multiple meanings.</p>
<p>Consider the word ‚Äúbat‚Äù in these two sentences:</p>
<ul>
<li>‚ÄúHe hit the ball with a bat.‚Äù</li>
<li>‚ÄúThe bat flew out of the cave.‚Äù</li>
</ul>
<p>These are clearly very different uses of the word. But in both TF-IDF and Bag-of-Words, ‚Äúbat‚Äù is just one word‚Äîone vector. There‚Äôs no way for the model to distinguish between a wooden stick and a flying mammal.</p>
<p>That‚Äôs a serious limitation. It means the model has no way of knowing what a word means in a given sentence.</p>
<hr />
<h3 id="the-need-for-better-representations">üå± The Need for Better Representations</h3>
<p>By now, it should be clear:
Classical methods give us <strong>numbers</strong>, but not <strong>meaning</strong>.</p>
<p>We want something better‚Äîsomething that can tell us:</p>
<ul>
<li>That ‚Äúcat‚Äù and ‚Äúdog‚Äù are more similar than ‚Äúcat‚Äù and ‚Äúcarrot‚Äù</li>
<li>That ‚Äúking‚Äù and ‚Äúqueen‚Äù have a relationship that‚Äôs similar to ‚Äúman‚Äù and ‚Äúwoman‚Äù</li>
<li>That ‚Äúbank‚Äù means something different in ‚Äúriver bank‚Äù versus ‚Äúsavings bank‚Äù</li>
</ul>
<p>To do this, we need a new kind of representation‚Äîone that:</p>
<ul>
<li>Is <strong>dense</strong> (uses compact vectors with real information)</li>
<li>Is <strong>learned</strong> from actual usage of words in real text</li>
<li>Captures the <strong>semantic relationships</strong> between words</li>
<li>Reflects <strong>how words are used in context</strong></li>
</ul>
<p>This is where <strong>word embeddings</strong> come in. They offer a way to represent words as <strong>vectors in a multi-dimensional space</strong>, where the distance and direction between vectors actually <strong>mean something</strong>.</p>
<p>Words that occur in similar contexts will have similar vectors. Words that are used in different ways will be positioned far apart. And, as we‚Äôll soon see, even abstract relationships‚Äîlike gender, verb tense, or geographical association‚Äîcan emerge from these vector spaces.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>