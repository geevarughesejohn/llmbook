
## **3.4.6 Strengths and Limitations of Word2Vec**

Word2Vec was a major leap forward in how machines understand languageâ€”but itâ€™s not perfect. Letâ€™s briefly look at what it does well, and where it falls short.

---

### âœ… Strengths

* **Learns from raw text** without needing any labels.
* Produces **dense, low-dimensional embeddings** that capture word meaning.
* Embeddings reflect **semantic relationships** and analogies (e.g., king - man + woman â‰ˆ queen).
* **Fast and efficient**â€”especially with Negative Sampling.
* Outperforms traditional methods like TF-IDF in tasks like similarity, clustering, and classification.

---

### âš ï¸ Limitations

* **Ignores word order** in CBOW (and to some extent in Skip-Gram).
* **Context is fixed**â€”each word has one vector, regardless of meaning.

  * For example, â€œbankâ€ (river bank vs. money bank) is treated the same.
* Doesnâ€™t handle **out-of-vocabulary words** unless retrained.
* Struggles with **morphological variations** (e.g., â€œrunâ€, â€œrunningâ€, â€œranâ€).

---

### ğŸ”„ Why This Matters

Word2Vec was a **foundational step** in the evolution of NLP. It proved that:

* Language meaning could be **learned, not programmed**.
* Words could be represented as **vectors in space**.
* And that these vectors could power better downstream models.

However, its limitations opened the door to improvements:

* **FastText** (adds subword information to handle morphology)
* **Contextual embeddings** like **ELMo** and **BERT**, which generate different vectors depending on usage

Weâ€™ll explore these advanced methods in later chapters.


