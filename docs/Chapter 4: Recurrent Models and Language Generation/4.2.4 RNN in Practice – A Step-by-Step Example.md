
## **4.2.4 RNN in Practice – A Step-by-Step Example**

Let’s now solidify everything we’ve learned so far by walking through a working example. First, we’ll see **how the hidden states evolve** as we process a sentence. Then, we’ll implement a **basic RNN from scratch in Python**, without using any deep learning frameworks.

---

### 📚 Example: “I love cats”

We’ll process the sentence “I love cats” using an RNN.

Assume the following:

* Our vocabulary is: `["I", "love", "cats"]`
* We assign one-hot vectors:

  * `I     → [1, 0, 0]`
  * `love  → [0, 1, 0]`
  * `cats → [0, 0, 1]`
* Input size = 3 (because of 3 unique words)
* Hidden size = 2 (we’ll keep this small for simplicity)

---

### 🧮 RNN Equations

At each time step $t$, the RNN updates the hidden state using:

$$
h_t = \tanh(Wx_t + Uh_{t-1} + b)
$$

Where:

* $x_t$ is the current input vector
* $h_{t-1}$ is the previous hidden state
* $W$, $U$, and $b$ are trainable parameters

We’ll initialize everything manually and walk through the computation.

---

### 💻 Python Implementation (from Scratch)

```python
import numpy as np

# ---- Step 1: Define the vocabulary ----
vocab = ["I", "love", "cats"]
word_to_ix = {word: i for i, word in enumerate(vocab)}

# ---- Step 2: One-hot encoding ----
def one_hot(idx, size):
    vec = np.zeros(size)
    vec[idx] = 1.0
    return vec

# ---- Step 3: Initialize weights and hidden state ----
input_size = 3     # one-hot size
hidden_size = 2    # size of the RNN memory

# Weights (random small numbers)
W = np.random.randn(hidden_size, input_size) * 0.1
U = np.random.randn(hidden_size, hidden_size) * 0.1
b = np.zeros(hidden_size)

# Initial hidden state
h_prev = np.zeros(hidden_size)

# ---- Step 4: Input sentence ----
sentence = ["I", "love", "cats"]

print("Step-by-step hidden states:\n")

for word in sentence:
    x = one_hot(word_to_ix[word], input_size)  # input vector
    h_t = np.tanh(np.dot(W, x) + np.dot(U, h_prev) + b)  # RNN update
    print(f"Word: {word:5}  | Hidden state: {h_t}")
    h_prev = h_t  # carry hidden state forward
```

---

### 🧠 What’s Happening?

At each step:

* The RNN reads the current word vector
* It combines that with the memory (`h_prev`)
* The result is a **new hidden state** that reflects the sentence so far

By the end of the sentence, the final hidden state contains a summary of:

> “I love cats”

If we were training a language model, the next step would be to use this hidden state to **predict the next word**. But even without prediction, you can see how the RNN builds memory step-by-step.

---

### 🔍 Why This Matters

This kind of “manual” RNN helps demystify what’s often treated as a black box. You can see clearly how:

* Each word updates the memory
* The model keeps a continuous state across time
* Everything is differentiable and trainable

But… we also start to see some weaknesses.

Even in this tiny example, what happens if we add five more words before “cats”? Will the memory still be strong enough to understand the context? Probably not.

That’s the core weakness of simple RNNs—they **struggle with long-term memory**. And that’s exactly where we’re headed next.

In the following section, we’ll explore the **vanishing gradient problem**, a major limitation of vanilla RNNs, and why it led to the invention of more powerful units like **LSTM** and **GRU**.

