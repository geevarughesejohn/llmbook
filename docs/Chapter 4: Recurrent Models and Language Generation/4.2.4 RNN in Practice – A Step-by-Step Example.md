
## **4.2.4 RNN in Practice â€“ A Step-by-Step Example**

Letâ€™s now solidify everything weâ€™ve learned so far by walking through a working example. First, weâ€™ll see **how the hidden states evolve** as we process a sentence. Then, weâ€™ll implement a **basic RNN from scratch in Python**, without using any deep learning frameworks.

---

### ğŸ“š Example: â€œI love catsâ€

Weâ€™ll process the sentence â€œI love catsâ€ using an RNN.

Assume the following:

* Our vocabulary is: `["I", "love", "cats"]`
* We assign one-hot vectors:

  * `I     â†’ [1, 0, 0]`
  * `love  â†’ [0, 1, 0]`
  * `cats â†’ [0, 0, 1]`
* Input size = 3 (because of 3 unique words)
* Hidden size = 2 (weâ€™ll keep this small for simplicity)

---

### ğŸ§® RNN Equations

At each time step $t$, the RNN updates the hidden state using:

$$
h_t = \tanh(Wx_t + Uh_{t-1} + b)
$$

Where:

* $x_t$ is the current input vector
* $h_{t-1}$ is the previous hidden state
* $W$, $U$, and $b$ are trainable parameters

Weâ€™ll initialize everything manually and walk through the computation.

---

### ğŸ’» Python Implementation (from Scratch)

```python
import numpy as np

# ---- Step 1: Define the vocabulary ----
vocab = ["I", "love", "cats"]
word_to_ix = {word: i for i, word in enumerate(vocab)}

# ---- Step 2: One-hot encoding ----
def one_hot(idx, size):
    vec = np.zeros(size)
    vec[idx] = 1.0
    return vec

# ---- Step 3: Initialize weights and hidden state ----
input_size = 3     # one-hot size
hidden_size = 2    # size of the RNN memory

# Weights (random small numbers)
W = np.random.randn(hidden_size, input_size) * 0.1
U = np.random.randn(hidden_size, hidden_size) * 0.1
b = np.zeros(hidden_size)

# Initial hidden state
h_prev = np.zeros(hidden_size)

# ---- Step 4: Input sentence ----
sentence = ["I", "love", "cats"]

print("Step-by-step hidden states:\n")

for word in sentence:
    x = one_hot(word_to_ix[word], input_size)  # input vector
    h_t = np.tanh(np.dot(W, x) + np.dot(U, h_prev) + b)  # RNN update
    print(f"Word: {word:5}  | Hidden state: {h_t}")
    h_prev = h_t  # carry hidden state forward
```

---

### ğŸ§  Whatâ€™s Happening?

At each step:

* The RNN reads the current word vector
* It combines that with the memory (`h_prev`)
* The result is a **new hidden state** that reflects the sentence so far

By the end of the sentence, the final hidden state contains a summary of:

> â€œI love catsâ€

If we were training a language model, the next step would be to use this hidden state to **predict the next word**. But even without prediction, you can see how the RNN builds memory step-by-step.

---

### ğŸ” Why This Matters

This kind of â€œmanualâ€ RNN helps demystify whatâ€™s often treated as a black box. You can see clearly how:

* Each word updates the memory
* The model keeps a continuous state across time
* Everything is differentiable and trainable

Butâ€¦ we also start to see some weaknesses.

Even in this tiny example, what happens if we add five more words before â€œcatsâ€? Will the memory still be strong enough to understand the context? Probably not.

Thatâ€™s the core weakness of simple RNNsâ€”they **struggle with long-term memory**. And thatâ€™s exactly where weâ€™re headed next.

In the following section, weâ€™ll explore the **vanishing gradient problem**, a major limitation of vanilla RNNs, and why it led to the invention of more powerful units like **LSTM** and **GRU**.

