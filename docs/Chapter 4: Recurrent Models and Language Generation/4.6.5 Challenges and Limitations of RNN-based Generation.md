
## **4.6.5 Challenges and Limitations of RNN-based Generation**

*Why vanilla sequence models struggle to scale*

By now, weâ€™ve seen how Recurrent Neural Networksâ€”especially with GRUs and LSTMsâ€”can learn to generate text, word by word or character by character. They can write short poems, answer questions, and even mimic basic sentence structure.

But as promising as RNN-based generation may seem, it has some major limitationsâ€”especially when the sequences get long, the context gets rich, or the logic becomes complex.

Letâ€™s explore why RNNs often fall short.

---

### â³ 1. Struggles with Long-Term Dependencies

RNNs process sequences **one token at a time**, passing memory from one step to the next. While GRUs and LSTMs improved the ability to **remember past words**, thereâ€™s still a limit.

For example:

> "The dog that chased the cat that ran across the garden barked."

To correctly interpret or generate such a sentence, the model must remember **what subject itâ€™s dealing with** several steps later. RNNs can lose this thread over long distances, especially if the sentence is complex or contains multiple nested clauses.

Even LSTMs, which are designed to combat this issue, start to lose **fidelity** as sequences grow longer.

---

### ğŸ” 2. Sequential Bottleneck

RNNs operate **strictly in sequence**â€”one time step must be processed before the next. This means:

* They **cannot be parallelized** easily on GPUs
* They take longer to train and to generate output
* The model becomes less efficient for very long sequences

In contrast, newer models like Transformers can look at all positions **simultaneously**, making them far more efficient.

---

### ğŸ§ª 3. Exposure Bias

During training, we often use **teacher forcing**: we feed the model the correct word from the training set.
But during generation, the model must feed itself its own predictionsâ€”mistakes and all.

This leads to a problem called **exposure bias**:

> One bad prediction early on â†’ weird context â†’ worse predictions â†’ gibberish

Since the model is rarely trained to handle its own errors, they **snowball during generation**.

---

### ğŸŒ€ 4. Repetition and Lack of Coherence

RNN-generated text often suffers from:

* **Repetitive phrases** ("I am a robot. I am a robot...")
* **Short-sightedness** (canâ€™t plan a full sentence or story)
* **Drift in meaning** (starts on one topic, ends somewhere else)

This is partly due to limited memory and the modelâ€™s inability to reason globally about the full sequence.

---

### âŒ 5. No Real Attention

RNNs treat each past word equally (or rely only on hidden states). They donâ€™t have a way to say:

> â€œThis specific past word is especially important right now.â€

They lack **attention**â€”a mechanism that helps later models like Transformers **focus on the most relevant parts** of a sequence.

---

### ğŸ§© Summary

While RNNs (and their gated cousins like LSTMs and GRUs) have been foundational in NLP, they face serious limitations:

* Hard to scale to long or complex text
* Slow and sequential in nature
* Prone to error accumulation
* Struggle to retain and use precise context

These limitations paved the way for a new idea:
**What if a model could look at the entire sentenceâ€”or even the entire paragraphâ€”all at once?**

In the next chapter, weâ€™ll step into the world of **attention** and **contextual embeddings**â€”the powerful innovations that made Transformers, BERT, and GPT possible.


