
## **4.4.3 How LSTMs Process Sequences**

Now that we‚Äôve seen the internal machinery of an LSTM cell‚Äîgates, memory, and all‚Äîit‚Äôs time to see it in action. What happens when you give an LSTM a sentence, one word at a time?

The beauty of LSTMs is not just in how they process a single word‚Äîbut in how they **carry memory across time steps**, carefully updating and retaining knowledge as needed.

Let‚Äôs walk through a simple example and trace the flow of information.

---

### üß™ Example: ‚ÄúThe dog barked loudly‚Äù

Suppose we want our model to process the sentence:

> ‚ÄúThe dog barked loudly‚Äù

We feed the words into the LSTM one at a time. At each step, the LSTM takes in:

* The current input word (converted to a vector $x_t$)
* The previous **hidden state** $h_{t-1}$
* The previous **cell state** $c_{t-1}$

It returns:

* A new hidden state $h_t$
* A new cell state $c_t$

Let‚Äôs visualize the timeline:

```
Time Step:     t=1        t=2        t=3         t=4
Input:        "The"      "dog"     "barked"    "loudly"
             ------     ------     -------     -------
Cell:        c‚ÇÅ         c‚ÇÇ         c‚ÇÉ          c‚ÇÑ
Hidden:      h‚ÇÅ         h‚ÇÇ         h‚ÇÉ          h‚ÇÑ
```

Each LSTM cell receives updated memory from the previous cell and decides:

* Should I keep the old memory?
* Should I overwrite it with new info?
* What part of it should I share outward?

This continuous flow allows it to **remember context** that spans across several steps.

---

### üìã Step-by-Step Flow (Conceptually)

1. **Input: "The" (t = 1)**

   * There's no prior context, so the model starts fresh.
   * The input and forget gates decide what basic sentence structure info to store.

2. **Input: "dog" (t = 2)**

   * The model learns this is likely a **subject** noun.
   * This word updates the memory to reflect *who* the sentence is about.

3. **Input: "barked" (t = 3)**

   * A verb appears.
   * The LSTM now understands the **subject is acting** and updates memory accordingly.

4. **Input: "loudly" (t = 4)**

   * An adverb adds detail to the action.
   * The LSTM might store this as modifying the verb ‚Äúbarked‚Äù and start preparing for the sentence end.

---

### üß† Why This Matters

At each point, the model‚Äôs **hidden state** reflects not just the current word, but also **everything it has learned so far**. That‚Äôs crucial for tasks like:

* **Next word prediction**: The LSTM uses its memory to guess what word might come next.
* **Translation**: The full sequence is processed before generating the translated version.
* **Sentiment analysis**: Early words like ‚Äúnot‚Äù can flip the meaning of words like ‚Äúgood‚Äù much later in the sentence.

---

### üßÆ Output at Each Step (Optional)

Some tasks require output **after each word** (e.g., POS tagging), while others only need output **at the end** (e.g., sentiment classification).

You can choose how to use the hidden states:

* Use all $h_1, h_2, h_3, h_4$
* Or just the final hidden state $h_4$

This flexibility is one of the reasons LSTMs became so widely adopted.

---

### üß© Summary

* LSTMs process sequences step by step, maintaining both short- and long-term memory.
* Gates control how memory is updated and used.
* The final hidden state contains a rich, context-aware summary of the entire sentence so far.

Next, we‚Äôll explore when and why LSTMs are used in real-world NLP tasks‚Äîand compare them to other models like GRUs.

Shall we move on to **Section 4.4.4: When and Why to Use LSTMs**?
