

## **4.3 The Problem of Vanishing Gradients**

At this point, we've seen how Recurrent Neural Networks (RNNs) can pass information from one step to the next through a hidden state. This hidden state acts like a memory that gets updated every time a new word is read.

But here’s the problem: RNNs have a *short memory*. They might do well on short phrases like:

> “The cat sat on the mat.”

But they struggle with longer dependencies, like:

> “The cat, which had been missing for several days and was finally found near the old barn by the river, sat on the mat.”

The model might completely forget about *“the cat”* by the time it reaches *“sat on the mat”*.

Why does this happen? To understand that, we need to take a peek under the hood and talk about one of the most important training challenges in deep learning: the **vanishing gradient problem**.

---

### 🧠 A Quick Refresher: What Is a Gradient?

When training a neural network, we use a method called **backpropagation** to update its weights. This involves calculating a gradient—a kind of signal that tells each weight how much it contributed to the error, and how it should change to improve.

If the gradient is large, the weight gets updated significantly. If it’s small, the update is tiny.

This works great in short networks. But in deep networks—or in **long unrolled RNNs**—a single mistake at the end of the sequence has to travel **backward** through many time steps to adjust earlier weights.

---

### 🧯 The Vanishing Gradient Problem

As this gradient flows backward through each step of the RNN, it often gets multiplied by small numbers (less than 1). After 10 or 20 steps, the gradient becomes so small that it’s almost zero.

> It “vanishes.”

This means the early parts of the sequence stop receiving useful updates during training. The model can’t “learn” to remember what happened a long time ago.

Imagine trying to pass a message down a line of 50 people by whispering it ear to ear. By the time it reaches the end, the message is barely recognizable. That’s what happens to the gradient during backpropagation through time.

---

### 💥 The Exploding Gradient Problem

There's a twin problem called the **exploding gradient**—when the gradient becomes too large instead of too small. This causes unstable updates and can crash the training.

While exploding gradients can be controlled using techniques like **gradient clipping**, vanishing gradients are harder to fix.

---

### 😕 Why Vanilla RNNs Struggle

The recurrence formula of RNNs uses a **nonlinear activation function** like `tanh` or `sigmoid`. These functions squash outputs into a limited range, which contributes to shrinking gradients over time.

So while RNNs can theoretically model long-term dependencies, in practice, they **rarely** do unless the sequence is very short or the pattern is very strong.

This limitation sparked a breakthrough in neural network design—architectures that could **learn to remember** more effectively, by controlling what to store and what to forget.

---

### 💡 Enter LSTM and GRU

To overcome these problems, researchers developed new types of RNN cells—most notably the **Long Short-Term Memory (LSTM)** and the **Gated Recurrent Unit (GRU)**.

These models introduced **gates** that decide:

* What new information to add
* What old information to forget
* What to keep and carry forward

By doing this, they solve the vanishing gradient problem and allow the network to capture **long-range dependencies** in text.

In the next section, we’ll explore the **LSTM** in detail: how it works, what it looks like inside, and how it enables machines to hold on to important information for longer periods.

