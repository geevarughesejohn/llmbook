
## **4.6.2 Training a Recurrent Language Model**

*Teaching machines to predict the next word*

Now that we understand the goal of language generation—predicting the next word in a sequence—let’s explore how we train a model to do exactly that.

Training a recurrent language model is like teaching a child to write by filling in blanks: show them a sentence, hide the last word, and ask them to guess what comes next. If they're wrong, they adjust based on feedback. Over time, they get better at spotting patterns.

Machines learn the same way—just faster, and with lots of data.

---

### 🧾 Step 1: Preparing the Dataset

To train a language model, we first need a **text corpus**—a large collection of sentences. This is broken into sequences like so:

> Input: “The cat sat on the”
> Target: “cat sat on the mat”

Each training example is made of:

* An **input sequence** $(w_1, w_2, ..., w_{t-1})$
* A **target sequence** $(w_2, w_3, ..., w_t)$

This way, the model learns to **predict the next word** at every time step.

We also tokenize the text into integers or word embeddings (like Word2Vec or GloVe) before feeding it into the model.

---

### 🔁 Step 2: Feeding Sequences into the Model

We pass the input sequence word by word through the recurrent model—RNN, GRU, or LSTM. At each time step $t$, the model computes a hidden state $h_t$, which captures all the information seen so far.

Then it uses this hidden state to predict a probability distribution over the next word using a **softmax layer**.

$$
P(w_t | w_1, ..., w_{t-1}) = \text{softmax}(W \cdot h_t + b)
$$

---

### 🎯 Step 3: Comparing Predictions with Reality

The model's prediction is compared to the **actual next word** using a **loss function**—typically **categorical cross-entropy**, which penalizes incorrect guesses and rewards correct ones.

$$
\mathcal{L} = -\sum_{t} \log P_{\text{model}}(w_t)
$$

If the predicted word was "dog" but the correct word was "mat", the loss will be high. The model then adjusts its weights using **backpropagation through time (BPTT)**.

---

### 📚 Teacher Forcing: Helping the Model Learn Faster

One trick used during training is called **teacher forcing**.

Instead of feeding the model’s **own prediction** as the next input (which may be wrong), we feed the **actual word** from the training data.

> Example:
> True input: “The cat sat”
> True target: “cat sat on”
>
> Even if the model guesses “The cat ran”, we still feed “sat” as the next word, not “ran”.

This makes learning faster and more stable.

**Why it works:**

* Prevents early errors from derailing the sequence
* Gives a clearer training signal for what should have come next

**Drawback:**
At inference time, teacher forcing isn’t available—you have to generate word-by-word using the model’s **own** predictions. This mismatch can sometimes hurt performance.

---

### 🧩 Summary

* Training a language model involves feeding sequences and predicting the next word at each step.
* Cross-entropy loss is used to measure how well the model matches the actual text.
* **Teacher forcing** accelerates training by using the true previous word instead of the model’s guess.
* Once trained, the model can be used to generate text, one word at a time.

In the next section, we’ll explore **how inference works**: how to generate new sentences without ground-truth labels, using techniques like sampling and temperature control.

