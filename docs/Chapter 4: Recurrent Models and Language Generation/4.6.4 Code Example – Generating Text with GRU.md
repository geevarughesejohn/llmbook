

## **4.6.4 Code Example ‚Äì Generating Text with GRU**

*A small neural storyteller in action*

Let‚Äôs put theory into practice! In this section, we‚Äôll build a simple character-level text generator using a **GRU** in **PyTorch**. The model will be trained on a small string corpus and will learn to generate new sequences one character at a time.

This example won‚Äôt create Shakespeare, but it will give you hands-on experience with:

* Preparing sequential data
* Building a GRU-based model
* Training it to predict the next character
* Generating new text at inference time

---

### üîß Step 1: Import Libraries and Prepare the Data

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
```

```python
# Sample training data (you can use any text)
text = "hello world hello world"

# Create character vocabulary
chars = sorted(set(text))
char2idx = {ch: idx for idx, ch in enumerate(chars)}
idx2char = {idx: ch for ch, idx in char2idx.items()}

vocab_size = len(chars)

# Convert full text into a list of indices
data = [char2idx[c] for c in text]

# Hyperparameters
seq_length = 5
hidden_size = 32
embedding_dim = 16
lr = 0.01
epochs = 200
```

---

### üì¶ Step 2: Define the GRU Language Model

```python
class CharGRU(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, x, hidden=None):
        x = self.embedding(x)
        out, hidden = self.gru(x, hidden)
        logits = self.fc(out)
        return logits, hidden
```

---

### üîÅ Step 3: Prepare Batches

```python
def get_batch(data, seq_length):
    # Randomly pick a start index
    start = random.randint(0, len(data) - seq_length - 1)
    seq = data[start : start + seq_length]
    target = data[start + 1 : start + seq_length + 1]
    return torch.tensor(seq).unsqueeze(0), torch.tensor(target).unsqueeze(0)
```

---

### üß† Step 4: Train the Model

```python
model = CharGRU(vocab_size, embedding_dim, hidden_size)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

for epoch in range(1, epochs + 1):
    model.train()
    x_batch, y_batch = get_batch(data, seq_length)
    logits, _ = model(x_batch)
    loss = loss_fn(logits.view(-1, vocab_size), y_batch.view(-1))
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
```

---

### ‚úçÔ∏è Step 5: Generate Text from the Trained Model

```python
def generate_text(model, start_text, gen_length=50, temperature=1.0):
    model.eval()
    input_seq = torch.tensor([char2idx[ch] for ch in start_text]).unsqueeze(0)
    hidden = None
    output_text = start_text

    for _ in range(gen_length):
        logits, hidden = model(input_seq, hidden)
        logits = logits[:, -1, :] / temperature  # Get last time step
        probs = torch.softmax(logits, dim=-1)
        next_idx = torch.multinomial(probs, num_samples=1).item()
        next_char = idx2char[next_idx]
        output_text += next_char
        input_seq = torch.tensor([[next_idx]])
    
    return output_text
```

```python
# Try generating text
print(generate_text(model, start_text="hell", gen_length=40))
```

---

### üß© Summary

In this code example, we:

* Created a GRU-based character model using PyTorch
* Trained it to predict the next character in a simple sequence
* Used it to generate new text, character by character
* Saw how temperature controls creativity during sampling

This example is small by design‚Äîbut the same structure applies when training on larger datasets or switching from characters to words.

Next, we‚Äôll look at the **limitations** of RNN-based generation models‚Äîand how attention and transformers came to improve upon them.


