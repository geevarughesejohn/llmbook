## **4.6.3 Sampling and Generation During Inference**

*Letting the model speak for itself*

Training a language model is like teaching a student to complete sentences. But what happens when we let that student speak freely—without telling them the next word?

This is where **inference** comes in. During training, the model had access to the correct next word (via teacher forcing). But at inference time, we remove the training wheels. The model must **predict the next word on its own**, then use that prediction to guess the one after that… and so on.

---

### 🔁 How Generation Works at Inference

Let’s say we give the model a prompt like:

> “Once upon a time”

Here’s what happens next:

1. Feed this sequence into the model.
2. The model outputs a probability distribution over the vocabulary.
3. We **choose the next word** based on that distribution.
4. Append the chosen word to the input.
5. Repeat the process until we reach a stop condition (e.g., max length, special token, or end punctuation).

This process is called **autoregressive generation**—the model generates one word at a time, using its own outputs to continue the sequence.

---

### 🎲 Sampling Strategies: How to Pick the Next Word

The model gives us **a list of possible next words**, each with a probability. But how do we decide which word to choose?

There are several common strategies:

---

#### 🔹 **Greedy Sampling**

Always pick the word with the highest probability.

* ✅ Simple and fast
* ❌ Can lead to dull or repetitive text

  > "I am a robot. I am a robot. I am a robot."

---

#### 🔹 **Random Sampling**

Pick the next word randomly based on the predicted probability distribution.

* ✅ More creative and diverse
* ❌ Can produce incoherent or nonsensical results

---

#### 🔹 **Top-k Sampling**

Only consider the top **k** most likely words, then pick one randomly.

* ✅ Controls randomness while keeping diversity
* Example: if `k=5`, sample from top 5 predicted words

---

#### 🔹 **Top-p Sampling (Nucleus Sampling)**

Choose the **smallest set of words** whose combined probability exceeds **p** (like 0.9), then sample from that.

* ✅ More flexible than top-k
* Adaptively adjusts based on the distribution’s confidence

---

#### 🔹 **Temperature**

This controls the **creativity** of the sampling:

* High temperature (e.g. 1.5) → more randomness
* Low temperature (e.g. 0.3) → more confident, conservative choices
* Temperature of **1.0** is neutral

Temperature modifies the probability distribution before sampling:

$$
P_i^{\text{adjusted}} = \frac{e^{\log P_i / T}}{\sum_j e^{\log P_j / T}}
$$

Where $T$ is the temperature.

---

### 🧾 Example Walkthrough

Let’s say the model is given:

> “The weather today is”

And it predicts the following probabilities:

* sunny: 0.40
* cloudy: 0.35
* amazing: 0.10
* table: 0.01

**Greedy:** → “sunny”
**Top-k (k=2):** → sample between “sunny” and “cloudy”
**Top-p (p=0.8):** → “sunny”, “cloudy”, “amazing” included
**High temperature:** → might choose “amazing”
**Low temperature:** → probably chooses “sunny”

---

### 📌 Ending the Generation

You usually stop generating when:

* A **special end token** (like `<eos>`) is predicted
* A **maximum length** is reached (e.g., 50 tokens)
* The model generates a period or full stop

This ensures the output doesn’t go on forever.

---

### 🧩 Summary

* Inference is where the model finally generates text on its own.
* Each new word is sampled based on a probability distribution.
* Sampling strategies (greedy, top-k, temperature) control creativity vs. coherence.
* Proper tuning of sampling parameters can make the difference between boring and brilliant outputs.

In the next section, we’ll put this into practice with a working code example that trains a small GRU model and generates sentences.

