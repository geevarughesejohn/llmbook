## **4.6.3 Sampling and Generation During Inference**

*Letting the model speak for itself*

Training a language model is like teaching a student to complete sentences. But what happens when we let that student speak freelyâ€”without telling them the next word?

This is where **inference** comes in. During training, the model had access to the correct next word (via teacher forcing). But at inference time, we remove the training wheels. The model must **predict the next word on its own**, then use that prediction to guess the one after thatâ€¦ and so on.

---

### ğŸ” How Generation Works at Inference

Letâ€™s say we give the model a prompt like:

> â€œOnce upon a timeâ€

Hereâ€™s what happens next:

1. Feed this sequence into the model.
2. The model outputs a probability distribution over the vocabulary.
3. We **choose the next word** based on that distribution.
4. Append the chosen word to the input.
5. Repeat the process until we reach a stop condition (e.g., max length, special token, or end punctuation).

This process is called **autoregressive generation**â€”the model generates one word at a time, using its own outputs to continue the sequence.

---

### ğŸ² Sampling Strategies: How to Pick the Next Word

The model gives us **a list of possible next words**, each with a probability. But how do we decide which word to choose?

There are several common strategies:

---

#### ğŸ”¹ **Greedy Sampling**

Always pick the word with the highest probability.

* âœ… Simple and fast
* âŒ Can lead to dull or repetitive text

  > "I am a robot. I am a robot. I am a robot."

---

#### ğŸ”¹ **Random Sampling**

Pick the next word randomly based on the predicted probability distribution.

* âœ… More creative and diverse
* âŒ Can produce incoherent or nonsensical results

---

#### ğŸ”¹ **Top-k Sampling**

Only consider the top **k** most likely words, then pick one randomly.

* âœ… Controls randomness while keeping diversity
* Example: if `k=5`, sample from top 5 predicted words

---

#### ğŸ”¹ **Top-p Sampling (Nucleus Sampling)**

Choose the **smallest set of words** whose combined probability exceeds **p** (like 0.9), then sample from that.

* âœ… More flexible than top-k
* Adaptively adjusts based on the distributionâ€™s confidence

---

#### ğŸ”¹ **Temperature**

This controls the **creativity** of the sampling:

* High temperature (e.g. 1.5) â†’ more randomness
* Low temperature (e.g. 0.3) â†’ more confident, conservative choices
* Temperature of **1.0** is neutral

Temperature modifies the probability distribution before sampling:

$$
P_i^{\text{adjusted}} = \frac{e^{\log P_i / T}}{\sum_j e^{\log P_j / T}}
$$

Where $T$ is the temperature.

---

### ğŸ§¾ Example Walkthrough

Letâ€™s say the model is given:

> â€œThe weather today isâ€

And it predicts the following probabilities:

* sunny: 0.40
* cloudy: 0.35
* amazing: 0.10
* table: 0.01

**Greedy:** â†’ â€œsunnyâ€
**Top-k (k=2):** â†’ sample between â€œsunnyâ€ and â€œcloudyâ€
**Top-p (p=0.8):** â†’ â€œsunnyâ€, â€œcloudyâ€, â€œamazingâ€ included
**High temperature:** â†’ might choose â€œamazingâ€
**Low temperature:** â†’ probably chooses â€œsunnyâ€

---

### ğŸ“Œ Ending the Generation

You usually stop generating when:

* A **special end token** (like `<eos>`) is predicted
* A **maximum length** is reached (e.g., 50 tokens)
* The model generates a period or full stop

This ensures the output doesnâ€™t go on forever.

---

### ğŸ§© Summary

* Inference is where the model finally generates text on its own.
* Each new word is sampled based on a probability distribution.
* Sampling strategies (greedy, top-k, temperature) control creativity vs. coherence.
* Proper tuning of sampling parameters can make the difference between boring and brilliant outputs.

In the next section, weâ€™ll put this into practice with a working code example that trains a small GRU model and generates sentences.

