
## **2.3.2 Term Frequency–Inverse Document Frequency (TF-IDF)**

---

### 🧠 Why Not Just Count Words?

The Bag-of-Words model is a great starting point. It’s simple, fast, and often surprisingly effective. But as you saw earlier, it treats **all words as equally important**—and that’s a problem.

Imagine you’re building a document classifier, and every document starts with the phrase:

> “This document is about…”

The words *“this”*, *“is”*, and *“about”* will show up in almost every document. As a result, they’ll have high counts—but they’re not really helpful in telling documents apart.

What we want is a way to **give more importance to informative words** and **less importance to common, boring ones**.

That’s exactly what **TF-IDF** does.

---

### 📐 What is TF-IDF?

TF-IDF stands for:

* **Term Frequency (TF)** – How often a word appears in a document
* **Inverse Document Frequency (IDF)** – How rare the word is across all documents

By combining these two ideas, TF-IDF gives higher scores to **important words** (frequent in one document, rare in others), and lower scores to **common words** (frequent everywhere).

---

### 🔢 Let’s Break It Down

#### ✅ Term Frequency (TF)

This measures how often a word shows up in a document. The more frequent, the more important—*within that document*.

$$
\text{TF}(w, d) = \frac{\text{Number of times } w \text{ appears in } d}{\text{Total number of words in } d}
$$

So if “machine” appears **3 times** in a 100-word document, its TF is `3 / 100 = 0.03`.

---

#### ✅ Inverse Document Frequency (IDF)

This measures how **rare** a word is across all documents in the dataset. The fewer documents it appears in, the higher its score.

$$
\text{IDF}(w) = \log\left(\frac{N}{1 + n_w}\right)
$$

Where:

* $N$ is the total number of documents
* $n_w$ is the number of documents containing the word $w$
* Adding 1 prevents division by zero

So if “learning” appears in **2 out of 10 documents**, its IDF is:

$$
\log\left(\frac{10}{1 + 2}\right) = \log\left(\frac{10}{3}\right) \approx 0.52
$$

> 🔍 Common words like “is” or “the” will have **very low IDF** scores. That’s the magic of TF-IDF—it automatically **downweights** them.

---

#### ✅ TF-IDF = TF × IDF

To get the final score for a word in a document, multiply its **TF** and **IDF**.

$$
\text{TF-IDF}(w, d) = \text{TF}(w, d) \times \text{IDF}(w)
$$

This score is high when:

* The word is **frequent in one document**, and
* **Rare in the overall dataset**

---

### 🔍 Mini Example

Let’s use the same three documents again:

```text
Doc 1: "I love NLP"
Doc 2: "I love machine learning"
Doc 3: "NLP is fun"
```

#### Step 1: Count Term Frequencies

* “love” appears in Doc 1 and Doc 2: TF = 1 / 3 = 0.33 (for each)
* “machine” appears only in Doc 2: TF = 1 / 4 = 0.25

#### Step 2: Compute IDF

* Total documents = 3
* “love” appears in 2 docs → IDF = log(3 / (1 + 2)) = log(1) = 0.0
* “machine” appears in 1 doc → IDF = log(3 / (1 + 1)) = log(1.5) ≈ 0.405

#### Step 3: Final TF-IDF Scores

* “love” → 0.33 × 0.0 = 0.0 ✅ *common word, downweighted*
* “machine” → 0.25 × 0.405 ≈ 0.101 ✅ *rare, boosted*

> 💡 So TF-IDF gives **high scores to unique, meaningful words**—and **low scores to generic, frequent words**.

---

### 🧑‍💻 Code Example: TF-IDF in Python

Let’s implement TF-IDF using `TfidfVectorizer` from Scikit-learn:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

docs = [
    "I love NLP",
    "I love machine learning",
    "NLP is fun"
]

# Create and fit the TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(docs)

# Convert to readable format
import pandas as pd
df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
print(df)
```

#### 📤 Output

|       | fun   | is    | learning | love  | machine | nlp   |
| ----- | ----- | ----- | -------- | ----- | ------- | ----- |
| Doc 1 | 0     | 0     | 0        | 0.707 | 0       | 0.707 |
| Doc 2 | 0     | 0     | 0.577    | 0.577 | 0.577   | 0     |
| Doc 3 | 0.577 | 0.577 | 0        | 0     | 0       | 0.577 |

Notice:

* Words like **“love”** appear in multiple documents → **lower scores**
* Words like **“fun”**, **“machine”**, and **“learning”** are more specific → **higher scores**

---

### ⚖️ Comparison with Bag-of-Words

| Feature                | Bag-of-Words     | TF-IDF                     |
| ---------------------- | ---------------- | -------------------------- |
| Uses raw word counts   | ✅ Yes            | ❌ No                       |
| Weights rare terms     | ❌ No             | ✅ Yes                      |
| Penalizes common terms | ❌ No             | ✅ Yes                      |
| Captures meaning       | ❌ No             | ❌ Still limited            |
| Output vectors         | Sparse, high-dim | Sparse, weighted, high-dim |

---

### 🎯 When to Use TF-IDF

Use TF-IDF when:

* You want to highlight unique or rare words in your text
* You’re doing tasks like **document classification**, **search engines**, or **information retrieval**
* You want to improve performance over basic BoW without jumping into deep learning


