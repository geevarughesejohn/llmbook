
## **2.1 Introduction: What is Classical NLP?**

Before we dive into the world of deep learning and giant language models like GPT, it’s worth taking a step back and asking: *How did we get here?* How did computers handle language before neural networks became popular?

The answer lies in a field known as **classical Natural Language Processing (NLP)**—a collection of techniques developed over decades, which gave computers the first taste of understanding human language.

These methods didn’t rely on huge models or vast amounts of data. Instead, they used clever **rules**, **basic statistics**, and a fair amount of **linguistic knowledge** to process and analyze text. In fact, many tasks that we now perform with deep learning were once handled quite well using these older approaches. Detecting spam emails, tagging parts of speech in a sentence, or even doing simple sentiment analysis—*all of this was possible without transformers or GPUs.*

---

### Why Should You Learn Classical NLP?

You might be wondering: *If modern language models can do all of this and more, why should we bother with the old ways?*

That’s a great question—and one that many newcomers to the field ask. The truth is, **classical NLP is still incredibly valuable** for a number of reasons:

* **It teaches the basics.** Before we can appreciate the sophistication of large models, we need to understand the simple tools they build upon—things like breaking a sentence into words (tokenization), reducing words to their root forms (stemming), or representing text with numbers (TF-IDF).

* **It’s efficient and interpretable.** Classical techniques often work faster, use less memory, and produce results that are easier to understand. If you're working on a small project or a problem where interpretability is important, these methods might be the perfect fit.

* **It’s still used today.** Many real-world systems, especially those running in resource-constrained environments, continue to rely on classical NLP techniques. Even modern models often include preprocessing steps from the classical toolkit.

In short, classical NLP is like learning the fundamentals of music before composing symphonies with an orchestra of AI instruments.

---

### What Will You Learn in This Chapter?

In this chapter, we’ll build your classical NLP foundation step by step. We’ll begin with **text preprocessing**, where you'll learn how to clean and prepare raw text. You’ll meet concepts like **tokenization**, **stemming**, **lemmatization**, and **part-of-speech tagging**—the essential building blocks of language analysis.

Next, we’ll explore how to **represent text as numbers** using approaches like the **Bag-of-Words model** and **TF-IDF**. These models might seem simple, but they paved the way for everything that came later.

Then, we’ll see how these elements come together in **rule-based and statistical NLP pipelines**—the traditional ways of solving tasks like text classification.

Finally, we’ll reflect on the **limitations** of these classical methods, and understand why the world moved toward machine learning and, eventually, deep learning.

---

### A Quick Analogy: The Toolbox vs. the Robot Brain

Imagine classical NLP as a **well-stocked toolbox**. You’ve got scissors to cut out words, a measuring tape to count their frequency, and glue to piece ideas together. You, the human, must decide what tools to use and when.

Now contrast that with modern LLMs like GPT, which act like a **robot with a brain**. It has learned to use the tools, anticipate your needs, and build language understanding on its own.

But here’s the catch—if you don’t know what’s in the toolbox, you won’t know how the robot works, how to fix it, or when a simpler tool would do the job better.

This chapter teaches you that toolbox.

