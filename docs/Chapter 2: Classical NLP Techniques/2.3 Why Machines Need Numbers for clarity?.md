## **2.3 Representing Text with Numbers**

Before we explore the models like Bag-of-Words or TF-IDF, let’s take a step back and ask a simple but important question:

---

### **2.3.0 Why Machines Need Numbers**

---

### 🧠 Computers Don’t Understand Words—They Understand Numbers

Humans are great with words. We understand meaning, tone, sarcasm, and context. But machines? They don’t **see** words at all.

To a computer, the sentence:

> “Natural language is amazing.”

...is just a string of characters—meaningless symbols like `"N"`, `"a"`, `"t"`, `"u"`.

So before a machine can **analyze**, **compare**, or **make predictions** on text, we must translate that text into a form it can understand:
**numbers**.

This process is called **vectorization**—turning text into numerical vectors.

---

### 📦 Why Not Just Use a Dictionary?

You might think:

> Can’t we just assign each word an ID?

For example:

* `"natural"` → 1
* `"language"` → 2
* `"is"` → 3
* `"amazing"` → 4

That’s called **integer encoding**, and it's sometimes useful.

But this alone doesn’t help much.

> 🤔 The problem? It treats all words as **equally unrelated**.
> There’s no way for a model to know that `"amazing"` and `"great"` are more similar than `"amazing"` and `"banana"`.

Also, **order doesn’t matter** and **meaning is ignored**.

So instead of just assigning IDs, we need representations that:

* Capture **frequency** (how often a word appears)
* Capture **importance** (how useful a word is in a document)
* Eventually, capture **meaning and context** (as we’ll see in later chapters)

---

### 🔁 From Words to Vectors

What we really want is a way to represent text as **vectors**—but what does that mean?

A **vector** is simply an ordered list of numbers. In mathematics, vectors are used to represent points or directions in space. In NLP, we use vectors to represent words, sentences, or even entire documents as points in a high-dimensional space.

**How can a word become a vector?**  
Imagine you have a list of all the words in your vocabulary. For each word, you can create a vector—a row of numbers—where each number captures something about that word. For example, the numbers might represent:

- Whether a word appears in a document (1 for yes, 0 for no)
- How many times a word appears (its frequency)
- How important a word is in the context of a document or collection

By turning words into vectors, we can:

* **Input them into machine learning models**
* **Analyze them mathematically** (e.g., measure similarity or difference)
* **Bridge the gap between language and logic**—allowing computers to process and learn from text

This transformation is the foundation of modern NLP.

---

### 🧭 Where We’re Headed

In the rest of this section, you’ll explore two powerful and foundational methods for turning words into numbers:

* **Bag-of-Words** – which counts word occurrences
* **TF-IDF** – which scores words based on how unique and important they are

These techniques don’t “understand” language—but they’re good enough to power spam filters, search engines, and many real-world applications.

In the next chapter, we’ll move from **counting words** to **understanding meaning** using **word embeddings**—but for now, let’s master these classical representations first.

