## **3.4.4 Negative Sampling and Efficiency Tricks**

So far, weâ€™ve seen how Word2Vec learns word embeddings by predicting context words. But thereâ€™s a catchâ€”**real-world vocabularies are huge**.

Imagine trying to predict the correct context word out of a vocabulary of **100,000 words** or more. At every training step, the model would need to:

* Compute scores for **every word** in the vocabulary.
* Apply the softmax function (which needs to sum over all those scores).
* Update gradients for **every word** in the output layer.

Thatâ€™s... expensive. ğŸ˜“

This is where **efficiency tricks** like **Negative Sampling** come in. These techniques help us **approximate the full softmax**, allowing the model to train fasterâ€”without sacrificing quality.

Letâ€™s break it down intuitively.

---

### âš ï¸ The Problem: Softmax Over a Huge Vocabulary

In the original Skip-Gram architecture, predicting the context word means computing:

$$
P(\text{word}_i \mid \text{target}) = \frac{e^{v_i \cdot v_{\text{target}}}}{\sum_{j=1}^{|V|} e^{v_j \cdot v_{\text{target}}}}
$$

That denominator sums over **every single word** in the vocabulary `V`.
When `|V|` = 100,000 or more, this becomes **infeasible**.

Imagine doing this for **every word pair**, in every window, in every sentence, for **millions of sentences**. Ouch.

---

### âœ… The Solution: Negative Sampling

**Negative Sampling** offers a simple, elegant workaround:

> Donâ€™t try to update all words. Just update a **few "negative" words** per training step.

Hereâ€™s how it works:

#### ğŸ”§ Step-by-Step

1. For each (target, context) **positive pair** (e.g., `"sat"` â†’ `"on"`), we generate **k negative samples**: random words that are **not** true context words (e.g., `"banana"`, `"explosion"`, `"cloud"`).

2. The model tries to:

   * **Increase the similarity** between the target and the true context word
   * **Decrease the similarity** between the target and the negative samples

3. These are trained using a **binary classification objective**:

   * Label the true pair as `1`
   * Label the random pairs as `0`

So instead of training a huge softmax classifier, the model now just learns:

* â€œThis is a valid pairâ€ (positive example)
* â€œThis is a fake pairâ€ (negative example)

Much cheaper. Much faster.

---

### âš™ï¸ An Analogy: Security Checkpoint

Imagine youâ€™re at airport security. Normally, you'd have to check **every single bag**. Thatâ€™s what full softmax is doingâ€”**exhaustive inspection**.

Negative sampling is like saying:

* â€œI know what a dangerous item looks like.â€
* â€œIâ€™ll just check this one good bag... and compare it to 5 suspicious-looking ones.â€
* â€œIf the good bag looks *less dangerous* than the others, weâ€™re good!â€

Efficient, effective, and gets the job done. âœˆï¸

---

### ğŸ” Training Loop with Negative Sampling (Skip-Gram)

Letâ€™s say weâ€™re training on:

```text
Input: "sat"
Target context: "on"
```

Now:

* Choose 4 random words: `"banana"`, `"xylophone"`, `"river"`, `"furnace"`

We now train the model to:

* Maximize similarity with `"on"` (label = 1)
* Minimize similarity with each negative sample (label = 0)

This turns one prediction into **five quick binary classifications**.

---

### ğŸ§  Why Negative Sampling Works

It works well because:

* Most word pairs **donâ€™t co-occur**, so randomly sampled pairs are likely to be wrong.
* Over time, the model learns to **pull real context words together** and **push others apart**.

You donâ€™t need perfect classification over the whole vocabâ€”you just need to learn **relative meaning**.

---

### ğŸ“š Other Efficiency Trick: Hierarchical Softmax (Optional)

An alternative to negative sampling is **Hierarchical Softmax**, which organizes words into a binary tree. Predictions become **log-time** instead of linear-time.

While useful, it's a bit more complex, and negative sampling is far more popular due to its simplicity and speed.

