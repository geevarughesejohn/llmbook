## **3.4.4 Negative Sampling and Efficiency Tricks**

So far, we’ve seen how Word2Vec learns word embeddings by predicting context words. But there’s a catch—**real-world vocabularies are huge**.

Imagine trying to predict the correct context word out of a vocabulary of **100,000 words** or more. At every training step, the model would need to:

* Compute scores for **every word** in the vocabulary.
* Apply the softmax function (which needs to sum over all those scores).
* Update gradients for **every word** in the output layer.

That’s... expensive. 😓

This is where **efficiency tricks** like **Negative Sampling** come in. These techniques help us **approximate the full softmax**, allowing the model to train faster—without sacrificing quality.

Let’s break it down intuitively.

---

### ⚠️ The Problem: Softmax Over a Huge Vocabulary

In the original Skip-Gram architecture, predicting the context word means computing:

$$
P(\text{word}_i \mid \text{target}) = \frac{e^{v_i \cdot v_{\text{target}}}}{\sum_{j=1}^{|V|} e^{v_j \cdot v_{\text{target}}}}
$$

That denominator sums over **every single word** in the vocabulary `V`.
When `|V|` = 100,000 or more, this becomes **infeasible**.

Imagine doing this for **every word pair**, in every window, in every sentence, for **millions of sentences**. Ouch.

---

### ✅ The Solution: Negative Sampling

**Negative Sampling** offers a simple, elegant workaround:

> Don’t try to update all words. Just update a **few "negative" words** per training step.

Here’s how it works:

#### 🔧 Step-by-Step

1. For each (target, context) **positive pair** (e.g., `"sat"` → `"on"`), we generate **k negative samples**: random words that are **not** true context words (e.g., `"banana"`, `"explosion"`, `"cloud"`).

2. The model tries to:

   * **Increase the similarity** between the target and the true context word
   * **Decrease the similarity** between the target and the negative samples

3. These are trained using a **binary classification objective**:

   * Label the true pair as `1`
   * Label the random pairs as `0`

So instead of training a huge softmax classifier, the model now just learns:

* “This is a valid pair” (positive example)
* “This is a fake pair” (negative example)

Much cheaper. Much faster.

---

### ⚙️ An Analogy: Security Checkpoint

Imagine you’re at airport security. Normally, you'd have to check **every single bag**. That’s what full softmax is doing—**exhaustive inspection**.

Negative sampling is like saying:

* “I know what a dangerous item looks like.”
* “I’ll just check this one good bag... and compare it to 5 suspicious-looking ones.”
* “If the good bag looks *less dangerous* than the others, we’re good!”

Efficient, effective, and gets the job done. ✈️

---

### 🔁 Training Loop with Negative Sampling (Skip-Gram)

Let’s say we’re training on:

```text
Input: "sat"
Target context: "on"
```

Now:

* Choose 4 random words: `"banana"`, `"xylophone"`, `"river"`, `"furnace"`

We now train the model to:

* Maximize similarity with `"on"` (label = 1)
* Minimize similarity with each negative sample (label = 0)

This turns one prediction into **five quick binary classifications**.

---

### 🧠 Why Negative Sampling Works

It works well because:

* Most word pairs **don’t co-occur**, so randomly sampled pairs are likely to be wrong.
* Over time, the model learns to **pull real context words together** and **push others apart**.

You don’t need perfect classification over the whole vocab—you just need to learn **relative meaning**.

---

### 📚 Other Efficiency Trick: Hierarchical Softmax (Optional)

An alternative to negative sampling is **Hierarchical Softmax**, which organizes words into a binary tree. Predictions become **log-time** instead of linear-time.

While useful, it's a bit more complex, and negative sampling is far more popular due to its simplicity and speed.

