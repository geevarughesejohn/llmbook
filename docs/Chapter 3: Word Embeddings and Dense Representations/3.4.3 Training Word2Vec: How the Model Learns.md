## **3.4.3 Training Word2Vec: How the Model Learns**

Now that we understand what CBOW and Skip-Gram do at a high levelâ€”predicting words from context and vice versaâ€”itâ€™s time to peek under the hood and see how Word2Vec **actually learns** these embeddings.

This is where the abstract idea of "vector representations" becomes real. Behind the scenes, Word2Vec uses a **shallow neural network** to convert words into vectors that are good at predictionâ€”and, as a side effect, **good at capturing meaning**.

Letâ€™s walk through how that happens in an intuitive, beginner-friendly way.

---

### ğŸ§  The Word2Vec Architecture (Simplified)

Word2Vec uses a **very simple neural network** with just one hidden layer. Itâ€™s one of the most lightweight neural models youâ€™ll encounter, yet incredibly effective.

Hereâ€™s the basic structure:

```
Input Layer â†’ Hidden Layer (Embeddings) â†’ Output Layer
```

Letâ€™s go through it using **Skip-Gram** as the example:

1. The **input** is a one-hot encoded word.
   For example, if your vocabulary has 10,000 words, the word `"sat"` would be a vector like:

   ```
   [0, 0, ..., 1, ..., 0]  # 10,000 dimensions, only 1 is "on"
   ```

2. The input is passed through a **weight matrix** (the hidden layer).
   This matrix is actually what becomes your **word embedding matrix**.

   * If your embedding size is 100, the weight matrix is of size **10,000 Ã— 100**.
   * It maps the one-hot vector to a **dense vector of 100 numbers**.

3. The output layer tries to **predict the context words**.
   This involves comparing the hidden representation to **all other word vectors**, using something like a dot product or softmax.

---

### ğŸ”§ Learning by Prediction

The training goal is simple:

> Adjust the word vectors so that they become better at predicting the right context words.

Each time the model makes a prediction, it checks:

* Were the predicted words close to the actual ones?
* If not, it adjusts the vectorsâ€”**nudging** them in the right direction using gradient descent.

This happens millions of times. Slowly, the word vectors shift in space:

* Words that appear in similar contexts move closer together.
* Rare or unrelated words drift further apart.

---

### ğŸ” Letâ€™s Recap That Flow (Skip-Gram Version)

1. Input word â†’ one-hot vector (e.g., `"sat"`)
2. Multiply by embedding matrix â†’ get `"sat"`â€™s embedding
3. Multiply by another matrix to predict surrounding words
4. Compare predictions with true context words
5. Use the error to update both matrices via backpropagation

After training:

* The first matrix (input â†’ hidden) becomes your **embedding matrix**.
* You throw away the rest. You donâ€™t need the output layer anymore!

---

### ğŸ“¥ What About CBOW?

In **CBOW**, you reverse the direction of prediction:

* You **average** the embeddings of the context words.
* Then you use that average vector to **predict the center word**.

The learning process is otherwise similar:

* You pass the average through a layer.
* You get a prediction.
* You update the vectors to reduce the prediction error.

---

### ğŸ¯ What Makes This Effective?

You might be wondering: why does this simple model work so well?

Itâ€™s because:

* The model sees **millions of examples** of word usage.
* It optimizes the embeddings to capture **useful generalizations**.
* The embeddings are not hand-craftedâ€”they are **discovered** from real text.

In doing so, the vectors come to **reflect semantic properties**:

* `"strong"` and `"powerful"` end up close.
* `"man"` - `"king"` + `"woman"` â‰ˆ `"queen"`

Thatâ€™s learned purely by solving this **context prediction task** over and over.

