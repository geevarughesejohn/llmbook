## **3.4.3 Training Word2Vec: How the Model Learns**

Now that we understand what CBOW and Skip-Gram do at a high level—predicting words from context and vice versa—it’s time to peek under the hood and see how Word2Vec **actually learns** these embeddings.

This is where the abstract idea of "vector representations" becomes real. Behind the scenes, Word2Vec uses a **shallow neural network** to convert words into vectors that are good at prediction—and, as a side effect, **good at capturing meaning**.

Let’s walk through how that happens in an intuitive, beginner-friendly way.

---

### 🧠 The Word2Vec Architecture (Simplified)

Word2Vec uses a **very simple neural network** with just one hidden layer. It’s one of the most lightweight neural models you’ll encounter, yet incredibly effective.

Here’s the basic structure:

```
Input Layer → Hidden Layer (Embeddings) → Output Layer
```

Let’s go through it using **Skip-Gram** as the example:

1. The **input** is a one-hot encoded word.
   For example, if your vocabulary has 10,000 words, the word `"sat"` would be a vector like:

   ```
   [0, 0, ..., 1, ..., 0]  # 10,000 dimensions, only 1 is "on"
   ```

2. The input is passed through a **weight matrix** (the hidden layer).
   This matrix is actually what becomes your **word embedding matrix**.

   * If your embedding size is 100, the weight matrix is of size **10,000 × 100**.
   * It maps the one-hot vector to a **dense vector of 100 numbers**.

3. The output layer tries to **predict the context words**.
   This involves comparing the hidden representation to **all other word vectors**, using something like a dot product or softmax.

---

### 🔧 Learning by Prediction

The training goal is simple:

> Adjust the word vectors so that they become better at predicting the right context words.

Each time the model makes a prediction, it checks:

* Were the predicted words close to the actual ones?
* If not, it adjusts the vectors—**nudging** them in the right direction using gradient descent.

This happens millions of times. Slowly, the word vectors shift in space:

* Words that appear in similar contexts move closer together.
* Rare or unrelated words drift further apart.

---

### 🔁 Let’s Recap That Flow (Skip-Gram Version)

1. Input word → one-hot vector (e.g., `"sat"`)
2. Multiply by embedding matrix → get `"sat"`’s embedding
3. Multiply by another matrix to predict surrounding words
4. Compare predictions with true context words
5. Use the error to update both matrices via backpropagation

After training:

* The first matrix (input → hidden) becomes your **embedding matrix**.
* You throw away the rest. You don’t need the output layer anymore!

---

### 📥 What About CBOW?

In **CBOW**, you reverse the direction of prediction:

* You **average** the embeddings of the context words.
* Then you use that average vector to **predict the center word**.

The learning process is otherwise similar:

* You pass the average through a layer.
* You get a prediction.
* You update the vectors to reduce the prediction error.

---

### 🎯 What Makes This Effective?

You might be wondering: why does this simple model work so well?

It’s because:

* The model sees **millions of examples** of word usage.
* It optimizes the embeddings to capture **useful generalizations**.
* The embeddings are not hand-crafted—they are **discovered** from real text.

In doing so, the vectors come to **reflect semantic properties**:

* `"strong"` and `"powerful"` end up close.
* `"man"` - `"king"` + `"woman"` ≈ `"queen"`

That’s learned purely by solving this **context prediction task** over and over.

