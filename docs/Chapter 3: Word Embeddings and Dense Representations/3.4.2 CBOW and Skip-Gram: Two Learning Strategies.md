
## **3.4.2 CBOW and Skip-Gram: Two Learning Strategies**

When we say "Word2Vec," we‚Äôre actually referring to **two related models**‚Äîeach with a slightly different way of learning from language. Both models share the same goal: **to learn word embeddings that capture meaning**. But they approach that goal from opposite directions.

These two strategies are:

* **CBOW** (*Continuous Bag-of-Words*): Predict the word in the middle from the surrounding context.
* **Skip-Gram**: Predict the surrounding context words from the word in the middle.

Let‚Äôs explore each one step-by-step with examples, intuition, and diagrams (you can visualize them later using illustrations as placeholders).

---

### üß± CBOW: Predict the Word from the Context

CBOW is like playing a guessing game:

> "Can you guess the missing word if I give you the words around it?"

#### üìå Example

Take the sentence:

> ‚ÄúThe cat **sat** on the mat.‚Äù

Suppose the target word is `"sat"` and we choose a **window size of 2**, which means we look at the 2 words before and after the target.

The context becomes: `"The"`, `"cat"`, `"on"`, `"the"`
CBOW‚Äôs task is to predict the center word: `"sat"`

So:

```text
Input:  ["The", "cat", "on", "the"]
Target: "sat"
```

The model processes the surrounding words and learns to predict the center word. To do this effectively, it must create vector representations of the context words that are informative enough to guess the target. This is how embeddings get learned.

#### ‚úÖ Characteristics of CBOW

* **Faster** to train because it averages context vectors.
* Works well for **larger corpora**.
* Tends to smooth over rare word representations (averaging effects).
* Trains on more data points (every target word appears in a context window).

---

Absolutely! Here is the corrected **Skip-Gram** portion of **Section 3.4.2**, now written clearly and accurately in book-style format:

---

### üîÑ Skip-Gram: Predict the Context from the Word

If CBOW is about guessing the word in the middle, **Skip-Gram flips that idea** on its head. Instead of predicting the center word from its neighbors, Skip-Gram does the opposite:

> ‚ÄúCan you guess the words around me if I give you the center word?‚Äù

This turns each word in the corpus into a kind of **anchor point**, and the model tries to learn which words are likely to appear nearby.

---

#### üìå Example

Consider the sentence:

> ‚ÄúThe cat **sat** on the mat.‚Äù

Let‚Äôs say our **window size is 2**. That means we look at two words on either side of the target word.

Now let‚Äôs take `"sat"` as our center word.
The Skip-Gram model will try to **predict its context**:

```text
Input:   "sat"  
Targets: ["The", "cat", "on", "the"]
```

The model turns this into four training pairs:

* ("sat", "The")
* ("sat", "cat")
* ("sat", "on")
* ("sat", "the")

So instead of predicting one word from multiple inputs (like CBOW does), **Skip-Gram predicts multiple outputs from one input word.**

---

#### ‚úÖ Characteristics of Skip-Gram

* **Focuses on the target word** and tries to predict surrounding words.
* Trains on **more pairs**, especially useful for rare words.
* Tends to produce **more fine-grained** and **informative embeddings**.
* Typically **slower to train**, since it creates more training examples than CBOW.
* Better for **smaller datasets** or when you care about learning rare word behavior.

---

### üîç CBOW vs Skip-Gram: Side-by-Side Summary

| Feature                   | **CBOW**                        | **Skip-Gram**                  |
| ------------------------- | ------------------------------- | ------------------------------ |
| Prediction Task           | Context ‚Üí Target word           | Target word ‚Üí Context words    |
| Training Speed            | Faster                          | Slower                         |
| Performance on Rare Words | Weaker                          | Better                         |
| Data Efficiency           | More efficient                  | Needs more training examples   |
| Use Case                  | Large datasets, general purpose | Smaller datasets, finer detail |

---

### üß† Why Two Models?

Both models were designed to solve the same problem‚Äîlearning useful word embeddings‚Äîbut in slightly different ways.
Which one you choose depends on:

* Your **dataset size**
* Whether you want to prioritize **speed** or **detail**
* Your specific **task requirements**

Most modern applications use **Skip-Gram**, as it tends to yield **richer word vectors**, especially when working with fewer data.

