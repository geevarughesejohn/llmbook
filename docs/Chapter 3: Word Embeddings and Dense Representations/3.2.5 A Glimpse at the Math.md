
## **3.2.5 A Glimpse at the Math**

By now, you‚Äôve seen that word embeddings are dense vectors that capture meaning, similarity, and even relationships like analogies. But how exactly are these embeddings formed? What do those numbers actually *represent*?

In this section, we‚Äôll take a gentle look at the math behind word embeddings. Don‚Äôt worry‚Äîwe‚Äôll keep it light and intuitive. You don‚Äôt need to be a mathematician to understand the big ideas.

---

### üî¢ What Is a Word Vector, Mathematically?

A **word embedding** is just a vector: a list of real numbers.

```python
"apple" ‚Üí [0.25, -0.17, 0.08, ..., 0.41]  # Maybe 100 or 300 numbers long
```

Each number in this list is a **dimension**‚Äîthink of them as ‚Äúfeatures‚Äù or ‚Äúcoordinates‚Äù in a high-dimensional space. A 100-dimensional embedding means every word is located somewhere in a 100D semantic space.

So mathematically, we can say:

> A word embedding is a point in ‚Ñù‚Åø (n-dimensional real space).

What‚Äôs important is that the numbers are **not randomly chosen**. They‚Äôre **learned** from massive text corpora, based on how often and in what contexts a word appears.

---

### üß† How Are These Vectors Learned?

Word embeddings are typically learned using **unsupervised learning**. That means we don‚Äôt tell the model what a word means‚Äîwe just let it figure things out by looking at real-world text.

Here‚Äôs the idea:

> **Words that appear in similar contexts should have similar vectors.**

This is the **distributional hypothesis** in action.

To learn the vectors, algorithms like **Word2Vec** or **GloVe** define a simple prediction task:

* In **Word2Vec**, the model is trained to:

  * Predict a word from its surrounding words (CBOW)
  * Or predict surrounding words from the current word (Skip-gram)

The model tries to adjust the word vectors so that predictions become more accurate. In doing so, the vectors gradually take on meaningful shapes.

It works a bit like this:

1. Each word is first assigned a random vector.
2. The model reads many word sequences from real text.
3. For each training step, it updates the vectors to better predict neighboring words.
4. Over time, similar words end up with similar vectors.

No human labels. Just statistics. Just context.

---

### üßÆ What About Similarity?

Once we have these vectors, we can compare any two words using **cosine similarity**.

Mathematically, the **cosine similarity** between two vectors **A** and **B** is:

$$
\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{||A|| \times ||B||}
$$

This means:

* Take the **dot product** of the two vectors.
* Divide by the product of their **magnitudes** (lengths).
* The result is between `-1` (opposite) and `1` (identical direction).

This tells us how aligned the two word vectors are in space. If they point in the same direction, they‚Äôre semantically similar.

---

### üß© A Matrix View (Optional Insight)

If we zoom out from individual vectors, we can think of all embeddings as forming a **matrix**:

```
Word     |  dim_1 | dim_2 | dim_3 | ... | dim_n
---------|--------|--------|--------|-----|-------
apple    |  0.25  | -0.17  | 0.08   | ... | 0.41
banana   |  0.22  | -0.13  | 0.09   | ... | 0.37
laptop   | -0.34  | 0.48   | 0.91   | ... | -0.03
...
```

Each row is a word. Each column is a learned semantic feature. The model doesn‚Äôt name these features‚Äîthey‚Äôre abstract‚Äîbut they might (roughly) correspond to properties like ‚Äúis a fruit,‚Äù ‚Äúhas emotion,‚Äù or ‚Äúis a location.‚Äù

---

### ü§î Do We Need to Understand the Math to Use Embeddings?

Not at all!

You don‚Äôt need to do any math to use word embeddings in your projects. Libraries like `gensim`, `spaCy`, and `transformers` handle it all for you. But having this **intuition** helps you appreciate what‚Äôs happening under the hood‚Äîand why embeddings are so effective.


