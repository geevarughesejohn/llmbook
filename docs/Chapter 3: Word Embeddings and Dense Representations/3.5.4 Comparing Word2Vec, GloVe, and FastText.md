
## **3.5.4 Comparing Word2Vec, GloVe, and FastText**

By now, youâ€™ve met the three giants of classical word embeddings:

* **Word2Vec**: Predicts words based on local context.
* **FastText**: Adds subword awareness to Word2Vec.
* **GloVe**: Learns embeddings using global co-occurrence statistics.

Each of these methods has its own strengths, weaknesses, and best-use scenarios. Letâ€™s now put them side by side for a clear comparison.

---

### ğŸ“Š Comparison Table

| Feature                  | **Word2Vec**                | **FastText**                    | **GloVe**                       |
| ------------------------ | --------------------------- | ------------------------------- | ------------------------------- |
| **Context Type**         | Local (sliding window)      | Local (sliding window)          | Global (co-occurrence matrix)   |
| **Architecture**         | Neural (CBOW/Skip-Gram)     | Neural + Subword units          | Matrix factorization            |
| **Subword Awareness**    | âŒ No                        | âœ… Yes (character n-grams)       | âŒ No                            |
| **Handles OOV Words**    | âŒ No                        | âœ… Yes (via subwords)            | âŒ No                            |
| **One Vector per Word?** | âœ… Yes                       | âœ… Yes (composed from subwords)  | âœ… Yes                           |
| **Training Speed**       | Fast                        | Slightly slower                 | Fast (after matrix is built)    |
| **Interpretability**     | Medium                      | Medium                          | High (explicit co-occurrence)   |
| **Pretrained Models?**   | âœ… Widely available          | âœ… Available (less common)       | âœ… Very popular                  |
| **Popular Use Cases**    | General NLP, classification | Multilingual, noisy data, typos | Semantic tasks, analogy solving |

---

### ğŸ” Key Observations

* **Word2Vec** is fast, intuitive, and easy to trainâ€”but blind to morphology and word structure.
* **FastText** extends Word2Vec to handle rare and new words better by breaking words into subword chunks.
* **GloVe** is better at capturing **global relationships** and is often preferred for tasks involving semantic similarity or analogies.

---

### ğŸ¯ When to Use Each?

| Use Case                                           | Best Model   |
| -------------------------------------------------- | ------------ |
| You want fast training on clean data               | **Word2Vec** |
| You need to handle typos or rare words             | **FastText** |
| You want to use pretrained vectors quickly         | **GloVe**    |
| You need high-quality analogies or global meaning  | **GloVe**    |
| You're working with morphologically rich languages | **FastText** |

---

### ğŸ§  But Remember: Theyâ€™re All Static

All three models share one major limitation:

> **They assign the same vector to a word, regardless of its context.**

This means:

* They canâ€™t distinguish â€œrockâ€ (music) vs. â€œrockâ€ (stone)
* They struggle with context shifts and ambiguity
* They miss the syntactic and semantic flow of a sentence

And thatâ€™s what the next generation of models set out to fix.

