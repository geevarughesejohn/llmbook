
## **3.5.2 FastText – Words as Bags of Subwords**

Imagine trying to understand a language without knowing how to break down words like “running,” “runner,” or “happiness” into parts. That’s essentially what Word2Vec does—it treats every word as a unique, indivisible object.

**FastText**, developed by Facebook AI Research in 2016, offered a clever upgrade:

> **“Let’s look inside the words.”**

Instead of learning a vector for each word as a whole, FastText learns vectors for **parts of words**—called **character n-grams**—and then combines them to build word embeddings.

This simple idea makes FastText **smarter with rare words**, **better at generalizing**, and **more robust** to spelling variations and new vocabulary.

---

### 🔠 The Key Idea: Subword Units

FastText represents each word as a collection of **character-level n-grams**. These are overlapping sequences of characters inside the word.

For example, for the word `"playing"` and n-gram size 3, we might get:

```
<pl, pla, lay, ayi, yin, ing, ng>
```

(Note: FastText also uses special boundary symbols like `<` and `>` to distinguish word edges.)

Each n-gram gets its own vector. To create a word vector:

* FastText **adds up the vectors** for all its n-grams.
* The final embedding represents not just the whole word, but its internal parts.

---

### 📦 Why This Matters

This design gives FastText a few powerful advantages over Word2Vec:

#### ✅ 1. Handles Rare and Unseen Words

If the model has never seen the word `"unhappiness"` before, that’s okay!
As long as it has seen `"happy"`, `"ness"`, `"un"`, etc., it can still **compose a reasonable embedding** by combining subword vectors.

This is especially valuable in:

* **Small datasets**
* **Highly inflected languages**
* **Misspelled or made-up words** (like names or slang)

#### ✅ 2. Learns Morphology Implicitly

Words like `"run"`, `"runs"`, `"running"`, and `"runner"` will share many n-grams, so their vectors will naturally be **close in space**.

This leads to better generalization and **more compact** embedding spaces.

#### ✅ 3. Robust to Typos and Variants

Even if a user types `"runnning"` or `"plaiyng"`, FastText can still **extract familiar subword patterns**, making it less brittle.

---

### 🧠 Architecturally Speaking

Internally, FastText extends the **Skip-Gram** model.
The key difference is that it doesn’t use a one-hot vector for the input word—instead, it uses all the subword vectors to create the input.

The rest of the training process (negative sampling, context prediction) works just like in Word2Vec.

---

### 🧪 Example: Training FastText in Python

Let’s train FastText on a few simple sentences using `gensim`:

```python
from gensim.models import FastText
from nltk.tokenize import word_tokenize

sentences = [
    "Dogs are running fast",
    "A dog runs faster than a cat",
    "Running is good exercise"
]

tokenized = [word_tokenize(sent.lower()) for sent in sentences]

model = FastText(
    sentences=tokenized,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1,            # Skip-Gram
    epochs=50
)

# Try a rare word
print(model.wv['runnning'])  # Misspelled on purpose!
```

Even though `"runnning"` (with triple n’s) wasn’t seen, FastText can still **build a vector** using its subwords.

---

### 📊 When Should You Use FastText?

Use FastText when:

* You’re dealing with **morphologically rich languages**
* Your data includes **misspellings or rare words**
* You want better **generalization** from smaller datasets

It’s slightly slower to train than Word2Vec but **far more robust**.


