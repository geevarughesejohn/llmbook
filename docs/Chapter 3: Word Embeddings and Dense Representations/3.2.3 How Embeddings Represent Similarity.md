
## **3.2.3 How Embeddings Represent Similarity**

One of the most powerful and exciting features of word embeddings is that they make it possible to **measure how similar two words are—not by spelling or frequency, but by meaning.**

This is something that classical NLP methods couldn’t do. In Bag-of-Words or TF-IDF, the words `"happy"` and `"joyful"` are just two separate entries in a list. There’s no mathematical reason to treat them as similar, even if they often appear in the same kinds of texts.

But with word embeddings, we get something different. Words are not just symbols anymore—they’re **points in space**, and that changes everything.

---

### 🧭 Similarity as Distance in Space

Let’s say each word is represented by a vector: a list of numbers like this:

```python
"happy"   → [0.62, -0.14, 0.55, ..., 0.09]  
"joyful"  → [0.60, -0.18, 0.58, ..., 0.12]
"sad"     → [-0.45,  0.27, -0.32, ..., 0.78]
```

Now, we can use simple geometry to ask:

> How close are these points to each other?

Words that are **similar in meaning** will have **vectors that point in the same direction**, or are located near one another in the vector space.

---

### 📐 Cosine Similarity: A Common Metric

To measure how “close” two vectors are, we often use a technique called **cosine similarity**. Don’t worry—the name sounds more complicated than it really is.

Cosine similarity looks at the **angle between two vectors**. If they point in the same direction, the angle is small, and the cosine value is close to `1`—meaning the words are similar. If they point in opposite directions, the cosine is close to `-1`—meaning they’re dissimilar.

You don’t need to calculate this by hand. But conceptually, here’s how it works:

* `"happy"` and `"joyful"` → cosine similarity ≈ `0.95` → **very similar**
* `"happy"` and `"sad"` → cosine similarity ≈ `-0.2` → **very different**
* `"happy"` and `"dog"` → cosine similarity ≈ `0.1` → **mostly unrelated**

This is what makes embeddings **semantically meaningful**: the numbers are not arbitrary. They reflect the **actual behavior of words in language**.

---

### 🧠 Understanding Through Context

Why do similar words end up with similar vectors?

Because embeddings are learned by observing the **contexts** in which words appear. If two words appear in similar contexts, they’re likely to mean similar things.

For example:

* `"happy"` might appear in sentences like:

  * “She felt happy after the exam.”
  * “It was a happy day for everyone.”
* `"joyful"` might appear in:

  * “He looked joyful at the celebration.”
  * “The joyful news spread quickly.”

The words around them—like “felt,” “celebration,” “news,” “day”—are often shared or similar. This shared context pulls their vectors closer together during training.

This idea is based on something called the **distributional hypothesis**:

> *“You shall know a word by the company it keeps.”* — J.R. Firth (1957)

In short: **meaning emerges from usage**.

---

### 📍 Words in Clusters

If you could see embeddings in 2D or 3D space (which we’ll do later using tools like t-SNE or PCA), you’d notice something amazing:

* Words form **clusters** based on themes:
  “cat”, “dog”, “hamster” → pets
  “Paris”, “Berlin”, “Rome” → European cities
  “run”, “walk”, “jog” → physical actions

* Words also form **semantic gradients**—smooth transitions across related meanings.

This is a huge leap from earlier methods. We’re not just seeing which words appear together; we’re seeing **how their meanings relate to one another in space.**

---

### ✨ Why It Matters

Once we can measure similarity between words, we can:

* **Find synonyms automatically**
* **Group documents by meaning**
* **Improve search results** (e.g., searching for “doctor” might also match “physician”)
* **Power neural networks** for tasks like sentiment analysis, question answering, and machine translation

This is the beating heart of modern NLP: instead of treating words as symbols, we treat them as **concepts with geometry**.

