
## **3.5.3 GloVe – Global Vectors for Word Representation**

While Word2Vec and FastText focus on **local context windows**—predicting words from nearby neighbors—another school of thought asked:

> “What if we looked at the **big picture**? What if we used global statistics of how words co-occur in an entire corpus?”

That’s exactly what **GloVe** (Global Vectors for Word Representation) does. Developed by researchers at Stanford, GloVe blends the best of both worlds:

* The **predictive power** of neural embeddings (like Word2Vec)
* The **global co-occurrence information** found in traditional count-based methods (like TF-IDF)

Let’s see how it works—and why it became another major success in the evolution of NLP.

---

### 📦 The Core Idea

At its heart, GloVe builds on a simple intuition:

> “Words that appear in similar **global** contexts should have similar vector representations.”

But instead of predicting one word from another (like Word2Vec), GloVe looks at **how often word pairs occur together** across the entire corpus.

It creates a **co-occurrence matrix**, where each cell tells you:

* How many times word *i* appears near word *j* in the corpus

From this matrix, it learns word vectors that try to **preserve the ratios** of co-occurrence probabilities.

---

### 🔢 A Bit More Formally

Let’s say the word **“ice”** co-occurs often with **“cold”** and rarely with **“steam”**. Meanwhile, **“fire”** co-occurs more with **“steam”** and less with **“cold.”**

GloVe doesn’t just count these occurrences—it looks at their **ratios**:

$$
\frac{P(\text{cold} \mid \text{ice})}{P(\text{cold} \mid \text{steam})} \gg 1
$$

It then tries to learn word vectors such that:

* Their **dot product** approximates the **log of their co-occurrence count**
* This setup ensures that **semantic relationships** are encoded geometrically

The model is trained by minimizing a **weighted least squares loss** over all word pairs.

---

### 📊 Why GloVe Works Well

Here’s what makes GloVe unique:

| Feature            | Description                                                |
| ------------------ | ---------------------------------------------------------- |
| **Global context** | Uses statistics from the entire corpus (not just a window) |
| **Efficient**      | Pre-computes co-occurrence matrix, then trains             |
| **Interpretable**  | Word relationships emerge naturally in vector space        |

In fact, GloVe is famous for producing **very clean vector spaces**, where analogies like:

```text
king - man + woman ≈ queen
```

emerge even more clearly than in Word2Vec.

---

### 🛠️ Using Pretrained GloVe Vectors in Python

GloVe is often used with **pretrained embeddings**, such as the 100-dimensional vectors trained on Wikipedia and Gigaword.

Let’s see how to load and use them:

```python
import gensim.downloader as api

# Load GloVe vectors (alternative: load from a local .txt file)
glove_model = api.load("glove-wiki-gigaword-100")

# Find most similar words to 'king'
print(glove_model.most_similar("king"))
```

You can also do vector math:

```python
result = glove_model.most_similar(positive=["woman", "king"], negative=["man"])
print(result[0])  # likely "queen"
```

---

### 🤔 When to Use GloVe?

GloVe is great when:

* You want **ready-to-use embeddings** trained on large corpora
* You care about **semantic similarity** and analogy solving
* You’re doing a task that doesn’t need context-sensitive meaning (like classification)

But GloVe has its own limits:

* Like Word2Vec, it gives **one vector per word**
* It still can’t differentiate between meanings in different contexts
* It doesn’t learn from task-specific data—it’s **static**

---

### 🧠 Wrap-up

GloVe showed that you don’t need to use deep neural networks to learn good embeddings—sometimes, **word counts + smart math** can go a long way.

Still, like Word2Vec and FastText, GloVe is a **static embedding method**. No matter where or how a word appears, it gets the same vector.

In the next section, we’ll compare these three approaches side-by-side.

