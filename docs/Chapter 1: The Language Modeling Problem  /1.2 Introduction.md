
## 2. **Introduction**

Imagine you're having a conversation with someone who always knows what to say next—whether it's finishing your sentence, answering a question, or crafting a story. That’s what a **language model** tries to do.

A **language model** is a computer program trained to understand and generate human language. At its core, it's answering one big question:

> **What is the most likely next word, sentence, or idea, given what has already been said?**

You interact with language models every day—when your phone suggests words while you type, when a chatbot answers your query, or when translation apps convert text between languages. These systems work thanks to powerful models trained on massive amounts of text.

But building machines that understand language is not as simple as feeding them grammar rules or dictionaries. Language is full of **ambiguity**, **context**, **sarcasm**, **multiple meanings**, and ever-changing patterns. That’s what makes the **language modeling problem** so fascinating—and so challenging.

In this chapter, we’ll trace the journey from early rule-based methods to modern **neural networks** that power tools like ChatGPT. You’ll see how language modeling evolved, why it matters, and how it works at a fundamental level.

Let’s begin by exploring what it means to model language in the first place.


